,title,conference,year,authors,abstract,pdf_path,pdf_url,code_url,relevant,tran_flag
0,Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding,CVPR,2024,"Jin-Chuan Shi, Miao Wang, Hao-Bin Duan, Shao-Hua Guan",Open-vocabulary querying in 3D space is challenging but essential for scene understanding tasks such as object localization and segmentation. Language-embedded scene representations have made progress by incorporating language features into 3D spaces. However their efficacy heavily depends on neural networks that are resource-intensive in training and rendering. Although recent 3D Gaussians offer efficient and high-quality novel view synthesis directly embedding language features in them leads to prohibitive memory usage and decreased performance. In this work we introduce Language Embedded 3D Gaussians a novel scene representation for open-vocabulary query tasks. Instead of embedding high-dimensional raw semantic features on 3D Gaussians we propose a dedicated quantization scheme that drastically alleviates the memory requirement and a novel embedding procedure that achieves smoother yet high accuracy query countering the multi-view feature inconsistencies and the high-frequency inductive bias in point-based representations. Our comprehensive experiments show that our representation achieves the best visual quality and language querying accuracy across current language-embedded representations while maintaining real-time rendering frame rates on a single desktop GPU.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Shi_Language_Embedded_3D_Gaussians_for_Open-Vocabulary_Scene_Understanding_CVPR_2024_paper.pdf,,2,在三维空间中进行开放式词汇查询对于场景理解任务（如目标定位和分割）是具有挑战性但必不可少的。语言嵌入式场景表示通过将语言特征融入三维空间中取得了进展。然而，它们的有效性严重依赖于训练和渲染中资源密集型的神经网络。尽管最近的三维高斯模型提供了高效且高质量的新视角合成，但直接将语言特征嵌入其中会导致内存使用过大和性能下降。在这项工作中，我们引入了一种新颖的场景表示——语言嵌入式三维高斯模型，用于开放式词汇查询任务。我们提出了一种专门的量化方案，大大减轻了内存需求，并提出了一种新颖的嵌入过程，实现了更平滑但又高准确度的查询，对抗多视图特征不
1,Sparse Semi-DETR: Sparse Learnable Queries for Semi-Supervised Object Detection,CVPR,2024,"Tahira Shehzadi, Khurram Azeem Hashmi, Didier Stricker, Muhammad Zeshan Afzal",In this paper we address the limitations of the DETR-based semi-supervised object detection (SSOD) framework particularly focusing on the challenges posed by the quality of object queries. In DETR-based SSOD the one-to-one assignment strategy provides inaccurate pseudo-labels while the one-to-many assignments strategy leads to overlapping predictions. These issues compromise training efficiency and degrade model performance especially in detecting small or occluded objects. We introduce Sparse Semi-DETR a novel transformer-based end-to-end semi-supervised object detection solution to overcome these challenges. Sparse Semi-DETR incorporates a Query Refinement Module to enhance the quality of object queries significantly improving detection capabilities for small and partially obscured objects. Additionally we integrate a Reliable Pseudo-Label Filtering Module that selectively filters high-quality pseudo-labels thereby enhancing detection accuracy and consistency. On the MS-COCO and Pascal VOC object detection benchmarks Sparse Semi-DETR achieves a significant improvement over current state-of-the-art methods that highlight Sparse Semi-DETR's effectiveness in semi-supervised object detection particularly in challenging scenarios involving small or partially obscured objects.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Shehzadi_Sparse_Semi-DETR_Sparse_Learnable_Queries_for_Semi-Supervised_Object_Detection_CVPR_2024_paper.pdf,,1,本文讨论了基于DETR的半监督目标检测(SSOD)框架的局限性，特别关注对象查询质量带来的挑战。在基于DETR的SSOD中，一对一分配策略提供不准确的伪标签，而一对多分配策略会导致重叠预测。这些问题影响了训练效率，并且尤其在检测小型或遮挡对象时降低了模型性能。我们引入了Sparse Semi-DETR，这是一种新颖的基于变压器的端到端半监督目标检测解决方案，用于克服这些挑战。Sparse Semi-DETR包含一个查询精化模块，显著提高了对象查询的质量，从而改善了对小型和部分遮挡对象的检测能力。此外，我们还整合了一个可靠的伪标签筛选模块，有选择地过滤高质量的伪标签，从而提高
2,Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks,CVPR,2024,"Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan",We introduce Florence-2 a novel vision foundation model with a unified prompt-based representation for various computer vision and vision-language tasks. While existing large vision models excel in transfer learning they struggle to perform diverse tasks with simple instructions a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. Florence-2 was designed to take text-prompt as task instructions and generate desirable results in text forms whether it be captioning object detection grounding or segmentation. This multi-task learning setup demands large-scale high-quality annotated data. To this end we co-developed FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train Florence-2 to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated Florence-2 to be a strong vision foundation model contender with unprecedented zero-shot and fine-tuning capabilities.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Florence-2_Advancing_a_Unified_Representation_for_a_Variety_of_Vision_CVPR_2024_paper.pdf,,2,我们介绍了一种名为Florence-2的新型视觉基础模型，具有统一的基于提示的表示，适用于各种计算机视觉和视觉-语言任务。现有的大型视觉模型擅长迁移学习，但在执行简单指令的多样任务时往往遇到困难，这意味着处理各种空间层次和语义粒度的复杂性。Florence-2被设计为以文本提示作为任务说明，并生成文本形式的理想结果，无论是字幕、目标检测、定位还是分割。这种多任务学习设置需要大规模高质量的带标注数据。因此，我们共同开发了FLD-5B，其中包含了540亿个综合的视觉标注，涵盖了1.26亿张图像，采用了自动图像标注和模型精化的迭代策略。我们采用了序列到序列结构来训练Florence-2执行多功能综合
3,AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents,CVPR,2024,"Jieming Cui, Tengyu Liu, Nian Liu, Yaodong Yang, Yixin Zhu, Siyuan Huang",Traditional approaches in physics-based motion generation centered around imitation learning and reward shaping often struggle to adapt to new scenarios. To tackle this limitation we propose AnySkill a novel hierarchical method that learns physically plausible interactions following open-vocabulary instructions. Our approach begins by developing a set of atomic actions via a low-level controller trained via imitation learning. Upon receiving an open-vocabulary textual instruction AnySkill employs a high-level policy that selects and integrates these atomic actions to maximize the CLIP similarity between the agent's rendered images and the text. An important feature of our method is the use of image-based rewards for the high-level policy which allows the agent to learn interactions with objects without manual reward engineering. We demonstrate AnySkill's capability to generate realistic and natural motion sequences in response to unseen instructions of varying lengths marking it the first method capable of open-vocabulary physical skill learning for interactive humanoid agents.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Cui_AnySkill_Learning_Open-Vocabulary_Physical_Skill_for_Interactive_Agents_CVPR_2024_paper.pdf,,2,传统的基于物理学的运动生成方法主要集中在模仿学习和奖励塑造上，往往难以适应新的情景。为了解决这一限制，我们提出了 AnySkill，这是一种新颖的分层方法，通过学习遵循开放词汇指令的物理合理互动。我们的方法首先通过模仿学习训练了一个低层控制器，开发了一组原子动作。在收到一个开放词汇文本指令后，AnySkill利用一个高级策略选择和整合这些原子动作，以最大化代理渲染图像与文本之间的CLIP相似性。我们方法的一个重要特点是高级策略使用基于图像的奖励，这使得代理能够学习与对象的互动而无需手动设计奖励。我们展示了AnySkill生成逼真自然的运动序列的能力，以响应长度不同的未知指令，这使得AnySkill
4,DiverGen: Improving Instance Segmentation by Learning Wider Data Distribution with More Diverse Generative Data,CVPR,2024,"Chengxiang Fan, Muzhi Zhu, Hao Chen, Yang Liu, Weijia Wu, Huaqi Zhang, Chunhua Shen",Instance segmentation is data-hungry and as model capacity increases data scale becomes crucial for improving the accuracy. Most instance segmentation datasets today require costly manual annotation limiting their data scale. Models trained on such data are prone to overfitting on the training set especially for those rare categories. While recent works have delved into exploiting generative models to create synthetic datasets for data augmentation these approaches do not efficiently harness the full potential of generative models. To address these issues we introduce a more efficient strategy to construct generative datasets for data augmentation termed DiverGen. Firstly we provide an explanation of the role of generative data from the perspective of distribution discrepancy. We investigate the impact of different data on the distribution learned by the model. We argue that generative data can expand the data distribution that the model can learn thus mitigating overfitting. Additionally we find that the diversity of generative data is crucial for improving model performance and enhance it through various strategies including category diversity prompt diversity and generative model diversity. With these strategies we can scale the data to millions while maintaining the trend of model performance improvement. On the LVIS dataset DiverGen significantly outperforms the strong model X-Paste achieving +1.1 box AP and +1.1 mask AP across all categories and +1.9 box AP and +2.5 mask AP for rare categories. Our codes are available at https://github.com/aim-uofa/DiverGen.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Fan_DiverGen_Improving_Instance_Segmentation_by_Learning_Wider_Data_Distribution_with_CVPR_2024_paper.pdf,https://github.com/aim-uofa/DiverGen,2,实例分割需要大量数据，随着模型容量的增加，数据规模对提高准确性至关重要。目前大多数实例分割数据集需要昂贵的手工注释，限制了它们的数据规模。在这些数据上训练的模型容易在训练集上过拟合，尤其是对于那些罕见的类别。最近的研究探讨了利用生成模型创建合成数据集进行数据增强，但这些方法并没有有效地发挥生成模型的全部潜力。为了解决这些问题，我们提出了一种更有效的策略，用于构建生成数据集进行数据增强，称为DiverGen。首先，我们从分布差异的角度解释了生成数据的作用。我们研究了不同数据对模型学习的分布的影响。我们认为生成数据可以扩展模型可以学习的数据分布，从而减轻过拟合。此外，我们发现生成数据的多样性对提高模型性
5,Exploring the Potential of Large Foundation Models for Open-Vocabulary HOI Detection,CVPR,2024,"Ting Lei, Shaofeng Yin, Yang Liu",Open-vocabulary human-object interaction (HOI) detection which is concerned with the problem of detecting novel HOIs guided by natural language is crucial for understanding human-centric scenes. However prior zero-shot HOI detectors often employ the same levels of feature maps to model HOIs with varying distances leading to suboptimal performance in scenes containing human-object pairs with a wide range of distances. In addition these detectors primarily rely on category names and overlook the rich contextual information that language can provide which is essential for capturing open vocabulary concepts that are typically rare and not well-represented by category names alone. In this paper we introduce a novel end-to-end open vocabulary HOI detection framework with conditional multi-level decoding and fine-grained semantic enhancement (CMD-SE) harnessing the potential of Visual-Language Models (VLMs). Specifically we propose to model human-object pairs with different distances with different levels of feature maps by incorporating a soft constraint during the bipartite matching process. Furthermore by leveraging large language models (LLMs) such as GPT models we exploit their extensive world knowledge to generate descriptions of human body part states for various interactions. Then we integrate the generalizable and fine-grained semantics of human body parts to improve interaction recognition. Experimental results on two datasets SWIG-HOI and HICO-DET demonstrate that our proposed method achieves state-of-the-art results in open vocabulary HOI detection. The code and models are available at https://github.com/ltttpku/CMD-SE-release.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Lei_Exploring_the_Potential_of_Large_Foundation_Models_for_Open-Vocabulary_HOI_CVPR_2024_paper.pdf,https://github.com/ltttpku/CMD-SE-release,2,本文介绍了一种新颖的端到端开放词汇HOI检测框架，通过条件多级解码和细粒度语义增强（CMD-SE）利用视觉语言模型（VLMs）的潜力。作者提出利用大型语言模型（LLMs）如GPT模型来生成各种互动的人体部位状态描述，进而改进交互识别。在SWIG-HOI和HICO-DET两个数据集上的实验结果表明，我们提出的方法在开放词汇HOI检测方面取得了最先进的结果。
6,Transferable and Principled Efficiency for Open-Vocabulary Segmentation,CVPR,2024,"Jingxuan Xu, Wuyang Chen, Yao Zhao, Yunchao Wei",Recent success of pre-trained foundation vision-language models makes Open-Vocabulary Segmentation (OVS) possible. Despite the promising performance this approach introduces heavy computational overheads for two challenges: 1) large model sizes of the backbone; 2) expensive costs during the fine-tuning. These challenges hinder this OVS strategy from being widely applicable and affordable in real-world scenarios. Although traditional methods such as model compression and efficient fine-tuning can address these challenges they often rely on heuristics. This means that their solutions cannot be easily transferred and necessitate re-training on different models which comes at a cost. In the context of efficient OVS we target achieving performance that is comparable to or even better than prior OVS works based on large vision-language foundation models by utilizing smaller models that incur lower training costs. The core strategy is to make our efficiency principled and thus seamlessly transferable from one OVS framework to others without further customization. Comprehensive experiments on diverse OVS benchmarks demonstrate our superior trade-off between segmentation accuracy and computation costs over previous works. Our code is available on https://github.com/Xujxyang/OpenTrans,,https://openaccess.thecvf.com/content/CVPR2024/papers/Xu_Transferable_and_Principled_Efficiency_for_Open-Vocabulary_Segmentation_CVPR_2024_paper.pdf,https://github.com/Xujxyang/OpenTrans,2,基于最近预训练视觉语言模型的成功，使得开放词汇分割（OVS）成为可能。然而，尽管这种方法表现出色，但却引入了两大挑战：1）主干模型的体积巨大；2）微调过程中的高昂成本。这些挑战妨碍了OVS策略在真实场景中的广泛应用和可负担性。尽管传统方法如模型压缩和高效微调可以解决这些挑战，但它们通常依赖于启发式方法。这意味着它们的解决方案不能轻松转移，并且需要在不同模型上重新训练，这带来了一定成本。在高效OVS的背景下，我们的目标是通过利用训练成本更低的较小模型来实现与基于大型视觉语言基础模型的先前OVS作品相当甚至更好的性能。核心策
7,CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor,CVPR,2024,"Shuyang Sun, Runjia Li, Philip Torr, Xiuye Gu, Siyang Li",Existing open-vocabulary image segmentation methods require a fine-tuning step on mask labels and/or image-text datasets. Mask labels are labor-intensive which limits the number of categories in segmentation datasets. Consequently the vocabulary capacity of pre-trained VLMs is severely reduced after fine-tuning. However without fine-tuning VLMs trained under weak image-text supervision tend to make suboptimal mask predictions. To alleviate these issues we introduce a novel recurrent framework that progressively filters out irrelevant texts and enhances mask quality without training efforts. The recurrent unit is a two-stage segmenter built upon a frozen VLM. Thus our model retains the VLM's broad vocabulary space and equips it with segmentation ability. Experiments show that our method outperforms not only the training-free counterparts but also those fine-tuned with millions of data samples and sets the new state-of-the-art records for both zero-shot semantic and referring segmentation. Concretely we improve the current record by 28.8 16.0 and 6.9 mIoU on Pascal VOC COCO Object and Pascal Context.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_CLIP_as_RNN_Segment_Countless_Visual_Concepts_without_Training_Endeavor_CVPR_2024_paper.pdf,,2,现有的开放词汇图像分割方法需要在掩膜标签和/或图像文本数据集上进行微调。掩膜标签需要大量人工工作，这限制了分割数据集中类别的数量。因此，在微调后，预训练的VLM的词汇容量严重减少。然而，如果不进行微调，受弱图像文本监督训练的VLM往往会产生次优的掩膜预测。为了解决这些问题，我们引入了一种新颖的递归框架，逐渐过滤掉不相关的文本并增强掩膜质量，无需额外的训练工作。递归单元是一个基于冻结VLM构建的两阶段分割器。因此，我们的模型保留了VLM的广泛词汇空间，并赋予了它分割能力。实验证明，我们的方法不仅优于无需训练的对应方法，而且
8,AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving,CVPR,2024,"Mingfu Liang, Jong-Chyi Su, Samuel Schulter, Sparsh Garg, Shiyu Zhao, Ying Wu, Manmohan Chandraker",Autonomous vehicle (AV) systems rely on robust perception models as a cornerstone of safety assurance. However objects encountered on the road exhibit a long-tailed distribution with rare or unseen categories posing challenges to a deployed perception model. This necessitates an expensive process of continuously curating and annotating data with significant human effort. We propose to leverage recent advances in vision-language and large language models to design an Automatic Data Engine (AIDE) that automatically identifies issues efficiently curates data improves the model through auto-labeling and verifies the model through generation of diverse scenarios. This process operates iteratively allowing for continuous self-improvement of the model. We further establish a benchmark for open-world detection on AV datasets to comprehensively evaluate various learning paradigms demonstrating our method's superior performance at a reduced cost.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_AIDE_An_Automatic_Data_Engine_for_Object_Detection_in_Autonomous_CVPR_2024_paper.pdf,,1,自动驾驶车辆（AV）系统依赖稳健的感知模型来确保安全。然而，在道路上遇到的物体呈现出长尾分布，罕见或未知类别给已部署的感知模型带来挑战。这需要一个昂贵的过程，不断地筛选和标注数据，需要大量的人力。我们提出利用视觉-语言和大型语言模型的最新进展，设计一个自动数据引擎（AIDE），通过自动识别问题、高效筛选数据、通过自动标注改进模型，并通过生成多样化场景验证模型。这个过程是迭代进行的，允许模型持续自我改进。我们进一步在AV数据集上建立了一个开放世界检测的基准，全面评估各种学习范式，展示了我们方法在降低成本的同时具有优越性能。
9,Generative Region-Language Pretraining for Open-Ended Object Detection,CVPR,2024,"Chuang Lin, Yi Jiang, Lizhen Qu, Zehuan Yuan, Jianfei Cai",In recent research significant attention has been devoted to the open-vocabulary object detection task aiming to generalize beyond the limited number of classes labeled during training and detect objects described by arbitrary category names at inference. Compared with conventional object detection open vocabulary object detection largely extends the object detection categories. However it relies on calculating the similarity between image regions and a set of arbitrary category names with a pretrained vision-and-language model. This implies that despite its open-set nature the task still needs the predefined object categories during the inference stage. This raises the question: What if we do not have exact knowledge of object categories during inference? In this paper we call such a new setting as generative open-ended object detection which is a more general and practical problem. To address it we formulate object detection as a generative problem and propose a simple framework named GenerateU which can detect dense objects and generate their names in a free-form way. Particularly we employ Deformable DETR as a region proposal generator with a language model translating visual regions to object names. To assess the free-form object detection task we introduce an evaluation method designed to quantitatively measure the performance of generative outcomes. Extensive experiments demonstrate strong zero-shot detection performance of our GenerateU. For example on the LVIS dataset our GenerateU achieves comparable results to the open-vocabulary object detection method GLIP even though the category names are not seen by GenerateU during inference. Code is available at: https://github.com/FoundationVision/GenerateU.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_Generative_Region-Language_Pretraining_for_Open-Ended_Object_Detection_CVPR_2024_paper.pdf,https://github.com/FoundationVision/GenerateU,1,最近的研究中，人们开始关注开放词汇物体检测任务，旨在在推断阶段超越训练过程中标记的有限类别数量，检测由任意类别名称描述的物体。相较于传统的物体检测，开放词汇物体检测大大扩展了物体检测的类别范围。然而，它仍依赖于计算图像区域与一组任意类别名称之间的相似性，使用预训练的视觉-语言模型。这意味着，尽管具有开放性，该任务在推断阶段仍需要预定义的物体类别。提出了一个新的设置，称为生成开放式物体检测，这是一个更一般且实际的问题。我们将物体检测视为一个生成问题，并提出了一个名为GenerateU的简单框架，可以以自由形式检测出密集物体并生成它们的名称。我们特别使用可变形DETR作为区域提
10,PEM: Prototype-based Efficient MaskFormer for Image Segmentation,CVPR,2024,"Niccolò Cavagnero, Gabriele Rosi, Claudia Cuttano, Francesca Pistilli, Marco Ciccone, Giuseppe Averta, Fabio Cermelli",Recent transformer-based architectures have shown impressive results in the field of image segmentation. Thanks to their flexibility they obtain outstanding performance in multiple segmentation tasks such as semantic and panoptic under a single unified framework. To achieve such impressive performance these architectures employ intensive operations and require substantial computational resources which are often not available especially on edge devices. To fill this gap we propose Prototype-based Efficient MaskFormer (PEM) an efficient transformer-based architecture that can operate in multiple segmentation tasks. PEM proposes a novel prototype-based cross-attention which leverages the redundancy of visual features to restrict the computation and improve the efficiency without harming the performance. In addition PEM introduces an efficient multi-scale feature pyramid network capable of extracting features that have high semantic content in an efficient way thanks to the combination of deformable convolutions and context-based self-modulation. We benchmark the proposed PEM architecture on two tasks semantic and panoptic segmentation evaluated on two different datasets Cityscapes and ADE20K. PEM demonstrates outstanding performance on every task and dataset outperforming task-specific architectures while being comparable and even better than computationally expensive baselines. Code is available at https://github.com/NiccoloCavagnero/PEM.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Cavagnero_PEM_Prototype-based_Efficient_MaskFormer_for_Image_Segmentation_CVPR_2024_paper.pdf,https://github.com/NiccoloCavagnero/PEM,2,最近基于Transformer的架构在图像分割领域取得了令人印象深刻的成果。由于其灵活性，它们在语义和全景等多个分割任务中表现出色，且在单一统一框架下取得优异性能。为了实现这样令人印象深刻的性能，这些架构采用了密集操作并需要大量计算资源，而这些资源通常在边缘设备上不可用。为了填补这一差距，我们提出了基于原型的高效MaskFormer（PEM），这是一种能够在多个分割任务中操作的高效Transformer架构。PEM提出了一种新颖的基于原型的交叉注意力，利用视觉特征的冗余性来限制计算，提高效率而不损害性能。此外，PEM引入了一种高效的多尺度特征金字塔网络，能够通过可变卷积和基于上下文的自调节的结合以
11,InstanceDiffusion: Instance-level Control for Image Generation,CVPR,2024,"Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra",Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points scribbles bounding boxes or intricate instance segmentation masks and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models the ScaleU block improves image fidelity and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably on the COCO dataset we outperform previous state-of-the-art by 20.4% AP50box for box inputs and 25.4% IoU for mask inputs.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_InstanceDiffusion_Instance-level_Control_for_Image_Generation_CVPR_2024_paper.pdf,,1,文字到图像扩散模型生成高质量图像，但无法控制图像中的单个实例。我们引入InstanceDiffusion，为文字到图像扩散模型添加精确的实例级控制。InstanceDiffusion支持每个实例的自由形式语言条件，并允许灵活地指定实例位置，如简单的单个点、涂鸦边界框或复杂的实例分割蒙版及其组合。我们提出了三项主要改进，以实现精确的实例级控制。我们的UniFusion模块支持文字到图像模型的实例级条件，ScaleU模块提高了图像的保真度，而Multi-instance Sampler改进了多实例的生成。InstanceDiffusion明显超过了每个位置条件的专门的最先进模型。值得注意的是，在COCO数据集上，我们超过以前的最先进模型，对于边界框输入，AP50box性能提高了20.4％，对于蒙版输入，IoU提高了25.4％。
12,ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning,CVPR,2024,"Beomyoung Kim, Joonsang Yu, Sung Ju Hwang",Panoptic segmentation combining semantic and instance segmentation stands as a cutting-edge computer vision task. Despite recent progress with deep learning models the dynamic nature of real-world applications necessitates continual learning where models adapt to new classes (plasticity) over time without forgetting old ones (catastrophic forgetting). Current continual segmentation methods often rely on distillation strategies like knowledge distillation and pseudo-labeling which are effective but result in increased training complexity and computational overhead. In this paper we introduce a novel and efficient method for continual panoptic segmentation based on Visual Prompt Tuning dubbed ECLIPSE. Our approach involves freezing the base model parameters and fine-tuning only a small set of prompt embeddings addressing both catastrophic forgetting and plasticity and significantly reducing the trainable parameters. To mitigate inherent challenges such as error propagation and semantic drift in continual segmentation we propose logit manipulation to effectively leverage common knowledge across the classes. Experiments on ADE20K continual panoptic segmentation benchmark demonstrate the superiority of ECLIPSE notably its robustness against catastrophic forgetting and its reasonable plasticity achieving a new state-of-the-art. The code is available at https://github.com/clovaai/ECLIPSE.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_ECLIPSE_Efficient_Continual_Learning_in_Panoptic_Segmentation_with_Visual_Prompt_CVPR_2024_paper.pdf,https://github.com/clovaai/ECLIPSE,1,本文介绍了一种结合语义分割和实例分割的全景分割方法，是一项前沿的计算机视觉任务。针对现实世界应用的动态特性，需要模型进行持续学习，使其在时间上适应新类别（可塑性），同时不忘记旧类别（灾难性遗忘）。目前的持续分割方法通常依赖于蒸馏策略，如知识蒸馏和伪标记，这些策略有效但会增加训练复杂性和计算开销。提出了一种基于 Visual Prompt Tuning 的新颖高效的持续全景分割方法 ECLIPSE。该方法涉及冻结基础模型参数，仅微调一小部分提示嵌入，旨在解决灾难性遗忘和可塑性问题，并显著减少可训练参数。为了缓解持续分割中的错误传播和语义漂移等固有
13,Hybrid Proposal Refiner: Revisiting DETR Series from the Faster R-CNN Perspective,CVPR,2024,"Jinjing Zhao, Fangyun Wei, Chang Xu",With the transformative impact of the Transformer DETR pioneered the application of the encoder-decoder architecture to object detection. A collection of follow-up research e.g. Deformable DETR aims to enhance DETR while adhering to the encoder-decoder design. In this work we revisit the DETR series through the lens of Faster R-CNN. We find that the DETR resonates with the underlying principles of Faster R-CNN's RPN-refiner design but benefits from end-to-end detection owing to the incorporation of Hungarian matching. We systematically adapt the Faster R-CNN towards the Deformable DETR by integrating or repurposing each component of Deformable DETR and note that Deformable DETR's improved performance over Faster R-CNN is attributed to the adoption of advanced modules such as a superior proposal refiner (e.g. deformable attention rather than RoI Align). When viewing the DETR through the RPN-refiner paradigm we delve into various proposal refinement techniques such as deformable attention cross attention and dynamic convolution. These proposal refiners cooperate well with each other; thus we synergistically combine them to establish a Hybrid Proposal Refiner (HPR). Our HPR is versatile and can be incorporated into various DETR detectors. For instance by integrating HPR to a strong DETR detector we achieve an AP of 54.9 on the COCO benchmark utilizing a ResNet-50 backbone and a 36-epoch training schedule. Code and models are available at https://github.com/ZhaoJingjing713/HPR.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Hybrid_Proposal_Refiner_Revisiting_DETR_Series_from_the_Faster_R-CNN_CVPR_2024_paper.pdf,https://github.com/ZhaoJingjing713/HPR,2,Transformer DETR开创性地将编码器-解码器架构应用于目标检测，随后的研究如Deformable DETR旨在增强DETR并遵循编码器-解码器设计。本研究通过Faster R-CNN重新审视DETR系列，发现DETR resonates with the underlying principles of Faster R-CNN's RPN-refiner design，但由于采用了匈牙利匹配，受益于端到端检测。我们通过集成或重新利用Deformable DETR的每个组件系统地将Faster R-CNN调整至Deformable DETR，注意到Deformable DETR相对Faster R-CNN的改进性能归因于采用高级模块，如上乘的提案优化器（例如，可变形注意力而不是RoI Align）。当从RPN-refiner范式审视DETR时，我们深入研究各种提案优化技术，如可变形注意力、跨注意力和动态卷积。这些提案优化器良好协作，因此我们将
14,Retrieval-Augmented Open-Vocabulary Object Detection,CVPR,2024,"Jooyeon Kim, Eulrang Cho, Sehyung Kim, Hyunwoo J. Kim",Open-vocabulary object detection (OVD) has been studied with Vision-Language Models (VLMs) to detect novel objects beyond the pre-trained categories. Previous approaches improve the generalization ability to expand the knowledge of the detector using 'positive' pseudo-labels with additional 'class' names e.g. sock iPod and alligator. To extend the previous methods in two aspects we propose Retrieval-Augmented Losses and visual Features (RALF). Our method retrieves related 'negative' classes and augments loss functions. Also visual features are augmented with 'verbalized concepts' of classes e.g. worn on the feet handheld music player and sharp teeth. Specifically RALF consists of two modules: Retrieval Augmented Losses (RAL) and Retrieval-Augmented visual Features (RAF). RAL constitutes two losses reflecting the semantic similarity with negative vocabularies. In addition RAF augments visual features with the verbalized concepts from a large language model (LLM). Our experiments demonstrate the effectiveness of RALF on COCO and LVIS benchmark datasets. We achieve improvement up to 3.4 box AP_ 50 ^ \text N   on novel categories of the COCO dataset and 3.6 mask AP_ \text r   gains on the LVIS dataset. Code is available at https://github.com/mlvlab/RALF.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Retrieval-Augmented_Open-Vocabulary_Object_Detection_CVPR_2024_paper.pdf,https://github.com/mlvlab/RALF,1,开放式词汇目标检测（OVD）已经与视觉语言模型（VLMs）结合研究，以便检测超出预训练类别之外的新物体。先前的方法通过使用额外的“类”名称（如袜子、iPod和鳄鱼）与“正面”伪标签来提高检测器的泛化能力，以拓展检测器的知识。为了在两个方面扩展以前的方法，我们提出了检索增强损失和视觉特征（RALF）。我们的方法检索相关的“负面”类别并增强损失函数。此外，视觉特征还使用类的“用词表述概念”进行增强，例如“穿在脚上的”、“手持的音乐播放器”和“尖牙”。具体来说，RALF包括两个模块：检索增强损失（RAL）和检索增强视觉特征（RAF）。RAL由两个损失组成，反
15,Unleashing Channel Potential: Space-Frequency Selection Convolution for SAR Object Detection,CVPR,2024,"Ke Li, Di Wang, Zhangyuan Hu, Wenxuan Zhu, Shaofeng Li, Quan Wang",Deep Convolutional Neural Networks (DCNNs) have achieved remarkable performance in synthetic aperture radar (SAR) object detection but this comes at the cost of tremendous computational resources partly due to extracting redundant features within a single convolutional layer. Recent works either delve into model compression methods or focus on the carefully-designed lightweight models both of which result in performance degradation. In this paper we propose an efficient convolution module for SAR object detection called SFS-Conv which increases feature diversity within each convolutional layer through a shunt-perceive-select strategy. Specifically we shunt input feature maps into space and frequency aspects. The former perceives the context of various objects by dynamically adjusting receptive field while the latter captures abundant frequency variations and textural features via fractional Gabor transformer. To adaptively fuse features from space and frequency aspects a parameter-free feature selection module is proposed to ensure that the most representative and distinctive information are preserved. With SFS-Conv we build a lightweight SAR object detection network called SFS-CNet. Experimental results show that SFS-CNet outperforms state-of-the-art (SoTA) models on a series of SAR object detection benchmarks while simultaneously reducing both the model size and computational cost.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Unleashing_Channel_Potential_Space-Frequency_Selection_Convolution_for_SAR_Object_Detection_CVPR_2024_paper.pdf,,2,本文提出了一种用于合成孔径雷达（SAR）目标检测的高效卷积模块SFS-Conv，通过引入分流-感知-选择策略，在每个卷积层内增加特征的多样性。具体而言，将输入特征图分流到空间和频率方面。前者通过动态调整感受野来感知各种对象的上下文，而后者通过分数Gabor变换捕获丰富的频率变化和纹理特征。为了自适应地融合来自空间和频率方面的特征，提出了一个无参数的特征选择模块，以确保最具代表性和独特性的信息得以保留。通过SFS-Conv，建立了一个轻量级SAR目标检测网络SFS-CNet。实验结果表明，SFS-CNet在一系列SAR目标检测基准上优于最先进模型（SoTA），同时还减少了模型大小和计算成本。
16,Taming Self-Training for Open-Vocabulary Object Detection,CVPR,2024,"Shiyu Zhao, Samuel Schulter, Long Zhao, Zhixing Zhang, Vijay Kumar B G, Yumin Suh, Manmohan Chandraker, Dimitris N. Metaxas",Recent studies have shown promising performance in open-vocabulary object detection (OVD) by utilizing pseudo labels (PLs) from pretrained vision and language models (VLMs). However teacher-student self-training a powerful and widely used paradigm to leverage PLs is rarely explored for OVD. This work identifies two challenges of using self-training in OVD: noisy PLs from VLMs and frequent distribution changes of PLs. To address these challenges we propose SAS-Det that tames self-training for OVD from two key perspectives. First we present a split-and-fusion (SAF) head that splits a standard detection into an open-branch and a closed-branch. This design can reduce noisy supervision from pseudo boxes. Moreover the two branches learn complementary knowledge from different training data significantly enhancing performance when fused together. Second in our view unlike in closed-set tasks the PL distributions in OVD are solely determined by the teacher model. We introduce a periodic update strategy to decrease the number of updates to the teacher thereby decreasing the frequency of changes in PL distributions which stabilizes the training process. Extensive experiments demonstrate SAS-Det is both efficient and effective. SAS-Det outperforms recent models of the same scale by a clear margin and achieves 37.4 AP50 and 29.1 APr on novel categories of the COCO and LVIS benchmarks respectively. Code is available at https://github.com/xiaofeng94/SAS-Det.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Taming_Self-Training_for_Open-Vocabulary_Object_Detection_CVPR_2024_paper.pdf,https://github.com/xiaofeng94/SAS-Det,1,最近的研究表明，利用预训练的视觉和语言模型（VLMs）生成伪标签（PLs）在开放词汇物体检测（OVD）中表现出有希望的性能。然而，教师-学生自我训练是一种强大且广泛使用的方法，以利用PLs，但在OVD中却很少被探索。本文确定了在OVD中使用自我训练的两个挑战：来自VLMs的嘈杂PLs和PLs的频繁分布变化。为了解决这些挑战，我们提出了SAS-Det，从两个关键角度来驯服OVD的自我训练。首先，我们提出了一个分离与融合（SAF）头部，将标准检测分为开放支路和闭合支路。这种设计可以减少来自伪框的嘈杂监督。此外，这两个支路可以从不同的训练数据中学
17,Plug and Play Active Learning for Object Detection,CVPR,2024,"Chenhongyi Yang, Lichao Huang, Elliot J. Crowley","Annotating datasets for object detection is an expensive and time-consuming endeavor. To minimize this burden active learning (AL) techniques are employed to select the most informative samples for annotation within a constrained ""annotation budget"". Traditional AL strategies typically rely on model uncertainty or sample diversity for query sampling while more advanced methods have focused on developing AL-specific object detector architectures to enhance performance. However these specialized approaches are not readily adaptable to different object detectors due to the significant engineering effort required for integration. To overcome this challenge we introduce Plug and Play Active Learning (PPAL) a simple and effective AL strategy for object detection. PPAL is a two-stage method comprising uncertainty-based and diversity-based sampling phases. In the first stage our Difficulty Calibrated Uncertainty Sampling leverage a category-wise difficulty coefficient that combines both classification and localisation difficulties to re-weight instance uncertainties from which we sample a candidate pool for the subsequent diversity-based sampling. In the second stage we propose Category Conditioned Matching Similarity to better compute the similarities of multi-instance images as ensembles of their instance similarities which is used by the k-Means++ algorithm to sample the final AL queries. PPAL makes no change to model architectures or detector training pipelines; hence it can be easily generalized to different object detectors. We benchmark PPAL on the MS-COCO and Pascal VOC datasets using different detector architectures and show that our method outperforms prior work by a large margin. Code is available at https://github.com/ChenhongyiYang/PPAL",,https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Plug_and_Play_Active_Learning_for_Object_Detection_CVPR_2024_paper.pdf,https://github.com/ChenhongyiYang/PPAL,1,为目标检测标注数据集是一项昂贵且耗时的工作。为了减轻这一负担，通常采用主动学习（AL）技术来在受限的“标注预算”内选择最具信息量的样本进行标注。传统的AL策略通常依赖于模型不确定性或样本多样性进行查询采样，而更先进的方法则着重于开发针对AL的目标检测器架构以增强性能。然而，这些专门的方法不易适用于不同的目标检测器，因为需要大量的工程工作来进行集成。为了克服这一挑战，我们提出了“即插即用主动学习”（PPAL）——一种简单而有效的目标检测AL策略。PPAL是一种两阶段方法，包括基于不确定性和基于多样性的采样阶段。在第一阶段，我们的难度校准不确定性采样利用了一个类别相关的
18,OpenEQA: Embodied Question Answering in the Era of Foundation Models,CVPR,2024,"Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Karmesh Yadav, Qiyang Li, Ben Newman, Mohit Sharma, Vincent Berges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk, Dhruv Batra, Mrinal Kalakrishnan, Franziska Meier, Chris Paxton, Alexander Sax, Aravind Rajeswaran",We present a modern formulation of Embodied Question Answering (EQA) as the task of understanding an environment well enough to answer questions about it in natural language. An agent can achieve such an understanding by either drawing upon episodic memory exemplified by agents on smart glasses or by actively exploring the environment as in the case of mobile robots. We accompany our formulation with OpenEQA -- the first open-vocabulary benchmark dataset for EQA supporting both episodic memory and active exploration use cases. OpenEQA contains over 1600 high-quality human generated questions drawn from over 180 real-world environments. In addition to the dataset we also provide an automatic LLM-powered evaluation protocol that has excellent correlation with human judgement. Using this dataset and evaluation protocol we evaluate several state-of-the-art foundation models including GPT-4V and find that they significantly lag behind human-level performance. Consequently OpenEQA stands out as a straightforward measurable and practically relevant benchmark that poses a considerable challenge to current generation of foundation models. We hope this inspires and stimulates future research at the intersection of Embodied AI conversational agents and world models.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Majumdar_OpenEQA_Embodied_Question_Answering_in_the_Era_of_Foundation_Models_CVPR_2024_paper.pdf,,3,我们提出了一种现代化的融合问答（EQA）的表述，作为理解环境足够以用自然语言回答问题的任务。一个智能体可以通过借助智能眼镜上的代理示例来实现这种理解，也可以通过积极探索环境来实现，就像移动机器人的情况一样。我们陪伴我们的表述提供了 OpenEQA - 第一个支持序列记忆和主动探索使用案例的开放词汇基准数据集。OpenEQA包含来自180多个现实世界环境的超过1600个高质量人工生成的问题。除了数据集，我们还提供一个自动LLM动力的评估协议，该协议与人类判断具有出色的相关性。利用该数据集和评估协议，我们评估了几种最先进的基础模型，包括GPT-4V，并发现它们明显落后于人类水平的表现。因此，OpenEQA
19,SFOD: Spiking Fusion Object Detector,CVPR,2024,"Yimeng Fan, Wei Zhang, Changsong Liu, Mingyang Li, Wenrui Lu",Event cameras characterized by high temporal resolution high dynamic range low power consumption and high pixel bandwidth offer unique capabilities for object detection in specialized contexts. Despite these advantages the inherent sparsity and asynchrony of event data pose challenges to existing object detection algorithms. Spiking Neural Networks (SNNs) inspired by the way the human brain codes and processes information offer a potential solution to these difficulties. However their performance in object detection using event cameras is limited in current implementations. In this paper we propose the Spiking Fusion Object Detector (SFOD) a simple and efficient approach to SNN-based object detection. Specifically we design a Spiking Fusion Module achieving the first-time fusion of feature maps from different scales in SNNs applied to event cameras. Additionally through integrating our analysis and experiments conducted during the pretraining of the backbone network on the NCAR dataset we delve deeply into the impact of spiking decoding strategies and loss functions on model performance. Thereby we establish state-of-the-art classification results based on SNNs achieving 93.7% accuracy on the NCAR dataset. Experimental results on the GEN1 detection dataset demonstrate that the SFOD achieves a state-of-the-art mAP of 32.1% outperforming existing SNN-based approaches. Our research not only underscores the potential of SNNs in object detection with event cameras but also propels the advancement of SNNs. Code is available at https://github.com/yimeng-fan/SFOD.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Fan_SFOD_Spiking_Fusion_Object_Detector_CVPR_2024_paper.pdf,https://github.com/yimeng-fan/SFOD,2,本文介绍了事件相机的特点，如高时间分辨率、高动态范围、低功耗和高像素带宽，这些特点使其在专用情境下进行目标检测具有独特能力。尽管具有这些优势，事件数据的固有稀疏性和异步性给现有目标检测算法带来挑战。受人脑编码和处理信息方式启发，脉冲神经网络（SNNs）可能是解决这些困难的潜在解决方案。然而，在当前实现中，它们在使用事件相机进行目标检测时性能受到限制。本文提出了Spiking Fusion Object Detector (SFOD)，这是一种简单高效的基于SNN的目标检测方法。具体而言，我们设计了一个Spiking Fusion Module，实现了在应用于事件相机的SNNs中首次对不同尺度的特征图进行融合。此外，通过在NCAR数据集上对骨干网络进行预
20,Neural Exposure Fusion for High-Dynamic Range Object Detection,CVPR,2024,"Emmanuel Onzon, Maximilian Bömer, Fahim Mannan, Felix Heide",Computer vision in unconstrained outdoor scenarios must tackle challenging high dynamic range (HDR) scenes and rapidly changing illumination conditions. Existing methods address this problem with multi-capture HDR sensors and a hardware image signal processor (ISP) that produces a single fused image as input to a downstream neural network. The output of the HDR sensor is a set of low dynamic range (LDR) exposures and the fusion in the ISP is performed in image space and typically optimized for human perception on a display. Preferring tonemapped content with smooth transition regions over detail (and noise) in the resulting image this image fusion does typically not preserve all information from the LDR exposures that may be essential for downstream computer vision tasks. In this work we depart from conventional HDR image fusion and propose a learned task-driven fusion in the feature domain. Instead of using a single companded image we introduce a novel local cross-attention fusion mechanism that exploits semantic features from all exposures learned in an end-to-end fashion with supervision from downstream detection losses. The proposed method outperforms all tested conventional HDR exposure fusion and auto-exposure methods in challenging automotive HDR scenarios.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Onzon_Neural_Exposure_Fusion_for_High-Dynamic_Range_Object_Detection_CVPR_2024_paper.pdf,,2,在室外不受限制的情景下，计算机视觉必须应对具有挑战性的高动态范围（HDR）场景和快速变化的光照条件。现有方法通过多拍摄HDR传感器和硬件图像信号处理器（ISP）来解决这个问题，后者生成一幅融合的图像作为下游神经网络的输入。HDR传感器的输出是一组低动态范围（LDR）曝光，ISP中的融合是在图像空间中完成的，通常针对显示器上的人类感知进行优化。与结果图像中的细节（和噪声）相比，倾向于具有平滑过渡区域的色调映射内容，这种图像融合通常不会保留可能对下游计算机视觉任务至关重要的所有LDR曝光的所有信息。在这项工作中，我们偏离传统的HDR图像融合，提出了一个在特征域中学习任务
21,YOLO-World: Real-Time Open-Vocabulary Object Detection,CVPR,2024,"Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, Ying Shan",The You Only Look Once (YOLO) series of detectors have established themselves as efficient and practical tools. However their reliance on predefined and trained object categories limits their applicability in open scenarios. Addressing this limitation we introduce YOLO-World an innovative approach that enhances YOLO with open-vocabulary detection capabilities through vision-language modeling and pre-training on large-scale datasets. Specifically we propose a new Re-parameterizable Vision-Language Path Aggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate the interaction between visual and linguistic information. Our method excels in detecting a wide range of objects in a zero-shot manner with high efficiency. On the challenging LVIS dataset YOLO-World achieves 35.4 AP with 52.0 FPS on V100 which outperforms many state-of-the-art methods in terms of both accuracy and speed. Furthermore the fine-tuned YOLO-World achieves remarkable performance on several downstream tasks including object detection and open-vocabulary instance segmentation. Code and models are available at https://github.com/AILab-CVC/YOLO-World,,https://openaccess.thecvf.com/content/CVPR2024/papers/Cheng_YOLO-World_Real-Time_Open-Vocabulary_Object_Detection_CVPR_2024_paper.pdf,https://github.com/AILab-CVC/YOLO-World,1,YOLO-World通过视觉-语言建模和在大规模数据集上的预训练，增强了YOLO的开放词汇检测能力。提出了一种新的可重新参数化的视觉-语言路径聚合网络（RepVL-PAN）和区域-文本对比损失，以促进视觉和语言信息之间的交互。该方法在零样本方式下高效地检测各种对象，在具有挑战性的LVIS数据集上，YOLO-World在V100上以52.0 FPS实现了35.4 AP，优于许多最新方法。此外，微调后的YOLO-World在目标检测和开放词汇实例分割等多个下游任务上表现出色。
22,Instance-Aware Group Quantization for Vision Transformers,CVPR,2024,"Jaehyeon Moon, Dohyung Kim, Junyong Cheon, Bumsub Ham",Post-training quantization (PTQ) is an efficient model compression technique that quantizes a pretrained full-precision model using only a small calibration set of unlabeled samples without retraining. PTQ methods for convolutional neural networks (CNNs) provide quantization results comparable to full-precision counterparts. Directly applying them to vision transformers (ViTs) however incurs severe performance degradation mainly due to the differences in architectures between CNNs and ViTs. In particular the distribution of activations for each channel vary drastically according to input instances making PTQ methods for CNNs inappropriate for ViTs. To address this we introduce instance-aware group quantization for ViTs (IGQ-ViT). To this end we propose to split the channels of activation maps into multiple groups dynamically for each input instance such that activations within each group share similar statistical properties. We also extend our scheme to quantize softmax attentions across tokens. In addition the number of groups for each layer is adjusted to minimize the discrepancies between predictions from quantized and full-precision models under a bit-operation (BOP) constraint. We show extensive experimental results on image classification object detection and instance segmentation with various transformer architectures demonstrating the effectiveness of our approach.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Moon_Instance-Aware_Group_Quantization_for_Vision_Transformers_CVPR_2024_paper.pdf,,2,后训练量化（PTQ）是一种高效的模型压缩技术，它使用一个小的标定集对预训练的全精度模型进行量化，而无需重新训练样本。卷积神经网络（CNN）的PTQ方法提供了与全精度对应物相媲美的量化结果。然而，直接应用它们到视觉Transformer（ViTs）会导致严重的性能下降，主要是由于CNN和ViTs之间的架构差异。为了解决这个问题，我们引入了ViTs的实例感知组量化（IGQ-ViT）。为此，我们建议将激活图的通道分为多个组，并针对每个输入实例动态地进行分组，这样每个组内的激活共享相似的统计特性。我们还将我们的方案扩展到跨标记量化softmax注意力。此外，每个层的组数经调整，以在比特运算（BOP）约束下最小化量化和全精度模型之
23,Scene-adaptive and Region-aware Multi-modal Prompt for Open Vocabulary Object Detection,CVPR,2024,"Xiaowei Zhao, Xianglong Liu, Duorui Wang, Yajun Gao, Zhide Liu",Open Vocabulary Object Detection (OVD) aims to detect objects from novel classes described by text inputs based on the generalization ability of trained classes. Existing methods mainly focus on transferring knowledge from large Vision and Language models (VLM) to detectors through knowledge distillation. However these approaches show weak ability in adapting to diverse classes and aligning between the image-level pre-training and region-level detection thereby impeding effective knowledge transfer. Motivated by the prompt tuning we propose scene-adaptive and region-aware multi-modal prompts to address these issues by effectively adapting class-aware knowledge from VLM to the detector at the region level. Specifically to enhance the adaptability to diverse classes we design a scene-adaptive prompt generator from a scene perspective to consider both the commonality and diversity of the class distributions and formulate a novel selection mechanism to facilitate the acquisition of common knowledge across all classes and specific insights relevant to each scene. Meanwhile to bridge the gap between the pre-trained model and the detector we present a region-aware multi-modal alignment module which employs the region prompt to incorporate the positional information for feature distillation and integrates textual prompts to align visual and linguistic representations. Extensive experimental results demonstrate that the proposed method significantly outperforms the state-of-the-art models on the OV-COCO and OV-LVIS datasets surpassing the current method by 3.0% mAP and 4.6% APr .,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Scene-adaptive_and_Region-aware_Multi-modal_Prompt_for_Open_Vocabulary_Object_Detection_CVPR_2024_paper.pdf,,1,Open Vocabulary Object Detection（OVD）旨在通过训练类别的泛化能力，基于文本输入检测描述新颖的类别。现有方法主要侧重于通过知识蒸馏从大型视觉和语言模型（VLM）向探测器传递知识。然而，这些方法在适应各种类别和在图像级预训练与区域级检测之间的对齐方面表现出较弱的能力，从而阻碍了有效的知识转移。受提示调整的启发，我们提出了场景自适应和区域感知的多模态提示，以有效地从VLM适应类别感知知识到检测器在区域级别。具体来说，为了增强对各种类别的适应能力，我们设计了一个从场景角度考虑类别分布的场景自适应提示生成器，制定了一种新颖的选择机制，以促进获取所有类别的共同知识以及与每个场景相关的
24,DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception,CVPR,2024,"Yibo Wang, Ruiyuan Gao, Kai Chen, Kaiqiang Zhou, Yingjie Cai, Lanqing Hong, Zhenguo Li, Lihui Jiang, Dit-Yan Yeung, Qiang Xu, Kai Zhang",Current perceptive models heavily depend on resource-intensive datasets prompting the need for innovative solutions. Leveraging recent advances in diffusion models synthetic data by constructing image inputs from various annotations proves beneficial for downstream tasks. While prior methods have separately addressed generative and perceptive models DetDiffusion for the first time harmonizes both tackling the challenges in generating effective data for perceptive models. To enhance image generation with perceptive models we introduce perception-aware loss (P.A. loss) through segmentation improving both quality and controllability. To boost the performance of specific perceptive models our method customizes data augmentation by extracting and utilizing perception-aware attribute (P.A. Attr) during generation. Experimental results from the object detection task highlight DetDiffusion's superior performance establishing a new state-of-the-art in layout-guided generation. Furthermore image syntheses from DetDiffusion can effectively augment training data significantly enhancing downstream detection performance.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DetDiffusion_Synergizing_Generative_and_Perceptive_Models_for_Enhanced_Data_Generation_CVPR_2024_paper.pdf,,2,当前的感知模型严重依赖资源密集型数据集，迫切需要创新解决方案。利用最新进展的扩散模型，通过从各种注释构建图像输入来合成数据，证明有益于下游任务。虽然先前的方法分别解决了生成和感知模型的问题，但DetDiffusion首次将两者融合，解决了为感知模型生成有效数据的挑战。为了增强感知模型的图像生成能力，我们通过分割引入感知感知损失（P.A.损失），从而提高质量和可控性。为了提升特定感知模型的性能，我们的方法通过提取和利用感知感知属性（P.A.属性）来自定义数据增强。目标检测任务的实验结果突显了DetDiffusion出色的表现，在布局引导生成领域建立了新的技术水平。此外，DetDiffusion生成的图像能够有效增强训练数据，显著提升下游
25,OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation,CVPR,2024,"Xiongwei Wu, Sicheng Yu, Ee-Peng Lim, Chong-Wah Ngo",In the realm of food computing segmenting ingredients from images poses substantial challenges due to the large intra-class variance among the same ingredients the emergence of new ingredients and the high annotation costs associated with large food segmentation datasets. Existing approaches primarily utilize a closed-vocabulary and static text embeddings setting. These methods often fall short in effectively handling the ingredients particularly new and diverse ones. In response to these limitations we introduce OVFoodSeg a framework that adopts an open-vocabulary setting and enhances text embeddings with visual context. By integrating vision-language models (VLMs) our approach enriches text embedding with image-specific information through two innovative modules e.g. an image-to-text learner FoodLearner and an Image-Informed Text Encoder. The training process of OVFoodSeg is divided into two stages: the pre-training of FoodLearner and the subsequent learning phase for segmentation. The pre-training phase equips FoodLearner with the capability to align visual information with corresponding textual representations that are specifically related to food while the second phase adapts both the FoodLearner and the Image-Informed Text Encoder for the segmentation task. By addressing the deficiencies of previous models OVFoodSeg demonstrates a significant improvement achieving an 4.9% increase in mean Intersection over Union (mIoU) on the FoodSeg103 dataset setting a new milestone for food image segmentation.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_OVFoodSeg_Elevating_Open-Vocabulary_Food_Image_Segmentation_via_Image-Informed_Textual_Representation_CVPR_2024_paper.pdf,,2,在食品计算领域，从图像中分割食材面临着重大挑战，原因是同类食材之间存在较大的内部差异、新食材的出现以及与大型食品分割数据集相关的高成本。现有方法主要利用封闭词汇和静态文本嵌入设置。这些方法通常在有效处理食材，特别是新的和多样化的食材方面效果不佳。为了应对这些局限性，我们引入了OVFoodSeg框架，该框架采用开放词汇设置，并通过视觉上下文增强了文本嵌入。通过集成视觉语言模型（VLMs），我们的方法通过两个创新模块（如图像到文本学习器FoodLearner和基于图像的文本编码器）丰富了文本嵌入与图像特定信息。OVFoodSeg的训练过程分为两个阶段：FoodLearner的预训练
26,OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers,CVPR,2024,"Han Liang, Jiacheng Bao, Ruichi Zhang, Sihan Ren, Yuecheng Xu, Sibei Yang, Xin Chen, Jingyi Yu, Lan Xu",We have recently seen tremendous progress in realistic text-to-motion generation. Yet the existing methods often fail or produce implausible motions with unseen text inputs which limits the applications. In this paper we present OMG a novel framework which enables compelling motion generation from zero-shot open-vocabulary text prompts. Our key idea is to carefully tailor the pretrain-then-finetune paradigm into the text-to-motion generation. At the pre-training stage our model improves the generation ability by learning the rich out-of-domain inherent motion traits. To this end we scale up a large unconditional diffusion model up to 1B parameters so as to utilize the massive unlabeled motion data up to over 20M motion instances. At the subsequent fine-tuning stage we introduce motion ControlNet which incorporates text prompts as conditioning information through a trainable copy of the pre-trained model and the proposed novel Mixture-of-Controllers (MoC) block. MoC block adaptively recognizes various ranges of the sub-motions with a cross-attention mechanism and processes them separately with the text-token-specific experts. Such a design effectively aligns the CLIP token embeddings of text prompts to various ranges of compact and expressive motion features. Extensive experiments demonstrate that our OMG achieves significant improvements over the state-of-the-art methods on zero-shot text-to-motion generation. Project page: https://tr3e.github.io/omg-page.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_OMG_Towards_Open-vocabulary_Motion_Generation_via_Mixture_of_Controllers_CVPR_2024_paper.pdf,,3,本文介绍了一个名为OMG的新框架，能够从零样本开放词汇文本提示中生成引人入胜的动作。该框架在预训练阶段通过学习丰富的领域外固有运动特征来提高生成能力，利用了超过2000万个未标记的运动数据，以1B参数规模扩展了大型无条件扩散模型。在微调阶段，引入了运动控制网络，通过一个可训练的预训练模型副本和建议的新型控制器混合（MoC）块将文本提示作为条件信息进行建模。MoC块通过交叉注意机制自适应识别子动作的各个范围，并用文本令牌特定专家分别处理它们。OMG在零样本文本到动作生成方面取得了显着的改进，超越了现有方法。
27,Layout-Agnostic Scene Text Image Synthesis with Diffusion Models,CVPR,2024,"Qilong Zhangli, Jindong Jiang, Di Liu, Licheng Yu, Xiaoliang Dai, Ankit Ramchandani, Guan Pang, Dimitris N. Metaxas, Praveen Krishnan",While diffusion models have significantly advanced the quality of image generation their capability to accurately and coherently render text within these images remains a substantial challenge. Conventional diffusion-based methods for scene text generation are typically limited by their reliance on an intermediate layout output. This dependency often results in a constrained diversity of text styles and fonts an inherent limitation stemming from the deterministic nature of the layout generation phase. To address these challenges this paper introduces SceneTextGen a novel diffusion-based model specifically designed to circumvent the need for a predefined layout stage. By doing so SceneTextGen facilitates a more natural and varied representation of text. The novelty of SceneTextGen lies in its integration of three key components: a character-level encoder for capturing detailed typographic properties coupled with a character-level instance segmentation model and a word-level spotting model to address the issues of unwanted text generation and minor character inaccuracies. We validate the performance of our method by demonstrating improved character recognition rates on generated images across different public visual text datasets in comparison to both standard diffusion based methods and text specific methods.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zhangli_Layout-Agnostic_Scene_Text_Image_Synthesis_with_Diffusion_Models_CVPR_2024_paper.pdf,,2,扩散模型在提高图像生成质量方面取得了显著进展，但在图像中准确连贯地呈现文本仍然是一个重大挑战。传统的基于扩散的场景文本生成方法通常受制于对中间布局输出的依赖。这种依赖经常导致文本样式和字体多样性受限，这是由于布局生成阶段的确定性特性所导致的固有限制。为了应对这些挑战，本文引入了SceneTextGen，这是一种新颖的基于扩散模型，专门设计用来避免预定义布局阶段的需求。通过这样做，SceneTextGen促进了文本的更自然和多样化表现。SceneTextGen的新颖性在于它集成了三个关键组件：一个用于捕捉详细排版属性的字符级编码器，一个字符级实例分割模型以及一个词级检测模型，以解决不需要的文本生成和
28,GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation,CVPR,2024,"Mukul Khanna, Ram Ramrakhya, Gunjan Chhablani, Sriram Yenamandra, Theophile Gervet, Matthew Chang, Zsolt Kira, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi",The Embodied AI community has recently made significant strides in visual navigation tasks exploring targets from 3D coordinates objects language description and images. However these navigation models often handle only a single input modality as the target. With the progress achieved so far it is time to move towards universal navigation models capable of handling various goal types enabling more effective user interaction with robots. To facilitate this goal we propose GOAT-Bench a benchmark for the universal navigation task referred to as GO to AnyThing (GOAT). In this task the agent is directed to navigate to a sequence of targets specified by the category name language description or instance image in an open-vocabulary fashion. We benchmark monolithic RL and modular methods on the GOAT task analyzing their performance across modalities the role of explicit and implicit scene memories their robustness to noise in goal specifications and the impact of memory in lifelong scenarios.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Khanna_GOAT-Bench_A_Benchmark_for_Multi-Modal_Lifelong_Navigation_CVPR_2024_paper.pdf,,2,最近，具有实体AI功能的社区在视觉导航任务方面取得了显著进展，探索了从3D坐标对象、语言描述和图像中寻找目标的方法。然而，这些导航模型通常只处理单一输入模态作为目标。随着迄今为止取得的进展，是时候转向能够处理各种目标类型的通用导航模型，从而实现更有效的用户与机器人交互。为了促进这一目标，我们提出了GOAT-Bench，这是一个用于通用导航任务的基准测试，称为GO to AnyThing（GOAT）。在这个任务中，代理被指示导航到由类别名称、语言描述或实例图像指定的一系列目标，以开放式词汇的方式。我们在GOAT任务上对单olithic RL和模块化方法进行基准测试，分析它们在不同模态下的性能、显式和隐式场景记忆的作用、对目标规范中噪声的
29,Learning Background Prompts to Discover Implicit Knowledge for Open Vocabulary Object Detection,CVPR,2024,"Jiaming Li, Jiacheng Zhang, Jichang Li, Ge Li, Si Liu, Liang Lin, Guanbin Li",Open vocabulary object detection (OVD) aims at seeking an optimal object detector capable of recognizing objects from both base and novel categories. Recent advances leverage knowledge distillation to transfer insightful knowledge from pre-trained large-scale vision-language models to the task of object detection significantly generalizing the powerful capabilities of the detector to identify more unknown object categories. However these methods face significant challenges in background interpretation and model overfitting and thus often result in the loss of crucial background knowledge giving rise to sub-optimal inference performance of the detector. To mitigate these issues we present a novel OVD framework termed LBP to propose learning background prompts to harness explored implicit background knowledge thus enhancing the detection performance w.r.t. base and novel categories. Specifically we devise three modules: Background Category-specific Prompt Background Object Discovery and Inference Probability Rectification to empower the detector to discover represent and leverage implicit object knowledge explored from background proposals. Evaluation on two benchmark datasets OV-COCO and OV-LVIS demonstrates the superiority of our proposed method over existing state-of-the-art approaches in handling the OVD tasks.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Learning_Background_Prompts_to_Discover_Implicit_Knowledge_for_Open_Vocabulary_CVPR_2024_paper.pdf,,1,开放词汇目标检测（OVD）旨在寻找一个能够识别基础和新颖类别对象的最佳目标检测器。最近的进展利用知识蒸馏，将来自预训练大规模视觉语言模型的深刻知识转移给目标检测任务，显著推广了检测器强大的能力，可以识别更多未知的对象类别。然而，这些方法面临着背景解释和模型过拟合等重大挑战，因此往往会导致关键背景知识的丢失，从而使检测器的推理性能不佳。为了缓解这些问题，我们提出了一种新颖的OVD框架，称为LBP，提出了学习背景提示来利用探索出的隐含背景知识，从而增强对基础和新颖类别的检测性能。具体地，我们设计了三个模块：背景
30,Scene Adaptive Sparse Transformer for Event-based Object Detection,CVPR,2024,"Yansong Peng, Hebei Li, Yueyi Zhang, Xiaoyan Sun, Feng Wu",While recent Transformer-based approaches have shown impressive performances on event-based object detection tasks their high computational costs still diminish the low power consumption advantage of event cameras. Image-based works attempt to reduce these costs by introducing sparse Transformers. However they display inadequate sparsity and adaptability when applied to event-based object detection since these approaches cannot balance the fine granularity of token-level sparsification and the efficiency of window-based Transformers leading to reduced performance and efficiency. Furthermore they lack scene-specific sparsity optimization resulting in information loss and a lower recall rate. To overcome these limitations we propose the Scene Adaptive Sparse Transformer (SAST). SAST enables window-token co-sparsification significantly enhancing fault tolerance and reducing computational overhead. Leveraging the innovative scoring and selection modules along with the Masked Sparse Window Self-Attention SAST showcases remarkable scene-aware adaptability: It focuses only on important objects and dynamically optimizes sparsity level according to scene complexity maintaining a remarkable balance between performance and computational cost. The evaluation results show that SAST outperforms all other dense and sparse networks in both performance and efficiency on two large-scale event-based object detection datasets (1Mpx and Gen1). Code: https://github.com/Peterande/SAST,,https://openaccess.thecvf.com/content/CVPR2024/papers/Peng_Scene_Adaptive_Sparse_Transformer_for_Event-based_Object_Detection_CVPR_2024_paper.pdf,https://github.com/Peterande/SAST,2,最近基于Transformer的方法在事件驱动的目标检测任务上表现出色，但其高计算成本仍然削弱了事件摄像头低功耗优势。基于图像的方法尝试通过引入稀疏Transformer来降低这些成本。然而，它们在应用于事件驱动的目标检测时显示出不足的稀疏性和适应性，因为这些方法无法平衡基于标记级稀疏化的细粒度和基于窗口的Transformer的效率，导致性能和效率降低。此外，它们缺乏场景特定的稀疏性优化，导致信息丢失和较低的召回率。为了克服这些限制，我们提出了场景自适应稀疏Transformer（SAST）。SAST实现了窗口-标记共稀疏化，显著增强了容错性并降低了计算开销。利用创新的评分和选择模块
31,YolOOD: Utilizing Object Detection Concepts for Multi-Label Out-of-Distribution Detection,CVPR,2024,"Alon Zolfi, Guy Amit, Amit Baras, Satoru Koda, Ikuya Morikawa, Yuval Elovici, Asaf Shabtai",Out-of-distribution (OOD) detection has attracted a large amount of attention from the machine learning research community in recent years due to its importance in deployed systems. Most of the previous studies focused on the detection of OOD samples in the multi-class classification task. However OOD detection in the multi-label classification task a more common real-world use case remains an underexplored domain. In this research we propose YolOOD - a method that utilizes concepts from the object detection domain to perform OOD detection in the multi-label classification task. Object detection models have an inherent ability to distinguish between objects of interest (in-distribution data) and irrelevant objects (OOD data) in images that contain multiple objects belonging to different class categories. These abilities allow us to convert a regular object detection model into an image classifier with inherent OOD detection capabilities with just minor changes. We compare our approach to state-of-the-art OOD detection methods and demonstrate YolOOD's ability to outperform these methods on a comprehensive suite of in-distribution and OOD benchmark datasets.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zolfi_YolOOD_Utilizing_Object_Detection_Concepts_for_Multi-Label_Out-of-Distribution_Detection_CVPR_2024_paper.pdf,,2,最近几年，由于在实际系统中的重要性，超出分布（OOD）检测引起了机器学习研究界的广泛关注。先前的大多数研究集中于在多类别分类任务中检测OOD样本。然而，在多标签分类任务中，OOD检测作为一个更常见的实际用例领域仍未被充分探索。在这项研究中，我们提出了YolOOD - 一种利用目标检测领域概念，在多标签分类任务中执行OOD检测的方法。目标检测模型具有在图像中区分感兴趣对象（分布内数据）和无关对象（OOD数据）的固有能力，这些对象包含属于不同类别的多个对象。这些能力使我们能够仅通过进行轻微更改，将常规目标检测模型转换为具有内在OOD检测能力的图像分类器。我们将我们的方法与最先进的OOD检测方法进行了比较，并展
32,Beyond Seen Primitive Concepts and Attribute-Object Compositional Learning,CVPR,2024,"Nirat Saini, Khoi Pham, Abhinav Shrivastava",Learning from seen attribute-object pairs to generalize to unseen compositions has been studied extensively in Compositional Zero-Shot Learning (CZSL). However CZSL setup is still limited to seen attributes and objects and cannot generalize to unseen concepts and their compositions. To overcome this limitation we propose a new task Open Vocabulary-Compositional Zero-shot Learning (OV-CZSL) where unseen attributes objects and unseen compositions are evaluated. To show that OV-CZSL is a challenging yet solvable problem we propose three new benchmarks based on existing datasets MIT-States C-GQA and VAW-CZSL along with new baselines and evaluation setup. We use language embeddings and external vocabulary with our novel neighborhood expansion loss to allow any method to learn semantic correlations between seen and unseen primitives.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Saini_Beyond_Seen_Primitive_Concepts_and_Attribute-Object_Compositional_Learning_CVPR_2024_paper.pdf,,1,学习从已知属性-对象配对中进行泛化以适用于未知组合在构成零样本学习（CZSL）中得到了广泛研究。然而，CZSL设置仍受限于已知属性和对象，并且不能泛化到未知概念及其组合。为了克服这一局限性，我们提出了一个新任务——开放词汇-组合零样本学习（OV-CZSL），其中评估了未知属性、对象和未知组合。为了展示OV-CZSL是一个具有挑战性但可解决的问题，我们提出了三个基于现有数据集MIT-States C-GQA和VAW-CZSL的新基准以及新的基线和评估设置。我们使用语言嵌入和外部词汇，结合我们的新颖邻域扩展损失，使得任何方法都可以学习到已知和未知基本元素之间的语义相关性。
33,Open Vocabulary Semantic Scene Sketch Understanding,CVPR,2024,"Ahmed Bourouis, Judith E. Fan, Yulia Gryaditskaya",We study the underexplored but fundamental vision problem of machine understanding of abstract freehand scene sketches. We introduce a sketch encoder that results in semantically-aware feature space which we evaluate by testing its performance on a semantic sketch segmentation task. To train our model we rely only on the availability of bitmap sketches with their brief captions and do not require any pixel-level annotations. To obtain generalization to a large set of sketches and categories we build on a vision transformer encoder pretrained with the CLIP model. We freeze the text encoder and perform visual-prompt tuning of the visual encoder branch while introducing a set of critical modifications. Firstly we augment the classical key-query (k-q) self-attention blocks with value-value (v-v) self-attention blocks. Central to our model is a two-level hierarchical network design that enables efficient semantic disentanglement: The first level ensures holistic scene sketch encoding and the second level focuses on individual categories. We then in the second level of the hierarchy introduce a cross-attention between textual and visual branches. Our method outperforms zero-shot CLIP pixel accuracy of segmentation results by 37 points reaching an accuracy of 85.5% on the FS-COCO sketch dataset. Finally we conduct a user study that allows us to identify further improvements needed over our method to reconcile machine and human understanding of scene sketches.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Bourouis_Open_Vocabulary_Semantic_Scene_Sketch_Understanding_CVPR_2024_paper.pdf,,2,我们研究了机器理解抽象自由场景素描的基础视觉问题。我们引入了一个素描编码器，产生语义感知特征空间，通过在语义素描分割任务上测试其性能进行评估。为了训练我们的模型，我们仅依赖于带有简短标题的位图素描的可用性，而不需要任何像素级注释。为了实现对大量素描和类别的泛化，我们以在CLIP模型预训练的视觉变压器编码器为基础构建。我们冻结文本编码器，并对视觉编码器分支进行视觉提示调整，同时引入一组关键修改。首先，我们使用值-值（v-v）自注意力块增强了经典的键-查询（k-q）自注意力块。我们模型的核心是一个两级分层网络设计，实现了有效的语义解缰：第一级确保了整体场景素描编码，第二级专注于个体类别。
34,Reg-PTQ: Regression-specialized Post-training Quantization for Fully Quantized Object Detector,CVPR,2024,"Yifu Ding, Weilun Feng, Chuyan Chen, Jinyang Guo, Xianglong Liu",Although deep learning based object detection is of great significance for various applications it faces challenges when deployed on edge devices due to the computation and energy limitations. Post-training quantization (PTQ) can improve inference efficiency through integer computing. However they suffer from severe performance degradation when performing full quantization due to overlooking the unique characteristics of regression tasks in object detection. In this paper we are the first to explore regression-friendly quantization and conduct full quantization on various detectors. We reveal the intrinsic reason behind the difficulty of quantizing regressors with empirical and theoretical justifications and introduce a novel Regression-specialized Post-Training Quantization (Reg-PTQ) scheme. It includes Filtered Global Loss Integration Calibration to combine the global loss with a two-step filtering mechanism mitigating the adverse impact of false positive bounding boxes and Learnable Logarithmic-Affine Quantizer tailored for the non-uniform distributed parameters in regression structures. Extensive experiments on prevalent detectors showcase the effectiveness of the well-designed Reg-PTQ. Notably our Reg-PTQ achieves 7.6 times and 5.4 times reduction in computation and storage consumption under INT4 with little performance degradation which indicates the immense potential of fully quantized detectors in real-world object detection applications.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Ding_Reg-PTQ_Regression-specialized_Post-training_Quantization_for_Fully_Quantized_Object_Detector_CVPR_2024_paper.pdf,,1,深度学习基于目标检测虽然在各种应用中具有重要意义，但在边缘设备上部署时会面临计算和能量限制的挑战。后训练量化（PTQ）可以通过整数计算提高推理效率。然而，在执行完全量化时，由于忽视目标检测中回归任务的独特特征，它们会遭受严重的性能退化。本文首次探索了适用于回归的量化方法，并在各种检测器上进行了全量化。我们揭示了量化回归器困难背后的内在原因，并通过经验和理论证明引入了一种新颖的适用于回归的后训练量化（Reg-PTQ）方案。它包括滤波全局损失集成校准，将全局损失与两步过滤机制相结合，减轻假阳性边界框的不利影响，以及适用于
35,Active Open-Vocabulary Recognition: Let Intelligent Moving Mitigate CLIP Limitations,CVPR,2024,"Lei Fan, Jianxiong Zhou, Xiaoying Xing, Ying Wu",Active recognition which allows intelligent agents to explore observations for better recognition performance serves as a prerequisite for various embodied AI tasks such as grasping navigation and room arrangements. Given the evolving environment and the multitude of object classes it is impractical to include all possible classes during the training stage. In this paper we aim at advancing active open-vocabulary recognition empowering embodied agents to actively perceive and classify arbitrary objects. However directly adopting recent open-vocabulary classification models like Contrastive Language Image Pretraining (CLIP) poses its unique challenges. Specifically we observe that CLIP's performance is heavily affected by the viewpoint and occlusions compromising its reliability in unconstrained embodied perception scenarios. Further the sequential nature of observations in agent-environment interactions necessitates an effective method for integrating features that maintains discriminative strength for open-vocabulary classification. To address these issues we introduce a novel agent for active open-vocabulary recognition. The proposed method leverages inter-frame and inter-concept similarities to navigate agent movements and to fuse features without relying on class-specific knowledge. Compared to baseline CLIP model with 29.6% accuracy on ShapeNet dataset the proposed agent could achieve 53.3% accuracy for open-vocabulary recognition without any fine-tuning to the equipped CLIP model. Additional experiments conducted with the Habitat simulator further affirm the efficacy of our method.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Fan_Active_Open-Vocabulary_Recognition_Let_Intelligent_Moving_Mitigate_CLIP_Limitations_CVPR_2024_paper.pdf,,2,本文旨在推动活跃的开放式识别，使具有智能代理的个体能够积极感知和分类任意对象，而不依赖于特定类别的知识。新方法利用帧间和概念间的相似性来引导代理移动并融合特征，从而在无约束的感知场景下提高识别准确性。与基准CLIP模型在ShapeNet数据集上29.6%的准确率相比，提出的代理在不对配备的CLIP模型进行任何微调的情况下，可实现53.3%的开放式识别准确率。通过在Habitat模拟器上进行的额外实验进一步证实了我们方法的有效性。
36,Active Object Detection with Knowledge Aggregation and Distillation from Large Models,CVPR,2024,"Dejie Yang, Yang Liu",Accurately detecting active objects undergoing state changes is essential for comprehending human interactions and facilitating decision-making. The existing methods for active object detection (AOD) primarily rely on visual appearance of the objects within input such as changes in size shape and relationship with hands. However these visual changes can be subtle posing challenges particularly in scenarios with multiple distracting no-change instances of the same category. We observe that the state changes are often the result of an interaction being performed upon the object thus propose to use informed priors about object related plausible interactions (including semantics and visual appearance) to provide more reliable cues for AOD. Specifically we propose a knowledge aggregation procedure to integrate the aforementioned informed priors into oracle queries within the teacher decoder offering more object affordance commonsense to locate the active object. To streamline the inference process and reduce extra knowledge inputs we propose a knowledge distillation approach that encourages the student decoder to mimic the detection capabilities of the teacher decoder using the oracle query by replicating its predictions and attention. Our proposed framework achieves state-of-the-art performance on four datasets namely Ego4D Epic-Kitchens MECCANO and 100DOH which demonstrates the effectiveness of our approach in improving AOD. The code and models are available at https://github.com/idejie/KAD.git.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Active_Object_Detection_with_Knowledge_Aggregation_and_Distillation_from_Large_CVPR_2024_paper.pdf,https://github.com/idejie/KAD.git,2,准确检测处于状态变化中的主动对象是理解人类互动和促进决策的关键。现有的主动对象检测（AOD）方法主要依赖于输入中对象的视觉外观，如大小、形状和与手的关系的变化。然而，这些视觉变化可能很微妙，特别在存在多个同一类别的分散无变化实例的场景中会带来挑战。我们观察到状态变化往往是对对象执行的交互的结果，因此提出使用关于对象相关可能交互（包括语义和视觉外观）的明确先验，为AOD提供更可靠的线索。具体而言，我们提出了一种知识聚合过程，将上述明确先验整合到教师解码器中的oracle查询中，为定位活动对象提供更多的对象可供性常识。为了简化推理过程并减少额外的知识输入，我们提出了一种知识蒸馏
37,DETRs Beat YOLOs on Real-time Object Detection,CVPR,2024,"Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, Jie Chen",The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper we propose the Real-Time DEtection TRansformer (RT-DETR) the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed followed by maintaining speed while improving accuracy. Specifically we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder thereby improving accuracy. In addition RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. After pre-training with Objects365 RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP. The project page: https://zhao-yian.github.io/RTDETR.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_DETRs_Beat_YOLOs_on_Real-time_Object_Detection_CVPR_2024_paper.pdf,,1,本文介绍了当前目标检测中使用最广泛的实时框架YOLO系列，由于速度和准确性之间的合理权衡，其受非极大值抑制（NMS）的负面影响。作者提出了一种名为Real-Time DEtection TRansformer (RT-DETR)的新的端到端对象检测器，旨在解决NMS所带来的问题。RT-DETR采用了端到端Transformer-based检测器(DETRs)的优点，并在保持准确性的同时提高速度，并在保持速度的同时提高准确性。相比之前的YOLOs，RT-DETR在速度和准确性上都有显著的性能优势。此外，作者还开发了性能更好的RT-DETRs，胜过轻量级的YOLO检测器，并表明其在准确性和速度上的优势。
38,Language-conditioned Detection Transformer,CVPR,2024,"Jang Hyun Cho, Philipp Krähenbühl",We present a new open-vocabulary detection framework. Our framework uses both image-level labels and detailed detection annotations when available. Our framework proceeds in three steps. We first train a language-conditioned object detector on fully-supervised detection data. This detector gets to see the presence or absence of ground truth classes during training and conditions prediction on the set of present classes. We use this detector to pseudo-label images with image-level labels. Our detector provides much more accurate pseudo-labels than prior approaches with its conditioning mechanism. Finally we train an unconditioned open-vocabulary detector on the pseudo-annotated images. The resulting detector named DECOLA shows strong zero-shot performance in open-vocabulary LVIS benchmark as well as direct zero-shot transfer benchmarks on LVIS COCO Object365 and OpenImages. DECOLA outperforms the prior arts by 17.1 AP-rare and 9.4 mAP on zero-shot LVIS benchmark. DECOLA achieves state-of-the-art results in various model sizes architectures and datasets by only training on open-sourced data and academic-scale computing. Code is available at https://github.com/janghyuncho/DECOLA.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Cho_Language-conditioned_Detection_Transformer_CVPR_2024_paper.pdf,https://github.com/janghyuncho/DECOLA,1,我们提出了一个新的开放词汇检测框架，利用图像级标签和详细的检测注释。框架分为三个步骤：首先，在完全监督的检测数据上训练一种语言条件的目标检测器；然后使用该检测器对图像进行伪标注；最后，在伪标注的图像上训练一个无条件的开放词汇检测器。这个名为DECOLA的检测器在开放词汇LVIS基准测试中表现出色，以及在LVIS COCO Object365和OpenImages上进行的直接零样本转移基准测试中。DECOLA在零样本LVIS基准测试上比之前的方法表现出17.1 AP-rare和9.4 mAP的提升，通过仅在开源数据和学术规模计算上进行训练，在不同模型规模架构和数据集上实现了最先进的结果。源代码可在https://github.com/janghyuncho/DECOLA找到。
39,Improving Single Domain-Generalized Object Detection: A Focus on Diversification and Alignment,CVPR,2024,"Muhammad Sohail Danish, Muhammad Haris Khan, Muhammad Akhtar Munir, M. Saquib Sarfraz, Mohsen Ali",In this work we tackle the problem of domain generalization for object detection specifically focusing on the scenario where only a single source domain is available. We propose an effective approach that involves two key steps: diversifying the source domain and aligning detections based on class prediction confidence and localization. Firstly we demonstrate that by carefully selecting a set of augmentations a base detector can outperform existing methods for single domain generalization by a good margin. This highlights the importance of domain diversification in improving the performance of object detectors. Secondly we introduce a method to align detections from multiple views considering both classification and localization outputs. This alignment procedure leads to better generalized and well-calibrated object detector models which are crucial for accurate decision-making in safety-critical applications. Our approach is detector-agnostic and can be seamlessly applied to both single-stage and two-stage detectors. To validate the effectiveness of our proposed methods we conduct extensive experiments and ablations on challenging domain-shift scenarios. The results consistently demonstrate the superiority of our approach compared to existing methods.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Danish_Improving_Single_Domain-Generalized_Object_Detection_A_Focus_on_Diversification_and_CVPR_2024_paper.pdf,,2,本文针对仅有单一源域的情况，解决目标检测的领域泛化问题。提出了一种有效方法，包括两个关键步骤：多样化源域和基于类别预测置信度和定位对齐检测。首先，通过精心选择一组数据增强方法，基础检测器可以在单一领域泛化方面胜过现有方法，突出了域多样化在提高目标检测性能中的重要性。其次，引入了一种方法，考虑到分类和定位输出，对多视图的检测进行对齐。这种对齐过程可以导致更好泛化和校准良好的目标检测器模型，对于安全关键应用中准确决策至关重要。我们的方法不依赖于检测器，并且可以无缝应用于单阶段和双阶段检测器。通过对具有挑战性的领域转移场景进行广
40,Task-aligned Part-aware Panoptic Segmentation through Joint Object-Part Representations,CVPR,2024,"Daan de Geus, Gijs Dubbelman",Part-aware panoptic segmentation (PPS) requires (a) that each foreground object and background region in an image is segmented and classified and (b) that all parts within foreground objects are segmented classified and linked to their parent object. Existing methods approach PPS by separately conducting object-level and part-level segmentation. However their part-level predictions are not linked to individual parent objects. Therefore their learning objective is not aligned with the PPS task objective which harms the PPS performance. To solve this and make more accurate PPS predictions we propose Task-Aligned Part-aware Panoptic Segmentation (TAPPS). This method uses a set of shared queries to jointly predict (a) object-level segments and (b) the part-level segments within those same objects. As a result TAPPS learns to predict part-level segments that are linked to individual parent objects aligning the learning objective with the task objective and allowing TAPPS to leverage joint object-part representations. With experiments we show that TAPPS considerably outperforms methods that predict objects and parts separately and achieves new state-of-the-art PPS results.,,https://openaccess.thecvf.com/content/CVPR2024/papers/de_Geus_Task-aligned_Part-aware_Panoptic_Segmentation_through_Joint_Object-Part_Representations_CVPR_2024_paper.pdf,,2,Part-aware panoptic segmentation (PPS)要求对图像中的每个前景对象和背景区域进行分割和分类，同时也要对前景对象中的所有部分进行分割、分类并链接到它们的父对象。现有方法通过分别进行对象级别和部分级别的分割来实现PPS。然而，它们的部分级别预测未与各个父对象链接。因此，它们的学习目标与PPS任务目标不一致，影响了PPS性能。为了解决这个问题并实现更准确的PPS预测，我们提出了Task-Aligned Part-aware Panoptic Segmentation（TAPPS）。该方法使用一组共享的查询来共同预测对象级别分割和那些同一对象内的部分级别分割。由此TAPPS学会了预测与各个父对象链接的部分级别分割，使学习目标与任务目标保持一致，并允许TAPPS利用联合对象-部分表示。通过实验证明，TAPPS明显优于分别预
41,Semantic-aware SAM for Point-Prompted Instance Segmentation,CVPR,2024,"Zhaoyang Wei, Pengfei Chen, Xuehui Yu, Guorong Li, Jianbin Jiao, Zhenjun Han",Single-point annotation in visual tasks with the goal of minimizing labeling costs is becoming increasingly prominent in research. Recently visual foundation models such as Segment Anything (SAM) have gained widespread usage due to their robust zero-shot capabilities and exceptional annotation performance. However SAM's class-agnostic output and high confidence in local segmentation introduce semantic ambiguity posing a challenge for precise category-specific segmentation. In this paper we introduce a cost-effective category-specific segmenter using SAM. To tackle this challenge we have devised a Semantic-Aware Instance Segmentation Network (SAPNet) that integrates Multiple Instance Learning (MIL) with matching capability and SAM with point prompts. SAPNet strategically selects the most representative mask proposals generated by SAM to supervise segmentation with a specific focus on object category information. Moreover we introduce the Point Distance Guidance and Box Mining Strategy to mitigate inherent challenges: group and local issues in weakly supervised segmentation. These strategies serve to further enhance the overall segmentation performance. The experimental results on Pascal VOC and COCO demonstrate the promising performance of our proposed SAPNet emphasizing its semantic matching capabilities and its potential to advance point-prompted instance segmentation. The code is available at https://github.com/zhaoyangwei123/SAPNet.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Wei_Semantic-aware_SAM_for_Point-Prompted_Instance_Segmentation_CVPR_2024_paper.pdf,https://github.com/zhaoyangwei123/SAPNet,2,本文介绍了一种成本效益高的类别特定分割器，使用了SAM。为了解决SAM输出的类别不可知性和局部分割高置信度引入的语义歧义挑战，提出了一种语义感知实例分割网络（SAPNet）。SAPNet结合了多实例学习（MIL）和SAM的点提示，以特定关注对象类别信息的方式监督分割。引入了点距离引导和框挖掘策略来缓解在弱监督分割中的团体和局部问题。实验结果表明SAPNet在Pascal VOC和COCO上表现出有潜力推进点提示实例分割的语义匹配能力，其整体分割性能表现出色。
42,Learning to Control Camera Exposure via Reinforcement Learning,CVPR,2024,"Kyunghyun Lee, Ukcheol Shin, Byeong-Uk Lee",Adjusting camera exposure in arbitrary lighting conditions is the first step to ensure the functionality of computer vision applications. Poorly adjusted camera exposure often leads to critical failure and performance degradation. Traditional camera exposure control methods require multiple convergence steps and time-consuming processes making them unsuitable for dynamic lighting conditions. In this paper we propose a new camera exposure control framework that rapidly controls camera exposure while performing real-time processing by exploiting deep reinforcement learning. The proposed framework consists of four contributions: 1) a simplified training ground to simulate real-world's diverse and dynamic lighting changes 2) flickering and image attribute-aware reward design along with lightweight state design for real-time processing 3) a static-to-dynamic lighting curriculum to gradually improve the agent's exposure-adjusting capability and 4) domain randomization techniques to alleviate the limitation of the training ground and achieve seamless generalization in the wild. As a result our proposed method rapidly reaches a desired exposure level within five steps with real-time processing (1 ms). Also the acquired images are well-exposed and show superiority in various computer vision tasks such as feature extraction and object detection.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_Learning_to_Control_Camera_Exposure_via_Reinforcement_Learning_CVPR_2024_paper.pdf,,2,调整相机曝光是确保计算机视觉应用功能的第一步。糟糕的相机曝光通常会导致严重故障和性能下降。传统的相机曝光控制方法需要多次收敛步骤和耗时的过程，使其不适用于动态照明条件。本文提出了一种新的相机曝光控制框架，通过利用深度强化学习在执行实时处理的同时快速控制相机曝光。提出的框架包括四个贡献：1）简化的训练平台，模拟真实世界的多样化和动态的照明变化；2）闪烁和图像属性感知奖励设计，以及用于实时处理的轻量级状态设计；3）静态到动态照明课程，逐渐提高代理的曝光调整能力；4）域随机化技术，缓解训练平台的限制
43,Mudslide: A Universal Nuclear Instance Segmentation Method,CVPR,2024,Jun Wang,Nuclear instance segmentation has played a critical role in pathology image analysis. The main challenges arise from the difficulty in accurately segmenting densely overlapping instances and the high cost of precise mask-level annotations. Existing fully-supervised nuclear instance segmentation methods such as boundary-based methods struggle to capture differences between overlapping instances and thus fail in densely distributed blurry regions. They also face challenges transitioning to point supervision where annotations are simple and effective. Inspired by natural mudslides we propose a universal method called Mudslide that uses simple representations to characterize differences between different instances and can easily be extended from fully-supervised to point-supervised. oncretely we introduce a collapse field and leverage it to construct a force map and initial boundary enabling a distinctive representation for each instance. Each pixel is assigned a collapse force with distinct directions between adjacent instances. Starting from the initial boundary Mudslide executes a pixel-by-pixel collapse along various force directions. Pixels that collapse into the same region are considered as one instance concurrently accounting for both inter-instance distinctions and intra-instance coherence. Experiments on public datasets show superior performance in both fully-supervised and point-supervised tasks.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Mudslide_A_Universal_Nuclear_Instance_Segmentation_Method_CVPR_2024_paper.pdf,,1,核实例分割在病理图像分析中发挥了关键作用，主要挑战来自于精确分割密集重叠的实例以及准确标注掩模的高成本。现有的全监督核实例分割方法，如基于边界的方法，难以捕捉重叠实例之间的差异，在密集分布的模糊区域中失败。它们还面临着转换为点级监督的挑战，其中标注简单而有效。受自然泥石流的启发，我们提出了一种称为“Mudslide”的通用方法，使用简单的表示来表征不同实例之间的差异，并且可以轻松从全监督扩展到点监督。我们引入了坍缩场，并利用它构建力量图和初始边界，为每个实例提供一个独特的表示。每个像素被分配一个坍缩力，其方向在相邻实例之间有所不同。从初始边
44,Exploring Orthogonality in Open World Object Detection,CVPR,2024,"Zhicheng Sun, Jinghan Li, Yadong Mu",Open world object detection aims to identify objects of unseen categories and incrementally recognize them once their annotations are provided. In distinction to the traditional paradigm that is limited to predefined categories this setting promises a continual and generalizable way of estimating objectness using class-agnostic information. However achieving such decorrelation between objectness and class information proves challenging. Without explicit consideration existing methods usually exhibit low recall on unknown objects and can misclassify them into known classes. To address this problem we exploit three levels of orthogonality in the detection process: First the objectness and classification heads are disentangled by operating on separate sets of features that are orthogonal to each other in a devised polar coordinate system. Secondly a prediction decorrelation loss is introduced to guide the detector towards more general and class-independent prediction. Furthermore we propose a calibration scheme that helps maintain orthogonality throughout the training process to mitigate catastrophic interference and facilitate incremental learning of previously unseen objects. Our method is comprehensively evaluated on open world and incremental object detection benchmarks demonstrating its effectiveness in detecting both known and unknown objects. Code and models are available at https://github.com/feifeiobama/OrthogonalDet.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_Exploring_Orthogonality_in_Open_World_Object_Detection_CVPR_2024_paper.pdf,https://github.com/feifeiobama/OrthogonalDet,1,开放世界目标检测旨在识别未知类别的物体，并一旦提供它们的注释就可以逐步识别它们。与传统范式不同的是，这种设置承诺使用与类别无关的信息以一种持续且具有泛化能力的方式估计物体性。然而，实现物体性和类别信息之间的解耦被证明具有挑战性。在没有明确考虑的情况下，现有方法通常在未知物体上表现出较低的召回率，并可能将它们误分类为已知类别。为了解决这个问题，我们在检测过程中利用三个层次的正交性：首先，通过在设计的极坐标系统中操作相互正交的特征集，使物体性和分类头部解耦。其次，引入了一种预测解耦损失，以指导检测器朝向更加通用和独立于类别的预测。此外，我们提出了一种校
45,MS-DETR: Efficient DETR Training with Mixed Supervision,CVPR,2024,"Chuyang Zhao, Yifan Sun, Wenhao Wang, Qiang Chen, Errui Ding, Yi Yang, Jingdong Wang",DETR accomplishes end-to-end object detection through iteratively generating multiple object candidates based on image features and promoting one candidate for each ground-truth object. The traditional training procedure using one-to-one supervision in the original DETR lacks direct supervision for the object detection candidates. We aim at improving the DETR training efficiency by explicitly supervising the candidate generation procedure through mixing one-to-one supervision and one-to-many supervision. Our approach namely MS-DETR is simple and places one-to-many supervision to the object queries of the primary decoder that is used for inference. In comparison to existing DETR variants with one-to-many supervision such as Group DETR and Hybrid DETR our approach does not need additional decoder branches or object queries. The object queries of the primary decoder in our approach directly benefit from one-to-many supervision and thus are superior in object candidate prediction. Experimental results show that our approach outperforms related DETR variants such as DN-DETR Hybrid DETR and Group DETR and the combination with related DETR variants further improves the performance.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_MS-DETR_Efficient_DETR_Training_with_Mixed_Supervision_CVPR_2024_paper.pdf,,2,DETR通过基于图像特征迭代生成多个目标候选对象，并为每个地面真实对象提升一个候选对象，实现端到端的目标检测。传统的DETR训练过程中缺乏对目标检测候选对象的直接监督，我们通过混合一对一监督和一对多监督的方法，明确监督候选对象生成过程，旨在提高DETR的训练效率。我们的方法MS-DETR简单明了，将一对多监督放置在用于推断的主要解码器的目标查询中。与现有DETR变体（如Group DETR和Hybrid DETR）相比，我们的方法不需要额外的解码器分支或目标查询。我们方法中主要解码器的目标查询直接受益于一对多监督，因此在目标候选预测方面更为优越。实验结果表明，我们的方法优于相关的DETR变体，如DN-DETR、Hybrid DETR和Group DETR，与相关的DETR变体的组合进
46,Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models,CVPR,2024,"Matthew Kowal, Richard P. Wildes, Konstantinos G. Derpanis",Understanding what deep network models capture in their learned representations is a fundamental challenge in computer vision. We present a new methodology to understanding such vision models the Visual Concept Connectome (VCC) which discovers human interpretable concepts and their interlayer connections in a fully unsupervised manner. Our approach simultaneously reveals fine-grained concepts at a layer connection weightings across all layers and is amendable to global analysis of network structure (e.g. branching pattern of hierarchical concept assemblies). Previous work yielded ways to extract interpretable concepts from single layers and examine their impact on classification but did not afford multilayer concept analysis across an entire network architecture. Quantitative and qualitative empirical results show the effectiveness of VCCs in the domain of image classification. Also we leverage VCCs for the application of failure mode debugging to reveal where mistakes arise in deep networks.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Kowal_Visual_Concept_Connectome_VCC_Open_World_Concept_Discovery_and_their_CVPR_2024_paper.pdf,,2,理解深度网络模型在学习表示中捕捉到的内容是计算机视觉中的基本挑战。我们提出了一种新的方法来理解这种视觉模型——Visual Concept Connectome（VCC），它以完全无监督的方式发现人类可解释的概念及其层间连接。我们的方法同时揭示了各层的细粒度概念和层间连接权重，并可进行网络结构的全局分析（例如，分层概念组合的分支模式）。先前的工作提出了从单个层提取可解释概念并检查其对分类的影响的方法，但没有提供整个网络架构上的多层概念分析。定量和定性实证结果显示了VCC在图像分类领域的有效性。我们还利用VCC来进行故障模式调试应用，以揭示深度网络中错误发生的位置。
47,See Say and Segment: Teaching LMMs to Overcome False Premises,CVPR,2024,"Tsung-Han Wu, Giscard Biamby, David Chan, Lisa Dunlap, Ritwik Gupta, Xudong Wang, Joseph E. Gonzalez, Trevor Darrell","Current open-source Large Multimodal Models (LMMs) excel at tasks such as open-vocabulary language grounding and segmentation but can suffer under false premises when queries imply the existence of something that is not actually present in the image. We observe that existing methods that fine-tune an LMM to segment images significantly degrade their ability to reliably determine (""see"") if an object is present and to interact naturally with humans (""say"") a form of catastrophic forgetting. In this work we propose a cascading and joint training approach for LMMs to solve this task avoiding catastrophic forgetting of previous skills. Our resulting model can ""see"" by detecting whether objects are present in an image ""say"" by telling the user if they are not proposing alternative queries or correcting semantic errors in the query and finally ""segment"" by outputting the mask of the desired objects if they exist. Additionally we introduce a novel False Premise Correction benchmark dataset an extension of existing RefCOCO(+/g) referring segmentation datasets (which we call FP-RefCOCO(+/g)). The results show that our method not only detects false premises up to 55% better than existing approaches but under false premise conditions produces relative cIOU improvements of more than 31% over baselines and produces natural language feedback judged helpful up to 67% of the time.",,https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_See_Say_and_Segment_Teaching_LMMs_to_Overcome_False_Premises_CVPR_2024_paper.pdf,,2,当前的开源大型多模态模型（LMMs）在诸如开放词汇语言基础和分割等任务上表现出色，但当查询暗示图像中不存在实际存在的内容时，可能会遭受误导。我们观察到，现有的对LMM进行微调以对图像进行分割的方法会显著地降低它们可靠地确定对象是否存在并与人类自然交互的能力（“看”“说”）。在这项工作中，我们提出了一种级联和联合训练的方法，用于解决LMM在此任务中避免以前技能的灾难性遗忘。我们的模型通过检测图像中是否存在对象来“看”，通过告诉用户对象是否不存在、提出替代查询或纠正查询中的语义错误来“说”，最终通过输出所需对象的掩膜来“分割”。此外，我们还引入了一个新颖的虚假前提校正基准数据集，这是对现有的RefCOCO（+/
48,Segment and Caption Anything,CVPR,2024,"Xiaoke Huang, Jianfeng Wang, Yansong Tang, Zheng Zhang, Han Hu, Jiwen Lu, Lijuan Wang, Zicheng Liu",We propose a method to efficiently equip the Segment Anything Model (SAM) with the ability to generate regional captions. SAM presents strong generalizability to segment anything while is short for semantic understanding. By introducing a lightweight query-based feature mixer we align the region-specific features with the embedding space of language models for later caption generation. As the number of trainable parameters is small (typically in the order of tens of millions) it costs less computation less memory usage and less communication bandwidth resulting in both fast and scalable training. To address the scarcity problem of regional caption data we propose to first pre-train our model on objection detection and segmentation tasks. We call this step weak supervision pretraining since the pretraining data only contains category names instead of full-sentence descriptions. The weak supervision pretraining allows us to leverage many publicly available object detection and segmentation datasets. We conduct extensive experiments to demonstrate the superiority of our method and validate each design choice. This work serves as a stepping stone towards scaling up regional captioning data and sheds light on exploring efficient ways to augment SAM with regional semantics.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Huang_Segment_and_Caption_Anything_CVPR_2024_paper.pdf,,2,我们提出了一种方法，可以有效地使段落任意模型（SAM）具备生成区域描述的能力。SAM对于分段的任何内容具有强大的普适性，同时也缩写为语义理解。通过引入轻量级的基于查询的特征混合器，我们将区域特定的特征与语言模型的嵌入空间对齐，以便后续生成描述。由于可训练参数的数量较少（通常在数千万量级），这种方法计算开销小，内存使用少，通信带宽少，既可训练速度快又可扩展性强。为解决区域描述数据稀缺的问题，我们建议首先在目标检测和分割任务上对模型进行预训练。我们称这个步骤为弱监督预训练，因为预训练数据仅包含类别名称，而不是完整的句子描述。弱监督预训练能够利用许多公开可
49,InstaGen: Enhancing Object Detection by Training on Synthetic Dataset,CVPR,2024,"Chengjian Feng, Yujie Zhong, Zequn Jie, Weidi Xie, Lin Ma",In this paper we present a novel paradigm to enhance the ability of object detector e.g. expanding categories or improving detection performance by training on syn- thetic dataset generated from diffusion models. Specifically we integrate an instance-level grounding head into a pre- trained generative diffusion model to augment it with the ability of localising instances in the generated images. The grounding head is trained to align the text embedding of category names with the regional visual feature of the diffusion model using supervision from an off-the-shelf object detector and a novel self-training scheme on (novel) categories not covered by the detector. We conduct thorough experiments to show that this enhanced version of diffusion model termed as InstaGen can serve as a data synthesizer to enhance object detectors by training on its generated samples demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 ? 5.2 AP) scenarios.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Feng_InstaGen_Enhancing_Object_Detection_by_Training_on_Synthetic_Dataset_CVPR_2024_paper.pdf,,1,本文介绍了一种增强目标检测器能力的新范式，例如通过在合成数据集上训练来扩展类别或提高检测性能。具体来说，我们将一个实例级别的定位头结合到一个预训练的生成扩散模型中，以赋予其在生成的图像中定位实例的能力。定位头通过监督学习从一个现成的目标检测器和一种新颖的自训练方案中学习，用于将类别名称的文本嵌入与扩散模型的区域视觉特征对齐。我们进行了彻底的实验，表明这种增强版扩散模型称为InstaGen可以作为数据合成器，通过在生成的样本上训练来增强目标检测器，展现出在开放词汇和数据稀疏情况下胜过现有先进方法的出色性能。
50,VkD: Improving Knowledge Distillation using Orthogonal Projections,CVPR,2024,"Roy Miles, Ismail Elezi, Jiankang Deng",Knowledge distillation is an effective method for training small and efficient deep learning models. However the efficacy of a single method can degenerate when transferring to other tasks modalities or even other architectures. To address this limitation we propose a novel constrained feature distillation method. This method is derived from a small set of core principles which results in two emerging components: an orthogonal projection and a task-specific normalisation. Equipped with both of these components our transformer models can outperform all previous methods on ImageNet and reach up to a 4.4% relative improvement over the previous state-of-the-art methods. To further demonstrate the generality of our method we apply it to object detection and image generation whereby we obtain consistent and substantial performance improvements over state-of-the-art. Code and models are publicly available.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Miles_VkD_Improving_Knowledge_Distillation_using_Orthogonal_Projections_CVPR_2024_paper.pdf,,2,知识蒸馏是训练小型高效深度学习模型的有效方法。然而，当将单一方法转移到其他任务、模态甚至其他架构时，其效力可能会减弱。为了解决这一局限性，我们提出了一种新颖的受限特征蒸馏方法。该方法源自一小组核心原则，产生了两个新的组成部分：正交投影和任务特定的归一化。配备这两个组件后，我们的变压器模型在ImageNet上表现出色，并相对于先前最先进的方法取得了高达4.4%的相对改进。为了进一步展示我们方法的普适性，我们将其应用于目标检测和图像生成领域，从而在先进技术上取得了一致且显著的性能提升。代码和模型均已公开可用。
51,SAI3D: Segment Any Instance in 3D Scenes,CVPR,2024,"Yingda Yin, Yuzheng Liu, Yang Xiao, Daniel Cohen-Or, Jingwei Huang, Baoquan Chen",Advancements in 3D instance segmentation have traditionally been tethered to the availability of annotated datasets limiting their application to a narrow spectrum of object categories. Recent efforts have sought to harness vision-language models like CLIP for open-set semantic reasoning yet these methods struggle to distinguish between objects of the same categories and rely on specific prompts that are not universally applicable. In this paper we introduce SAI3D a novel zero-shot 3D instance segmentation approach that synergistically leverages geometric priors and semantic cues derived from Segment Anything Model (SAM). Our method partitions a 3D scene into geometric primitives which are then progressively merged into 3D instance segmentations that are consistent with the multi-view SAM masks. Moreover we design a hierarchical region-growing algorithm with a dynamic thresholding mechanism which largely improves the robustness of fine-grained 3D scene parsing. Empirical evaluations on ScanNet Matterport3D and the more challenging ScanNet++ datasets demonstrate the superiority of our approach. Notably SAI3D outperforms existing open-vocabulary baselines and even surpasses fully-supervised methods in class-agnostic segmentation on ScanNet++. Our project page is at https://yd-yin.github.io/SAI3D/.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Yin_SAI3D_Segment_Any_Instance_in_3D_Scenes_CVPR_2024_paper.pdf,,1,本文介绍了一种新颖的零样本3D实例分割方法SAI3D，该方法同时利用了几何先验和来自Segment Anything Model (SAM)的语义线索。该方法将3D场景分区为几何基元，然后逐步将其合并为与多视角SAM蒙版一致的3D实例分割。此外，作者设计了一种具有动态阈值机制的分层区域生长算法，大大提高了细粒度3D场景解析的鲁棒性。在ScanNet、Matterport3D和更具挑战性的ScanNet++数据集上的实证评估表明了我们方法的优越性。值得注意的是，SAI3D在ScanNet++上优于现有的开放词汇基线方法，甚至超过了在类别无关分割上的完全监督方法。
52,Prompt-Driven Referring Image Segmentation with Instance Contrasting,CVPR,2024,"Chao Shang, Zichen Song, Heqian Qiu, Lanxiao Wang, Fanman Meng, Hongliang Li",Referring image segmentation (RIS) aims to segment the target referent described by natural language. Recently large-scale pre-trained models e.g. CLIP and SAM have been successfully applied in many downstream tasks but they are not well adapted to RIS task due to inter-task differences. In this paper we propose a new prompt-driven framework named Prompt-RIS which bridges CLIP and SAM end-to-end and transfers their rich knowledge and powerful capabilities to RIS task through prompt learning. To adapt CLIP to pixel-level task we first propose a Cross-Modal Prompting method which acquires more comprehensive vision-language interaction and fine-grained text-to-pixel alignment by performing bidirectional prompting. Then the prompt-tuned CLIP generates masks points and text prompts for SAM to generate more accurate mask predictions. Moreover we further propose Instance Contrastive Learning to improve the model's discriminability to different instances and robustness to diverse languages describing the same instance. Extensive experiments demonstrate that the performance of our method outperforms the state-of-the-art methods consistently in both general and open-vocabulary settings.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Shang_Prompt-Driven_Referring_Image_Segmentation_with_Instance_Contrasting_CVPR_2024_paper.pdf,,2,本文提出了一种名为Prompt-RIS的新型提示驱动框架，旨在将CLIP和SAM的丰富知识和强大功能通过提示学习传输到RIS任务中。通过跨模态提示方法和实例对比学习，该方法在像素级任务上将CLIP适应到SAM模型中，提高了模型对不同实例的可区分性和对描述同一实例的多种语言的鲁棒性。 大量实验证明，这种方法在一般和开放词汇设置下始终优于最先进的方法。
53,SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design,CVPR,2024,"Seokju Yun, Youngmin Ro",Recently efficient Vision Transformers have shown great performance with low latency on resource-constrained devices. Conventionally they use 4x4 patch embeddings and a 4-stage structure at the macro level while utilizing sophisticated attention with multi-head configuration at the micro level. This paper aims to address computational redundancy at all design levels in a memory-efficient manner. We discover that using larger-stride patchify stem not only reduces memory access costs but also achieves competitive performance by leveraging token representations with reduced spatial redundancy from the early stages. Furthermore our preliminary analyses suggest that attention layers in the early stages can be substituted with convolutions and several attention heads in the latter stages are computationally redundant. To handle this we introduce a single-head attention module that inherently prevents head redundancy and simultaneously boosts accuracy by parallelly combining global and local information. Building upon our solutions we introduce SHViT a Single-Head Vision Transformer that obtains the state-of-the-art speed-accuracy tradeoff. For example on ImageNet-1k our SHViT-S4 is 3.3x 8.1x and 2.4x faster than MobileViTv2x1.0 on GPU CPU and iPhone12 mobile device respectively while being 1.3% more accurate. For object detection and instance segmentation on MS COCO using Mask-RCNN head our model achieves performance comparable to FastViT-SA12 while exhibiting 3.8x and 2.0x lower backbone latency on GPU and mobile device respectively.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Yun_SHViT_Single-Head_Vision_Transformer_with_Memory_Efficient_Macro_Design_CVPR_2024_paper.pdf,,1,最近，高效的视觉Transformer在资源受限设备上展现出了出色的低延迟性能。传统上，它们在宏观级别使用4x4的patch嵌入和4阶段结构，同时在微观级别利用复杂的多头注意力。本文旨在以内存高效的方式解决所有设计层面上的计算冗余。我们发现，使用更大步长的patchify起始结构不仅降低了内存访问成本，还通过利用来自早期阶段减少空间冗余的标记表示实现了有竞争力的性能。此外，我们的初步分析表明，早期阶段的注意力层可以用卷积替换，而后期阶段的几个注意力头是计算冗余的。为了解决这个问题，我们引入了一个单头注意力模块，可以从根本上防止头部冗余，并通过同时结合全局和局部信息来提高准确性。基于我们的解决方案，
54,DIOD: Self-Distillation Meets Object Discovery,CVPR,2024,"Sandra Kara, Hejer Ammar, Julien Denize, Florian Chabot, Quoc-Cuong Pham",Instance segmentation demands substantial labeling resources. This has prompted increased interest to explore the object discovery task as an unsupervised alternative. In particular promising results were achieved in localizing instances using motion supervision only. However the motion signal introduces complexities due to its inherent noise and sparsity which constrains the effectiveness of current methodologies. In the present paper we propose DIOD (self DIstillation meets Object Discovery) the first method that places the motion-guided object discovery within a framework of continuous improvement through knowledge distillation providing solutions to existing limitations (i) DIOD robustly eliminates the noise present in the exploited motion maps providing accurate motion-supervision (ii) DIOD leverages the discovered objects within an iterative pseudo-labeling framework enriching the initial motion-supervision with static objects which results in a cost-efficient increase in performance. Through experiments on synthetic and real-world datasets we demonstrate the benefits of bridging the gap between object discovery and distillation by significantly improving the state-of-the-art. This enhancement is also sustained across other demanding metrics so far reserved for supervised tasks.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Kara_DIOD_Self-Distillation_Meets_Object_Discovery_CVPR_2024_paper.pdf,,2,本文讨论了实例分割对大量标注资源的需求，导致人们对把对象发现任务作为一种无监督替代方案的兴趣增加。特别是在仅使用运动监督时取得了定位实例的有希望结果。然而，由于运动信号本身的噪音和稀疏性，导致了复杂性的引入，从而限制了当前方法的效力。在本文中，我们提出了DIOD（自我蒸馏遇见对象发现），这是第一种将基于运动引导的对象发现置于连续改进框架中通过知识蒸馏提供现有限制的解决方法的方法。DIOD创新地消除了运动图中存在的噪音，提供准确的运动监督；DIOD利用在迭代伪标记框架中发现的对象，丰富了初始运动监督，静态对象的加入实现了性能的成本高效提升。通过对合成和实
55,Open-Vocabulary Segmentation with Semantic-Assisted Calibration,CVPR,2024,"Yong Liu, Sule Bai, Guanbin Li, Yitong Wang, Yansong Tang",This paper studies open-vocabulary segmentation (OVS) through calibrating in-vocabulary and domain-biased embedding space with generalized contextual prior of CLIP. As the core of open-vocabulary understanding alignment of visual content with the semantics of unbounded text has become the bottleneck of this field. To address this challenge recent works propose to utilize CLIP as an additional classifier and aggregate model predictions with CLIP classification results. Despite their remarkable progress performance of OVS methods in relevant scenarios is still unsatisfactory compared with supervised counterparts. We attribute this to the in-vocabulary embedding and domain-biased CLIP prediction. To this end we present a Semantic-assisted CAlibration Network (SCAN). In SCAN we incorporate generalized semantic prior of CLIP into proposal embedding to avoid collapsing on known categories. Besides a contextual shift strategy is applied to mitigate the lack of global context and unnatural background noise. With above designs SCAN achieves state-of-the-art performance on all popular open-vocabulary segmentation benchmarks. Furthermore we also focus on the problem of existing evaluation system that ignores semantic duplication across categories and propose a new metric called Semantic-Guided IoU (SG-IoU).,,https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Open-Vocabulary_Segmentation_with_Semantic-Assisted_Calibration_CVPR_2024_paper.pdf,,2,本文通过使用CLIP的广义上下文先验，对“开放词汇分割（OVS）”进行研究。在视觉内容与无限文本语义对齐的开放词汇理解中，相对于监督对照，OVS方法的性能仍不尽如人意。我们认为这是由于词汇表内嵌入和领域偏置的CLIP预测。为此，我们提出了一种“语义辅助校准网络（SCAN）”，将CLIP的广义语义先验纳入到提议的嵌入中，避免对已知类别进行折叠。此外，我们还采用上下文移位策略来减轻全局背景噪音的缺失。通过上述设计，SCAN在所有流行的开放词汇分割基准上取得最先进的性能。此外，我们还关注评估系统存在的语义重复跨类别的问题，并提出了一种名为“语义引导IoU（SG-IoU）”的
56,MonoCD: Monocular 3D Object Detection with Complementary Depths,CVPR,2024,"Longfei Yan, Pei Yan, Shengzhou Xiong, Xuanyu Xiang, Yihua Tan",Monocular 3D object detection has attracted widespread attention due to its potential to accurately obtain object 3D localization from a single image at a low cost. Depth estimation is an essential but challenging subtask of monocular 3D object detection due to the ill-posedness of 2D to 3D mapping. Many methods explore multiple local depth clues such as object heights and keypoints and then formulate the object depth estimation as an ensemble of multiple depth predictions to mitigate the insufficiency of single-depth information. However the errors of existing multiple depths tend to have the same sign which hinders them from neutralizing each other and limits the overall accuracy of combined depth. To alleviate this problem we propose to increase the complementarity of depths with two novel designs. First we add a new depth prediction branch named complementary depth that utilizes global and efficient depth clues from the entire image rather than the local clues to reduce the correlation of depth predictions. Second we propose to fully exploit the geometric relations between multiple depth clues to achieve complementarity in form. Benefiting from these designs our method achieves higher complementarity. Experiments on the KITTI benchmark demonstrate that our method achieves state-of-the-art performance without introducing extra data. In addition complementary depth can also be a lightweight and plug-and-play module to boost multiple existing monocular 3d object detectors. Code is available at https://github.com/elvintanhust/MonoCD.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_MonoCD_Monocular_3D_Object_Detection_with_Complementary_Depths_CVPR_2024_paper.pdf,https://github.com/elvintanhust/MonoCD,2,单目三维物体检测因可以从单个图像准确获取物体三维定位而引起广泛关注。深度估计是单目三维物体检测的一个关键且具有挑战性的子任务，由于2D到3D映射的不适定性。许多方法探索多个局部深度线索，如物体高度和关键点，然后将物体深度估计构建为多个深度预测的整合，以缓解单一深度信息的不足。然而，现有多个深度的误差往往具有相同的符号，这阻碍了它们彼此的抵消，限制了组合深度的整体准确性。为了解决这个问题，我们提出了两种新颖设计以增加深度的互补性。首先，我们添加了一个名为互补深度的新深度预测分支，利用整个图像的全局和高效深度线索，而不是局部线索
57,OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation,CVPR,2024,"Ganlong Zhao, Guanbin Li, Weikai Chen, Yizhou Yu",Recent advances in Iterative Vision-and-Language Navigation(IVLN) introduce a more meaningful and practical paradigm of VLN by maintaining the agent's memory across tours of scenes. Although the long-term memory aligns better with the persistent nature of the VLN task it poses more challenges on how to utilize the highly unstructured navigation memory with extremely sparse supervision. Towards this end we propose OVER-NAV which aims to go over and beyond the current arts of IVLN techniques. In particular we propose to incorporate LLMs and open-vocabulary detectors to distill key information and establish correspondence between multi-modal signals. Such a mechanism introduces reliable cross-modal supervision and enables on-the-fly generalization to unseen scenes without the need of extra annotation and re-training. To fully exploit the interpreted navigation data we further introduce a structured representation coded Omnigraph to effectively integrate multi-modal information along the tour. Accompanied with a novel omnigraph fusion mechanism OVER-NAV is able to extract the most relevant knowledge from omnigraph for a more accurate navigating action. In addition OVER-NAV seamlessly supports both discrete and continuous environments under a unified framework. We demonstrate the superiority of OVER-NAV in extensive experiments.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_OVER-NAV_Elevating_Iterative_Vision-and-Language_Navigation_with_Open-Vocabulary_Detection_and_StructurEd_CVPR_2024_paper.pdf,,2,最近的迭代视觉与语言导航（IVLN）领域取得了重大进展，通过在场景之间保持Agent的记忆，引入了更具意义和实用的VLN范式。尽管长期记忆更符合VLN任务的持久性特征，但如何利用高度非结构化的导航记忆与极度稀疏的监督之间提出了更多挑战。为此，我们提出了OVER-NAV，旨在超越当前IVLN技术的先进水平。具体而言，我们提出将LLMs和开放词汇探测器纳入其中，以提炼关键信息并建立多模态信号之间的对应关系。这种机制引入了可靠的跨模态监督，并实现了对未见场景的实时泛化，无需额外标注和重新训练。为了充分利用解释后的导航数据，我们进一步引入了结构化表示编码的全图图表，以有效整合整个行程中
58,Generating Enhanced Negatives for Training Language-Based Object Detectors,CVPR,2024,"Shiyu Zhao, Long Zhao, Vijay Kumar B G, Yumin Suh, Dimitris N. Metaxas, Manmohan Chandraker, Samuel Schulter",The recent progress in language-based open-vocabulary object detection can be largely attributed to finding better ways of leveraging large-scale data with free-form text annotations. Training such models with a discriminative objective function has proven successful but requires good positive and negative samples. However the free-form nature and the open vocabulary of object descriptions make the space of negatives extremely large. Prior works randomly sample negatives or use rule-based techniques to build them. In contrast we propose to leverage the vast knowledge built into modern generative models to automatically build negatives that are more relevant to the original data. Specifically we use large-language-models to generate negative text descriptions and text-to-image diffusion models to also generate corresponding negative images. Our experimental analysis confirms the relevance of the generated negative data and its use in language-based detectors improves performance on two complex benchmarks. Code is available at https://github.com/xiaofeng94/Gen-Enhanced-Negs.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Generating_Enhanced_Negatives_for_Training_Language-Based_Object_Detectors_CVPR_2024_paper.pdf,https://github.com/xiaofeng94/Gen-Enhanced-Negs,2,语言驱动的开放词汇目标检测取得了最近的进展，主要归功于找到更好的方法来利用带有自由形式文本标注的大规模数据。训练这种模型，使用判别性目标函数已被证明是成功的，但需要良好的正负样本。然而，物体描述的自由形式和开放词汇的性质使得负样本空间非常庞大。先前的工作随机采样负样本或使用基于规则的技术来构建它们。相比之下，我们提议利用现代生成模型中构建的丰富知识来自动构建更与原始数据相关的负样本。具体来说，我们使用大型语言模型生成负文本描述，并使用文本到图像扩散模型生成相应的负图像。我们的实验分析证实了生成的负数据的相关性，并在两个复杂基准上使用语言驱动的检测器改善了性能。源代码可在 https://
59,Depth-Aware Concealed Crop Detection in Dense Agricultural Scenes,CVPR,2024,"Liqiong Wang, Jinyu Yang, Yanfu Zhang, Fangyi Wang, Feng Zheng",Concealed Object Detection (COD) aims to identify objects visually embedded in their background. Existing COD datasets and methods predominantly focus on animals or humans ignoring the agricultural domain which often contains numerous small and concealed crops with severe occlusions. In this paper we introduce Concealed Crop Detection (CCD) which extends classic COD to agricultural domains. Experimental study shows that unimodal data provides insufficient information for CCD. To address this gap we first collect a large-scale RGB-D dataset ACOD-12K containing high-resolution crop images and depth maps. Then we propose a foundational framework named Recurrent Iterative Segmentation Network (RISNet). To tackle the challenge of dense objects we employ multi-scale receptive fields to capture objects of varying sizes thus enhancing the detection performance for dense objects. By fusing depth features our method can acquire spatial information about concealed objects to mitigate disturbances caused by intricate backgrounds and occlusions. Furthermore our model adopts a multi-stage iterative approach using predictions from each stage as gate attention to reinforce position information thereby improving the detection accuracy for small objects. Extensive experimental results demonstrate that our RISNet achieves new state-of-the-art performance on both newly proposed CCD and classic COD tasks. All resources will be available at https://github.com/Kki2Eve/RISNet.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Depth-Aware_Concealed_Crop_Detection_in_Dense_Agricultural_Scenes_CVPR_2024_paper.pdf,https://github.com/Kki2Eve/RISNet,2,这篇论文介绍了 Concealed Crop Detection (CCD)，该方法将经典的 Concealed Object Detection (COD) 扩展到农业领域。为了解决 CCD 中单模态数据提供信息不足的问题，作者首先收集了一个大规模的 RGB-D 数据集 ACOD-12K，其中包含高分辨率的作物图像和深度图。然后，作者提出了一个名为 Recurrent Iterative Segmentation Network (RISNet) 的基础框架。为了解决密集物体的挑战，他们使用多尺度感受野来捕捉不同尺寸的物体，从而增强了对密集物体的检测性能。通过融合深度特征，他们的方法可以获取有关隐蔽物体的空间信息，以减轻复杂背景和遮挡带来的干扰。此外，他们的模型采用了多阶段迭代方法，利用每个阶段的预测作为门控注意力来加强位置信息，从而提
60,SHiNe: Semantic Hierarchy Nexus for Open-vocabulary Object Detection,CVPR,2024,"Mingxuan Liu, Tyler L. Hayes, Elisa Ricci, Gabriela Csurka, Riccardo Volpi",Open-vocabulary object detection (OvOD) has transformed detection into a language-guided task empowering users to freely define their class vocabularies of interest during inference. However our initial investigation indicates that existing OvOD detectors exhibit significant variability when dealing with vocabularies across various semantic granularities posing a concern for real-world deployment. To this end we introduce Semantic Hierarchy Nexus (SHiNe) a novel classifier that uses semantic knowledge from class hierarchies. It runs offline in three steps: i) it retrieves relevant super-/sub-categories from a hierarchy for each target class; ii) it integrates these categories into hierarchy-aware sentences; iii) it fuses these sentence embeddings to generate the nexus classifier vector. Our evaluation on various detection benchmarks demonstrates that SHiNe enhances robustness across diverse vocabulary granularities achieving up to +31.9% mAP50 with ground truth hierarchies while retaining improvements using hierarchies generated by large language models. Moreover when applied to open-vocabulary classification on ImageNet-1k SHiNe improves the CLIP zero-shot baseline by +2.8% accuracy. SHiNe is training-free and can be seamlessly integrated with any off-the-shelf OvOD detector without incurring additional computational overhead during inference. The code is open source.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_SHiNe_Semantic_Hierarchy_Nexus_for_Open-vocabulary_Object_Detection_CVPR_2024_paper.pdf,,1,语义层次关联 (SHiNe) 是一种使用类层次结构中的语义知识的新型分类器，它离线运行，包括三个步骤：i) 检索每个目标类别的类层次结构中相关的超类别和子类别；ii) 将这些类别整合成基于层次结构的句子；iii) 融合这些句子嵌入以生成关联分类器向量。在各种检测基准上进行的评估表明，SHiNe提高了对不同词汇粒度的鲁棒性，当使用地面真相层次结构时，mAP50 可达 +31.9%，同时保留了使用大型语言模型生成的层次结构的改进。此外，当应用于ImageNet-1k的开放词汇分类时，SHiNe将CLIP零样本基准的准确率提高了 +2.8%。SHiNe无需训练，可与任何现成的OvOD检测器无缝集成，而
61,RadSimReal: Bridging the Gap Between Synthetic and Real Data in Radar Object Detection With Simulation,CVPR,2024,"Oded Bialer, Yuval Haitman",Object detection in radar imagery with neural networks shows great potential for improving autonomous driving. However obtaining annotated datasets from real radar images crucial for training these networks is challenging especially in scenarios with long-range detection and adverse weather and lighting conditions where radar performance excels. To address this challenge we present RadSimReal an innovative physical radar simulation capable of generating synthetic radar images with accompanying annotations for various radar types and environmental conditions all without the need for real data collection. Remarkably our findings demonstrate that training object detection models on RadSimReal data and subsequently evaluating them on real-world data produce performance levels comparable to models trained and tested on real data from the same dataset and even achieves better performance when testing across different real datasets. RadSimReal offers advantages over other physical radar simulations that it does not necessitate knowledge of the radar design details which are often not disclosed by radar suppliers and has faster run-time. This innovative tool has the potential to advance the development of computer vision algorithms for radar-based autonomous driving applications.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Bialer_RadSimReal_Bridging_the_Gap_Between_Synthetic_and_Real_Data_in_CVPR_2024_paper.pdf,,1,通过神经网络在雷达图像中进行目标检测在改进自动驾驶方面具有巨大潜力。然而，从真实雷达图像中获取标注数据集对于训练这些网络至关重要，在长距离检测和恶劣的天气和光照条件下尤为具有挑战性，雷达的性能在这些情况下表现出色。为了解决这一挑战，我们提出了一种创新的物理雷达模拟工具RadSimReal，能够生成带有各种雷达类型和环境条件的合成雷达图像，并附带注释，而无需真实数据收集。显著的是，我们的研究结果表明，在RadSimReal数据上训练目标检测模型，然后在真实数据上评估，产生的性能水平与在相同数据集上训练和测试的模型相媲美，甚至在跨不同真实数据集测试时表现更好。RadSimReal相对于其他物
62,Bootstrapping Autonomous Driving Radars with Self-Supervised Learning,CVPR,2024,"Yiduo Hao, Sohrab Madani, Junfeng Guan, Mohammed Alloulah, Saurabh Gupta, Haitham Hassanieh",The perception of autonomous vehicles using radars has attracted increased research interest due its ability to operate in fog and bad weather. However training radar models is hindered by the cost and difficulty of annotating large-scale radar data. To overcome this bottleneck we propose a self-supervised learning framework to leverage the large amount of unlabeled radar data to pre-train radar-only embeddings for self-driving perception tasks. The proposed method combines radar-to-radar and radar-to-vision contrastive losses to learn a general representation from unlabeled radar heatmaps paired with their corresponding camera images. When used for downstream object detection we demonstrate that the proposed self-supervision framework can improve the accuracy of state-of-the-art supervised baselines by 5.8% in mAP. Code is available at https://github.com/yiduohao/Radical.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Hao_Bootstrapping_Autonomous_Driving_Radars_with_Self-Supervised_Learning_CVPR_2024_paper.pdf,https://github.com/yiduohao/Radical,2,自主车辆感知使用雷达技术已经引起了研究人员的增加关注，因为它能够在雾和恶劣天气下运行。然而，训练雷达模型受到昂贵和难以注释大规模雷达数据的限制。为了克服这一瓶颈，我们提出了一个自监督学习框架，利用大量未标记的雷达数据为自动驾驶感知任务预训练仅雷达嵌入。所提出的方法结合了雷达到雷达和雷达到视觉对比损失，从未标记的雷达热图以及相应的摄像头图像中学习通用表示。在用于下游目标检测时，我们证明了所提出的自我监督框架可将最先进的有监督基准的准确性提高了5.8% 的mAP。该代码可在 https://github.com/yiduohao/Radical 上找到。
63,Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation,CVPR,2024,"Yunhao Ge, Xiaohui Zeng, Jacob Samuel Huffman, Tsung-Yi Lin, Ming-Yu Liu, Yin Cui",Existing automatic captioning methods for visual content face challenges such as lack of detail content hallucination and poor instruction following. In this work we propose VisualFactChecker (VFC) a flexible training-free pipeline that generates high-fidelity and detailed captions for both 2D images and 3D objects. VFC consists of three steps: 1) proposal where image-to-text captioning models propose multiple initial captions; 2) verification where a large language model (LLM) utilizes tools such as object detection and VQA models to fact-check proposed captions; 3) captioning where an LLM generates the final caption by summarizing caption proposals and the fact check verification results. In this step VFC can flexibly generate captions in various styles following complex instructions. We conduct comprehensive captioning evaluations using four metrics: 1) CLIP-Score for image-text similarity; 2) CLIP-Image-Score for measuring the image-image similarity between the original and the reconstructed image generated by a text-to-image model using the caption. 3) human study on Amazon Mechanical Turk; 4) GPT-4V for fine-grained evaluation. Evaluation results show that VFC outperforms state-of-the-art open-sourced captioning methods for 2D images on the COCO dataset and 3D assets on the Objaverse dataset. Our study demonstrates that by combining open-source models into a pipeline we can attain captioning capability comparable to proprietary models such as GPT-4V despite being over 10x smaller in model size.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Ge_Visual_Fact_Checker_Enabling_High-Fidelity_Detailed_Caption_Generation_CVPR_2024_paper.pdf,,2,本研究提出了VisualFactChecker （VFC），这是一个灵活的无需训练的管道，用于生成2D图像和3D对象的高保真和详细说明。VFC包括三个步骤：1）提议，其中图像到文本字幕模型提出多个初始字幕；2）验证，一个大型语言模型（LLM）利用对象检测和VQA模型等工具对提出的字幕进行事实检查；3）字幕，LLM通过总结字幕提议和事实检查验证结果生成最终字幕。在这一步中，VFC可以灵活生成各种风格的字幕，遵循复杂的说明。我们使用四个度量标准进行了全面的字幕评估：1）用于衡量图像文本相似性的CLIP-Score；2）用于测量由文本到图像模型生成的原始图像和重建图像之间的图像相似性的CLIP-Image-Score。3）亚马逊机械土耳其
64,SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes,CVPR,2024,"Alexandros Delitzas, Ayca Takmaz, Federico Tombari, Robert Sumner, Marc Pollefeys, Francis Engelmann",Existing 3D scene understanding methods are heavily focused on 3D semantic and instance segmentation. However identifying objects and their parts only constitutes an intermediate step towards a more fine-grained goal which is effectively interacting with the functional interactive elements (e.g. handles knobs buttons) in the scene to accomplish diverse tasks. To this end we introduce SceneFun3D a large-scale dataset with more than 14.8k highly accurate interaction annotations for 710 high-resolution real-world 3D indoor scenes. We accompany the annotations with motion parameter information describing how to interact with these elements and a diverse set of natural language descriptions of tasks that involve manipulating them in the scene context. To showcase the value of our dataset we introduce three novel tasks namely functionality segmentation task-driven affordance grounding and 3D motion estimation and adapt existing state-of-the-art methods to tackle them. Our experiments show that solving these tasks in real 3D scenes remains challenging despite recent progress in closed-set and open-set 3D scene understanding methods.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Delitzas_SceneFun3D_Fine-Grained_Functionality_and_Affordance_Understanding_in_3D_Scenes_CVPR_2024_paper.pdf,,2,现有的3D场景理解方法主要集中在3D语义和实例分割上。然而，仅仅识别物体及其部分只构成了与更细粒度目标有效互动的中间步骤，该目标是与场景中的功能性交互元素（如手柄、旋钮、按钮）进行交互以完成各种任务。为此，我们引入了SceneFun3D，这是一个大规模数据集，包含710个高分辨率真实世界3D室内场景的14.8k多个高度准确的交互注释。我们还附带运动参数信息，描述如何与这些元素进行交互，以及涉及在场景环境中操作它们的各种自然语言任务描述。为展示我们数据集的价值，我们提出了三项新任务，即功能分割任务驱动的契合性基础以及3D运动估计，并改编现有的最先进方法来处理它们。我们的实验证明，尽管在封闭集和开放集3D
65,Learning Transferable Negative Prompts for Out-of-Distribution Detection,CVPR,2024,"Tianqi Li, Guansong Pang, Xiao Bai, Wenjun Miao, Jin Zheng",Existing prompt learning methods have shown certain capabilities in Out-of-Distribution (OOD) detection but the lack of OOD images in the target dataset in their training can lead to mismatches between OOD images and In-Distribution (ID) categories resulting in a high false positive rate. To address this issue we introduce a novel OOD detection method named 'NegPrompt' to learn a set of negative prompts each representing a negative connotation of a given class label for delineating the boundaries between ID and OOD images. It learns such negative prompts with ID data only without any reliance on external outlier data. Further current methods assume the availability of samples of all ID classes rendering them ineffective in open-vocabulary learning scenarios where the inference stage can contain novel ID classes not present during training. In contrast our learned negative prompts are transferable to novel class labels. Experiments on various ImageNet benchmarks show that NegPrompt surpasses state-of-the-art prompt-learning-based OOD detection methods and maintains a consistent lead in hard OOD detection in closed- and open-vocabulary classification scenarios. Code is available at https://github.com/mala-lab/negprompt.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Learning_Transferable_Negative_Prompts_for_Out-of-Distribution_Detection_CVPR_2024_paper.pdf,https://github.com/mala-lab/negprompt,2,现有的提示学习方法在Out-of-Distribution (OOD)检测方面表现出一定的能力，但是在它们的训练中目标数据集中缺乏OOD图像可能会导致OOD图像与In-Distribution (ID)类别之间出现不匹配，从而导致高误报率。为了解决这个问题，我们引入了一种名为'NegPrompt'的新颖OOD检测方法，用于学习一组负面提示，每个提示代表给定类标的负面含义，以划分ID和OOD图像之间的边界。它仅使用ID数据学习这些负面提示，而无需依赖外部异常值数据。此外，当前方法假定所有ID类别的样本都是可用的，使它们在开放词汇学习场景中无效，推理阶段可能包含在训练期间不存在的新ID类别。相比之下，我们学到的负面提示是可转移的到新的类别标签上。在各种ImageNet基准测试上的实验证明
66,Exploring Region-Word Alignment in Built-in Detector for Open-Vocabulary Object Detection,CVPR,2024,"Heng Zhang, Qiuyu Zhao, Linyu Zheng, Hao Zeng, Zhiwei Ge, Tianhao Li, Sulong Xu",Open-vocabulary object detection aims to detect novel categories that are independent from the base categories used during training. Most modern methods adhere to the paradigm of learning vision-language space from a large-scale multi-modal corpus and subsequently transferring the acquired knowledge to off-the-shelf detectors like Faster-RCNN. However information attenuation or destruction may occur during the process of knowledge transfer due to the domain gap hampering the generalization ability on novel categories. To mitigate this predicament in this paper we present a novel framework named BIND standing for Bulit-IN Detector to eliminate the need for module replacement or knowledge transfer to off-the-shelf detectors. Specifically we design a two-stage training framework with an Encoder-Decoder structure. In the first stage an image-text dual encoder is trained to learn region-word alignment from a corpus of image-text pairs. In the second stage a DETR-style decoder is trained to perform detection on annotated object detection datasets. In contrast to conventional manually designed non-adaptive anchors which generate numerous redundant proposals we develop an anchor proposal network that generates anchor proposals with high likelihood based on candidates adaptively thereby substantially improving detection efficiency. Experimental results on two public benchmarks COCO and LVIS demonstrate that our method stands as a state-of-the-art approach for open-vocabulary object detection.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Exploring_Region-Word_Alignment_in_Built-in_Detector_for_Open-Vocabulary_Object_Detection_CVPR_2024_paper.pdf,,1,开放词汇的目标检测旨在检测与训练中使用的基本类别独立的新颖类别。大多数现代方法遵循从大规模多模态语料库中学习视觉语言空间的范式，并随后将所获知识转移到像Faster-RCNN这样的现成检测器的过程。然而，在知识转移过程中可能会发生信息衰减或破坏，由于领域差异阻碍了对新颖类别的泛化能力。为了缓解这种困境，在本文中我们提出了一种名为BIND的新颖框架，它消除了需要替换模块或将知识转移到现成检测器的需求。具体而言，我们设计了一个具有编码器-解码器结构的两阶段训练框架。在第一阶段，训练图像文本双编码器以从图像文本对的语料库中学习区域-单词
67,Multi-agent Collaborative Perception via Motion-aware Robust Communication Network,CVPR,2024,"Shixin Hong, Yu Liu, Zhi Li, Shaohui Li, You He",Collaborative perception allows for information sharing between multiple agents such as vehicles and infrastructure to obtain a comprehensive view of the environment through communication and fusion. Current research on multi-agent collaborative perception systems often assumes ideal communication and perception environments and neglects the effect of real-world noise such as pose noise motion blur and perception noise. To address this gap in this paper we propose a novel motion-aware robust communication network (MRCNet) that mitigates noise interference and achieves accurate and robust collaborative perception. MRCNet consists of two main components: multi-scale robust fusion (MRF) addresses pose noise by developing cross-semantic multi-scale enhanced aggregation to fuse features of different scales while motion enhanced mechanism (MEM) captures motion context to compensate for information blurring caused by moving objects. Experimental results on popular collaborative 3D object detection datasets demonstrate that MRCNet outperforms competing methods in noisy scenarios with improved perception performance using less bandwidth.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Hong_Multi-agent_Collaborative_Perception_via_Motion-aware_Robust_Communication_Network_CVPR_2024_paper.pdf,,2,协作感知允许多个代理（如车辆和基础设施）之间共享信息，通过通信和融合获得对环境的综合认知。现有关于多代理协作感知系统的研究通常假设理想的通信和感知环境，忽略了现实世界中的姿势噪声、运动模糊和感知噪声等噪声影响。为填补这一空白，本文提出了一种新颖的具有运动感知性的鲁棒通信网络（MRCNet），以减轻噪声干扰，实现准确和鲁棒的协作感知。MRCNet包括两个主要组成部分：多尺度鲁棒融合（MRF）通过开发跨语义多尺度增强聚合来解决姿势噪声，融合不同尺度的特征；而增强运动机制（MEM）捕获运
68,Open-World Human-Object Interaction Detection via Multi-modal Prompts,CVPR,2024,"Jie Yang, Bingliang Li, Ailing Zeng, Lei Zhang, Ruimao Zhang",In this paper we develop MP-HOI a powerful Multi-modal Prompt-based HOI detector designed to leverage both textual descriptions for open-set generalization and visual exemplars for handling high ambiguity in descriptions realizing HOI detection in the open world. Specifically it integrates visual prompts into existing language-guided-only HOI detectors to handle situations where textual descriptions face difficulties in generalization and to address complex scenarios with high interaction ambiguity. To facilitate MP-HOI training we build a large-scale HOI dataset named Magic-HOI which gathers six existing datasets into a unified label space forming over 186K images with 2.4K objects 1.2K actions and 20K HOI interactions. Furthermore to tackle the long-tail issue within the Magic-HOI dataset we introduce an automated pipeline for generating realistically annotated HOI images and present SynHOI a high-quality synthetic HOI dataset containing 100K images. Leveraging these two datasets MP-HOI optimizes the HOI task as a similarity learning process between multi-modal prompts and objects/interactions via a unified contrastive loss to learn generalizable and transferable objects/interactions representations from large-scale data. MP-HOI could serve as a generalist HOI detector surpassing the HOI vocabulary of existing expert models by more than 30 times. Concurrently our results demonstrate that MP-HOI exhibits remarkable zero-shot capability in real-world scenarios and consistently achieves a new state-of-the-art performance across various benchmarks. Our project homepage is available at https://MP-HOI.github.io/.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_Open-World_Human-Object_Interaction_Detection_via_Multi-modal_Prompts_CVPR_2024_paper.pdf,,1,本文提出了一种强大的多模态基于提示的HOI检测器MP-HOI，旨在利用文本描述进行开放集泛化，并利用视觉示例处理描述中的高歧义，实现在开放世界中的HOI检测。具体来说，它将视觉提示整合到现有的仅基于语言指导的HOI检测器中，以处理文本描述面临泛化困难的情况，并解决高互动模糊的复杂情景。为了便于MP-HOI的训练，我们构建了一个名为Magic-HOI的大规模HOI数据集，该数据集将六个现有数据集聚合到一个统一的标签空间中，形成了超过18.6万张图像，包含2.4K个对象、1.2K个动作和2万个HOI交互。此外，为了解决Magic-HOI数据集中的长尾问题，我们引入了一个用于生成真实注释HOI图像的自动化流程，并提出了SynHOI，一个包含10
69,Aligning and Prompting Everything All at Once for Universal Visual Perception,CVPR,2024,"Yunhang Shen, Chaoyou Fu, Peixian Chen, Mengdan Zhang, Ke Li, Xing Sun, Yunsheng Wu, Shaohui Lin, Rongrong Ji",Vision foundation models have been explored recently to build general-purpose vision systems. However predominant paradigms driven by casting instance-level tasks as an object-word alignment bring heavy cross-modality interaction which is not effective in prompting object detection and visual grounding. Another line of work that focuses on pixel-level tasks often encounters a large annotation gap of things and stuff and suffers from mutual interference between foreground-object and background-class segmentation. In stark contrast to the prevailing methods we present APE a universal visual perception model for aligning and prompting everything all at once in an image to perform diverse tasks i.e. detection segmentation and grounding as an instance-level sentence-object matching paradigm. Specifically APE advances the convergence of detection and grounding by reformulating language-guided grounding as open-vocabulary detection which efficiently scales up model prompting to thousands of category vocabularies and region descriptions while maintaining the effectiveness of cross-modality fusion. To bridge the granularity gap of different pixel-level tasks APE equalizes semantic and panoptic segmentation to proxy instance learning by considering any isolated regions as individual instances. APE aligns vision and language representation on broad data with natural and challenging characteristics all at once without task-specific fine-tuning. The extensive experiments on over 160 datasets demonstrate that with only one-suit of weights APE outperforms (or is on par with) the state-of-the-art models proving that an effective yet universal perception for anything aligning and prompting is indeed feasible. Codes and trained models are released at https://github.com/shenyunhang/APE.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Shen_Aligning_and_Prompting_Everything_All_at_Once_for_Universal_Visual_CVPR_2024_paper.pdf,https://github.com/shenyunhang/APE,1,最近，视觉基础模型被探索用于构建通用视觉系统。然而，主导的范式将实例级任务表现为对象-词对齐，带来了跨模态交互，这对于促进目标检测和视觉定位并不有效。另一方面，关注像素级任务的工作通常遇到事物和物体之间的大标注差距，并且在前景-对象和背景-类别分割之间遭受相互干扰。与当前方法形成鲜明对比，我们提出了 APE，这是一个通用的视觉感知模型，可以一次性在图像中对齐并促进所有事物，执行多样的任务，如检测、分割和定位，作为一个实例级句子-对象匹配范式。具体来说，APE通过将语言引导的定位重新构建为开放词汇检测，推进了检测和定位的融合，通过高效扩展模型提示到成千上
70,MaxQ: Multi-Axis Query for N:M Sparsity Network,CVPR,2024,"Jingyang Xiang, Siqi Li, Junhao Chen, Zhuangzhi Chen, Tianxin Huang, Linpeng Peng, Yong Liu",N:M sparsity has received increasing attention due to its remarkable performance and latency trade-off compared with structured and unstructured sparsity. However existing N:M sparsity methods do not differentiate the relative importance of weights among blocks and leave important weights underappreciated. Besides they directly apply N:M sparsity to the whole network which will cause severe information loss. Thus they are still sub-optimal. In this paper we propose an efficient and effective Multi-Axis Query methodology dubbed as MaxQ to rectify these problems. During the training MaxQ employs a dynamic approach to generate soft N:M masks considering the weight importance across multiple axes. This method enhances the weights with more importance and ensures more effective updates. Meanwhile a sparsity strategy that gradually increases the percentage of N:M weight blocks is applied which allows the network to heal from the pruning-induced damage progressively. During the runtime the N:M soft masks can be precomputed as constants and folded into weights without causing any distortion to the sparse pattern and incurring additional computational overhead. Comprehensive experiments demonstrate that MaxQ achieves consistent improvements across diverse CNN architectures in various computer vision tasks including image classification object detection and instance segmentation. For ResNet50 with 1:16 sparse pattern MaxQ can achieve 74.6% top-1 accuracy on ImageNet and improve by over 2.8% over the state-of-the-art. Codes and checkpoints are available at https://github.com/JingyangXiang/MaxQ.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Xiang_MaxQ_Multi-Axis_Query_for_NM_Sparsity_Network_CVPR_2024_paper.pdf,https://github.com/JingyangXiang/MaxQ,2,本论文关注N:M稀疏性，与结构化和非结构化稀疏性相比，在性能和延迟的权衡方面表现出色，但现有的N:M稀疏性方法没有区分权重在块之间的相对重要性，未能充分重视重要权重。此外，它们直接将N:M稀疏性应用于整个网络，这将导致严重的信息丢失，因此仍然不够优化。本文提出了一种高效有效的多轴查询方法MaxQ来解决这些问题。在训练期间，MaxQ采用动态方法生成软N:M掩码，考虑跨多个轴的权重重要性，增强更重要的权重，并确保更有效的更新。同时，应用渐增N:M权重块百分比的稀疏策略，使网络逐渐从修剪引起的损害中恢复。在运行时，N:M软掩码可以预先计算为常量，并折叠进权
71,Hyperbolic Learning with Synthetic Captions for Open-World Detection,CVPR,2024,"Fanjie Kong, Yanbei Chen, Jiarui Cai, Davide Modolo","Open-world detection poses significant challenges as it requires the detection of any object using either object class labels or free-form texts. Existing related works often use large-scale manual annotated caption datasets for training which are extremely expensive to collect. Instead we propose to transfer knowledge from vision-language models (VLMs) to enrich the open-vocabulary descriptions automatically. Specifically we bootstrap dense synthetic captions using pre-trained VLMs to provide rich descriptions on different regions in images and incorporate these captions to train a novel detector that generalizes to novel concepts. To mitigate the noise caused by hallucination in synthetic captions we also propose a novel hyperbolic vision-language learning approach to impose a hierarchy between visual and caption embeddings. We call our detector ""HyperLearner"". We conduct extensive experiments on a wide variety of open-world detection benchmarks (COCO LVIS Object Detection in the Wild RefCOCO) and our results show that our model consistently outperforms existing state-of-the-art methods such as GLIP GLIPv2 and Grounding DINO when using the same backbone.",,https://openaccess.thecvf.com/content/CVPR2024/papers/Kong_Hyperbolic_Learning_with_Synthetic_Captions_for_Open-World_Detection_CVPR_2024_paper.pdf,,1,开放世界检测面临重大挑战，因为它需要使用对象类标签或自由文本来检测任何对象。现有相关工作通常使用大规模手动注释的标题数据集进行训练，这些数据集收集起来非常昂贵。相反，我们建议从视觉语言模型（VLMs）中转移知识，以自动丰富开放词汇的描述。具体而言，我们使用预训练的VLMs引导密集的合成标题，为图像中的不同区域提供丰富的描述，并将这些标题整合到训练一个新颖的检测器，该检测器可泛化到新概念。为了减轻合成标题中幻觉造成的噪音，我们还提出了一种新颖的双曲视觉语言学习方法，以在视觉和标题嵌入之间建立层次结构。我们将我们的检测器称为“HyperLearner”。我们在各种开放世界检
72,ViTamin: Designing Scalable Vision Models in the Vision-Language Era,CVPR,2024,"Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen",Recent breakthroughs in vision-language models (VLMs) start a new page in the vision community. The VLMs provide stronger and more generalizable feature embeddings compared to those from ImageNet-pretrained models thanks to the training on the large-scale Internet image-text pairs. However despite the amazing achievement from the VLMs vanilla Vision Transformers (ViTs) remain the default choice for the image encoder. Although pure transformer proves its effectiveness in the text encoding area it remains questionable whether it is also the case for image encoding especially considering that various types of networks are proposed on the ImageNet benchmark which unfortunately are rarely studied in VLMs. Due to small data/model scale the original conclusions of model design on ImageNet can be limited and biased. In this paper we aim at building an evaluation protocol of vision models in the vision-language era under the contrastive language-image pretraining (CLIP) framework. We provide a comprehensive way to benchmark different vision models covering their zero-shot performance and scalability in both model and training data sizes. To this end we introduce ViTamin a new vision models tailored for VLMs. ViTamin-L significantly outperforms ViT-L by 2.0% ImageNet zero-shot accuracy when using the same publicly available DataComp-1B dataset and the same OpenCLIP training scheme. ViTamin-L presents promising results on 60 diverse benchmarks including classification retrieval open-vocabulary detection and segmentation and large multi-modal models. When further scaling up the model size our ViTamin-XL with only 436M parameters attains 82.9% ImageNet zero-shot accuracy surpassing 82.0% achieved by EVA-E that has ten times more parameters (4.4B).,,https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_ViTamin_Designing_Scalable_Vision_Models_in_the_Vision-Language_Era_CVPR_2024_paper.pdf,,2,在视觉-语言模型（VLM）领域的最新突破开启了视觉社区的新篇章。VLM提供比ImageNet预训练模型更强大和更具泛化性的特征嵌入，这要归功于在大规模互联网图像-文本对上进行训练。然而，尽管VLM取得了惊人的成就，基于纯Transformer的Vision Transformers（ViTs）仍然是图像编码器的默认选择。本文旨在建立一种视觉语言时代中视觉模型的评估方案，采用对比语言-图像预训练（CLIP）框架。ViTamin-L在相同的公开数据集DataComp-1B和相同的OpenCLIP训练方案下，比ViT-L提高了2.0%的ImageNet零样本精度。ViTamin-XL只有436M参数，在进一步扩展模型规模时达到了82.9%的ImageNet零样本准确率，超过了拥有十倍参数
73,Extreme Point Supervised Instance Segmentation,CVPR,2024,"Hyeonjun Lee, Sehyun Hwang, Suha Kwak",This paper introduces a novel approach to learning instance segmentation using extreme points i.e. the topmost leftmost bottommost and rightmost points of each object. These points are readily available in the modern bounding box annotation process while offering strong clues for precise segmentation and thus allows to improve performance at the same annotation cost with box-supervised methods. Our work considers extreme points as a part of the true instance mask and propagates them to identify potential foreground and background points which are all together used for training a pseudo label generator. Then pseudo labels given by the generator are in turn used for supervised learning of our final model. On three public benchmarks our method significantly outperforms existing box-supervised methods further narrowing the gap with its fully supervised counterpart. In particular our model generates high-quality masks when a target object is separated into multiple parts where previous box-supervised methods often fail.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_Extreme_Point_Supervised_Instance_Segmentation_CVPR_2024_paper.pdf,,1,本文介绍了一种利用极端点（即每个对象的最顶部、最左侧、最底部和最右侧的点）进行实例分割学习的新方法。这些点在现代边界框注释过程中很容易获得，同时为精确分割提供了强有力的线索，因此可以在相同的注释成本下通过盒子监督方法提高性能。我们的工作将极端点视为真实实例掩模的一部分，并将其传播以识别潜在的前景和背景点，这些点一起用于训练伪标签生成器。然后，生成器给出的伪标签又被用于最终模型的监督学习。在三个公共基准测试上，我们的方法显著优于现有的盒子监督方法，并进一步缩小与完全监督方法之间的差距。特别是，我们的模型在目标对象分为多个部分时生成高质量的掩模，在这种
74,Teeth-SEG: An Efficient Instance Segmentation Framework for Orthodontic Treatment based on Multi-Scale Aggregation and Anthropic Prior Knowledge,CVPR,2024,"Bo Zou, Shaofeng Wang, Hao Liu, Gaoyue Sun, Yajie Wang, FeiFei Zuo, Chengbin Quan, Youjian Zhao",Teeth localization segmentation and labeling in 2D images have great potential in modern dentistry to enhance dental diagnostics treatment planning and population-based studies on oral health. However general instance segmentation frameworks are incompetent due to 1) the subtle differences between some teeth' shapes (e.g. maxillary first premolar and second premolar) 2) the teeth's position and shape variation across subjects and 3) the presence of abnormalities in the dentition (e.g. caries and edentulism). To address these problems we propose a ViT-based framework named TeethSEG which consists of stacked Multi-Scale Aggregation (MSA) blocks and an Anthropic Prior Knowledge (APK) layer. Specifically to compose the two modules we design 1) a unique permutation-based upscaler to ensure high efficiency while establishing clear segmentation boundaries with 2) multi-head self/cross-gating layers to emphasize particular semantics meanwhile maintaining the divergence between token embeddings. Besides we collect 3) the first open-sourced intraoral image dataset IO150K which comprises over 150k intraoral photos and all photos are annotated by orthodontists using a human-machine hybrid algorithm. Experiments on IO150K demonstrate that our TeethSEG outperforms the state-of-the-art segmentation models on dental image segmentation.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zou_Teeth-SEG_An_Efficient_Instance_Segmentation_Framework_for_Orthodontic_Treatment_based_CVPR_2024_paper.pdf,,2,牙齿在2D图像中的定位、分割和标注对现代牙科学具有巨大潜力，可以提高牙科诊断、治疗规划以及口腔健康人群研究的水平。然而，由于一些牙齿形态之间的细微差异（例如上颌第一前磨牙和第二前磨牙）、牙齿在不同个体之间的位置和形状变化以及牙列异常（例如龋齿和缺失）的存在，通用实例分割框架表现不佳。为解决这些问题，提出了一种基于ViT的框架TeethSEG，包括堆叠的多尺度聚合（MSA）块和一层人类先验知识（APK）。具体而言，为构建这两个模块，设计了独特的基于排列的上采样器，确保高效率同时建立清晰的分割边界，使用多头自
75,CN-RMA: Combined Network with Ray Marching Aggregation for 3D Indoor Object Detection from Multi-view Images,CVPR,2024,"Guanlin Shen, Jingwei Huang, Zhihua Hu, Bin Wang",This paper introduces CN-RMA a novel approach for 3D indoor object detection from multi-view images. We observe the key challenge as the ambiguity of image and 3D correspondence without explicit geometry to provide occlusion information. To address this issue CN-RMA leverages the synergy of 3D reconstruction networks and 3D object detection networks where the reconstruction network provides a rough Truncated Signed Distance Function (TSDF) and guides image features to vote to 3D space correctly in an end-to-end manner. Specifically we associate weights to sampled points of each ray through ray marching representing the contribution of a pixel in an image to corresponding 3D locations. Such weights are determined by the predicted signed distances so that image features vote only to regions near the reconstructed surface. Our method achieves state-of-the-art performance in 3D object detection from multi-view images as measured by mAP@0.25 and mAP@0.5 on the ScanNet and ARKitScenes datasets. The code and models are released at https://github.com/SerCharles/CN-RMA.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Shen_CN-RMA_Combined_Network_with_Ray_Marching_Aggregation_for_3D_Indoor_CVPR_2024_paper.pdf,https://github.com/SerCharles/CN-RMA,2,本文介绍了 CN-RMA 一种新颖的方法，用于从多视角图像中检测 3D 室内对象。我们观察到图像和 3D 对应关系的关键挑战是缺乏明确几何来提供遮挡信息。为了解决这个问题，CN-RMA 利用了 3D 重建网络和 3D 对象检测网络的协同作用，其中重建网络提供粗糙的截断有符号距离函数（TSDF），并引导图像特征正确投票到 3D 空间，以端到端的方式。具体来说，我们为每个射线的采样点关联权重，通过射线行进表示图像中的一个像素对应的 3D 位置的贡献。这些权重由预测的有符号距离决定，使图像特征仅投票给重建表面附近的区域。我们的方法在 ScanNet 和 ARKitScenes 数据集上以 mAP@0.25 和 mAP@0.5 衡量，
76,Continual Forgetting for Pre-trained Vision Models,CVPR,2024,"Hongbo Zhao, Bolin Ni, Junsong Fan, Yuxi Wang, Yuntao Chen, Gaofeng Meng, Zhaoxiang Zhang",For privacy and security concerns the need to erase unwanted information from pre-trained vision models is becoming evident nowadays. In real-world scenarios erasure requests originate at any time from both users and model owners. These requests usually form a sequence. Therefore under such a setting selective information is expected to be continuously removed from a pre-trained model while maintaining the rest. We define this problem as continual forgetting and identify two key challenges. (i) For unwanted knowledge efficient and effective deleting is crucial. (ii) For remaining knowledge the impact brought by the forgetting procedure should be minimal. To address them we propose Group Sparse LoRA (GS-LoRA). Specifically towards (i) we use LoRA modules to fine-tune the FFN layers in Transformer blocks for each forgetting task independently and towards (ii) a simple group sparse regularization is adopted enabling automatic selection of specific LoRA groups and zeroing out the others. GS-LoRA is effective parameter-efficient data-efficient and easy to implement. We conduct extensive experiments on face recognition object detection and image classification and demonstrate that GS-LoRA manages to forget specific classes with minimal impact on other classes. Codes will be released on https://github.com/bjzhb666/GS-LoRA.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Continual_Forgetting_for_Pre-trained_Vision_Models_CVPR_2024_paper.pdf,https://github.com/bjzhb666/GS-LoRA,3,"为了解决隐私和安全问题，需要从预训练的视觉模型中擦除不必要的信息已经变得明显。现实情况下，擦除请求可以随时来自用户和模型所有者。这些请求通常形成一个序列。因此，在这样的情况下，预期会持续从预训练模型中删除选择性信息，同时保留其余信息。我们将这个问题定义为""持续遗忘""，并确定了两个关键挑战。解决这些挑战，我们提出了Group Sparse LoRA (GS-LoRA)。通过对（i）使用LoRA模块来独立为每个遗忘任务微调Transformer块中的FFN层，以实现对不需要的知识进行高效且有效的删除；（ii）采用简单的分组稀疏正则化，实现对特定LoRA组的自动选择并将其余的置零，以最小化遗忘过程带来的影响。GS-LoRA具有效果好、参数高效"
77,Enhancing 3D Object Detection with 2D Detection-Guided Query Anchors,CVPR,2024,"Haoxuanye Ji, Pengpeng Liang, Erkang Cheng",Multi-camera-based 3D object detection has made notable progress in the past several years. However we observe that there are cases (e.g. faraway regions) in which popular 2D object detectors are more reliable than state-of-the-art 3D detectors. In this paper to improve the performance of query-based 3D object detectors we present a novel query generating approach termed QAF2D which infers 3D query anchors from 2D detection results. A 2D bounding box of an object in an image is lifted to a set of 3D anchors by associating each sampled point within the box with depth yaw angle and size candidates. Then the validity of each 3D anchor is verified by comparing its projection in the image with its corresponding 2D box and only valid anchors are kept and used to construct queries. The class information of the 2D bounding box associated with each query is also utilized to match the predicted boxes with ground truth for the set-based loss. The image feature extraction backbone is shared between the 3D detector and 2D detector by adding a small number of prompt parameters. We integrate QAF2D into three popular query-based 3D object detectors and carry out comprehensive evaluations on the nuScenes dataset. The largest improvement that QAF2D can bring about on the nuScenes validation subset is 2.3% NDS and 2.7% mAP. Code is available at https://github.com/max-vision/QAF2D.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Ji_Enhancing_3D_Object_Detection_with_2D_Detection-Guided_Query_Anchors_CVPR_2024_paper.pdf,https://github.com/max-vision/QAF2D,2,本文提出了一种名为QAF2D的新型查询生成方法，通过将2D检测结果提升到一组3D锚点，以改善查询式3D物体检测器的性能。该方法在图像中将物体的2D边界框提升为一组3D锚点，然后通过比较其在图像中的投影和对应的2D框验证每个3D锚点的有效性，并仅保留有效锚点用于构建查询。此外，还利用与每个查询相关联的2D边界框的类信息来进行匹配，以用于基于集合的损失。通过在三种流行的查询式3D物体检测器中集成QAF2D，并在nuScenes数据集上进行全面评估，我们发现QAF2D能够在nuScenes验证子集上带来最大的改进，其中NDS提高了2.3%，mAP提高了2.7%。
78,GeoChat: Grounded Large Vision-Language Model for Remote Sensing,CVPR,2024,"Kartik Kuckreja, Muhammad Sohail Danish, Muzammal Naseer, Abhijit Das, Salman Khan, Fahad Shahbaz Khan",Recent advancements in Large Vision-Language Models (VLMs) have shown great promise in natural image domains allowing users to hold a dialogue about given visual content. However such general-domain VLMs perform poorly for Remote Sensing (RS) scenarios leading to inaccurate or fabricated information when presented with RS domain-specific queries. Such a behavior emerges due to the unique challenges introduced by RS imagery. For example to handle high-resolution RS imagery with diverse scale changes across categories and many small objects region-level reasoning is necessary alongside holistic scene interpretation. Furthermore the lack of domain-specific multimodal instruction following data as well as strong backbone models for RS make it hard for the models to align their behavior with user queries. To address these limitations we propose GeoChat - the first versatile remote sensing VLM that offers multitask conversational capabilities with high-resolution RS images. Specifically GeoChat can not only answer image-level queries but also accepts region inputs to hold region-specific dialogue. Furthermore it can visually ground objects in its responses by referring to their spatial coordinates. To address the lack of domain-specific datasets we generate a novel RS multimodal instruction-following dataset by extending image-text pairs from existing diverse RS datasets. Leveraging this rich dataset we fine-tune our remote sensing VLM based on the LLaVA-1.5 architecture. We establish a comprehensive benchmark for RS multitask conversations and compare with a number of baseline methods. GeoChat demonstrates robust zero-shot performance on various remote sensing tasks e.g. image and region captioning visual question answering scene classification visually grounded conversations and referring object detection. Our codes will be open-sourced.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Kuckreja_GeoChat_Grounded_Large_Vision-Language_Model_for_Remote_Sensing_CVPR_2024_paper.pdf,,1,最近在大型视觉-语言模型（VLMs）领域取得了重大进展，展示出在自然图像领域具有巨大潜力，使用户能够就给定的视觉内容进行对话。然而，这种通用领域的 VLMs 在遥感（RS）场景中表现不佳，导致在面对 RS 领域特定查询时产生不准确或虚构的信息。这种行为的出现是由于 RS 图像引入的独特挑战。为了解决这些限制，我们提出了 GeoChat - 第一个多功能遥感 VLM，提供了高分辨率 RS 图像的多任务对话能力。具体来说，GeoChat 不仅可以回答图像级别的查询，还可以接受区域输入进行区域特定的对话。此外，它可以通过参考它们的空间坐标在其回应中对对象进行视觉定位。为了解决缺乏领域特定数据集的问题，我们通过扩展现有多
79,VTQA: Visual Text Question Answering via Entity Alignment and Cross-Media Reasoning,CVPR,2024,"Kang Chen, Xiangqian Wu",Achieving the optimal form of Visual Question Answering mandates a profound grasp of understanding grounding and reasoning within the intersecting domains of vision and language. Traditional VQA benchmarks have predominantly focused on simplistic tasks such as counting visual attributes and object detection which do not necessitate intricate cross-modal information understanding and inference. Motivated by the need for a more comprehensive evaluation we introduce a novel dataset comprising 23781 questions derived from 10124 image-text pairs. Specifically the task of this dataset requires the model to align multimedia representations of the same entity to implement multi-hop reasoning between image and text and finally use natural language to answer the question. Furthermore we evaluate this VTQA dataset comparing the performance of both state-of-the-art VQA models and our proposed baseline model the Key Entity Cross-Media Reasoning Network (KECMRN). The VTQA task poses formidable challenges for traditional VQA models underscoring its intrinsic complexity. Conversely KECMRN exhibits a modest improvement signifying its potential in multimedia entity alignment and multi-step reasoning. Our analysis underscores the diversity difficulty and scale of the VTQA task compared to previous multimodal QA datasets. In conclusion we anticipate that this dataset will serve as a pivotal resource for advancing and evaluating models proficient in multimedia entity alignment multi-step reasoning and open-ended answer generation. Our dataset and code is available at https://visual-text-qa.github.io/.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_VTQA_Visual_Text_Question_Answering_via_Entity_Alignment_and_Cross-Media_CVPR_2024_paper.pdf,,2,为了实现视觉问答的最佳形式，必须深刻理解视觉和语言交叉领域内的基础和推理。传统的VQA基准主要关注简单的任务，如计数视觉属性和目标检测，这些任务不需要复杂的跨模态信息理解和推理。为了更全面地评估，我们引入了一个新的数据集，包括来自10124个图像-文本对的23781个问题。该数据集的任务要求模型对相同实体的多媒体表示进行对齐，实现图像和文本之间的多跳推理，并最终使用自然语言回答问题。此外，我们评估了这个VTQA数据集，比较了最先进的VQA模型和我们提出的基准模型Key Entity Cross-Media Reasoning Network (KECMRN)的性能。VTQA任务对传统VQA模型提出了巨大挑战，凸显了其固有复杂性。相反，KECMRN表现出一定改进，显示出
80,Poly Kernel Inception Network for Remote Sensing Detection,CVPR,2024,"Xinhao Cai, Qiuxia Lai, Yuwei Wang, Wenguan Wang, Zeren Sun, Yazhou Yao",Object detection in remote sensing images (RSIs) often suffers from several increasing challenges including the large variation in object scales and the diverse-ranging context. Prior methods tried to address these challenges by expanding the spatial receptive field of the backbone either through large-kernel convolution or dilated convolution. However the former typically introduces considerable background noise while the latter risks generating overly sparse feature representations. In this paper we introduce the Poly Kernel Inception Network (PKINet) to handle the above challenges. PKINet employs multi-scale convolution kernels without dilation to extract object features of varying scales and capture local context. In addition a Context Anchor Attention (CAA) module is introduced in parallel to capture long-range contextual information. These two components work jointly to advance the performance of PKINet on four challenging remote sensing object detection benchmarks namely DOTA-v1.0 DOTA-v1.5 HRSC2016 and DIOR-R.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Cai_Poly_Kernel_Inception_Network_for_Remote_Sensing_Detection_CVPR_2024_paper.pdf,,2,本文介绍了一种用于远程感知图像中目标检测的Poly Kernel Inception网络（PKINet）。PKINet利用多尺度卷积核提取各种尺度的目标特征，并捕获局部上下文。此外，还引入了上下文锚点注意力（CAA）模块，以捕获远距离的上下文信息。这两个组件共同推动了PKINet在四个具有挑战性的远程感知目标检测基准上的性能，即 DOTA-v1.0，DOTA-v1.5，HRSC2016 和 DIOR-R。
81,Universal Novelty Detection Through Adaptive Contrastive Learning,CVPR,2024,"Hossein Mirzaei, Mojtaba Nafez, Mohammad Jafari, Mohammad Bagher Soltani, Mohammad Azizmalayeri, Jafar Habibi, Mohammad Sabokrou, Mohammad Hossein Rohban",Novelty detection is a critical task for deploying machine learning models in the open world. A crucial property of novelty detection methods is universality which can be interpreted as generalization across various distributions of training or test data. More precisely for novelty detection distribution shifts may occur in the training set or the test set. Shifts in the training set refer to cases where we train a novelty detector on a new dataset and expect strong transferability. Conversely distribution shifts in the test set indicate the methods' performance when the trained model encounters a shifted test sample. We experimentally show that existing methods falter in maintaining universality which stems from their rigid inductive biases. Motivated by this we aim for more generalized techniques that have more adaptable inductive biases. In this context we leverage the fact that contrastive learning provides an efficient framework to easily switch and adapt to new inductive biases through the proper choice of augmentations in forming the negative pairs. We propose a novel probabilistic auto-negative pair generation method AutoAugOOD along with contrastive learning to yield a universal novelty detector method. Our experiments demonstrate the superiority of our method under different distribution shifts in various image benchmark datasets. Notably our method emerges universality in the lens of adaptability to different setups of novelty detection including one-class unlabeled multi-class and labeled multi-class settings.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Mirzaei_Universal_Novelty_Detection_Through_Adaptive_Contrastive_Learning_CVPR_2024_paper.pdf,,1,检测新奇性是在开放世界中部署机器学习模型的关键任务。新奇性检测方法的一个关键属性是普适性，可以解释为在训练或测试数据的各种分布之间的泛化能力。在新奇性检测中，分布转移可能发生在训练集或测试集中。训练集中的转移是指在新数据集上训练新奇性检测器，并期望强大的可迁移性。相反，测试集中的分布转移指的是当训练模型遇到转移的测试样本时方法的性能。我们通过实验证明，现有方法在保持普适性方面存在问题，这源于它们刚性的归纳偏见。受此启发，我们致力于更广义的技术，具有更可适应的归纳偏见。在这种情况下，我们利用对比学习为了通过恰当选择增强来形成负对而轻松地切换和
82,Prompt3D: Random Prompt Assisted Weakly-Supervised 3D Object Detection,CVPR,2024,"Xiaohong Zhang, Huisheng Ye, Jingwen Li, Qinyu Tang, Yuanqi Li, Yanwen Guo, Jie Guo",The prohibitive cost of annotations for fully supervised 3D indoor object detection limits its practicality. In this work we propose Random Prompt Assisted Weakly-supervised 3D Object Detection termed as Prompt3D a weakly-supervised approach that leverages position-level labels to overcome this challenge. Explicitly our method focuses on enhancing labeling using synthetic scenes crafted from 3D shapes generated via random prompts. First a Synthetic Scene Generation (SSG) module is introduced to assemble synthetic scenes with a curated collection of 3D shapes created via random prompts for each category. These scenes are enriched with automatically generated point-level annotations providing a robust supervisory framework for training the detection algorithm. To enhance the transfer of knowledge from virtual to real datasets we then introduce a Prototypical Proposal Feature Alignment (PPFA) module. This module effectively alleviates the domain gap by directly minimizing the distance between feature prototypes of the same class proposals across two domains. Compared with sota BR our method improves by 5.4% and 8.7% on mAP with VoteNet and GroupFree3D serving as detectors respectively demonstrating the effectiveness of our proposed method. Code is available at: https://github.com/huishengye/prompt3d.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Prompt3D_Random_Prompt_Assisted_Weakly-Supervised_3D_Object_Detection_CVPR_2024_paper.pdf,https://github.com/huishengye/prompt3d,2,本研究提出了一种名为Prompt3D的弱监督方法，利用位置级标签来增强标注，从而克服全监督3D室内物体检测注释成本高昂且不切实际的限制。通过使用从随机提示生成的3D形状创建的合成场景，我们的方法专注于提高标注质量。我们引入了一个合成场景生成（SSG）模块，用于组装包含每个类别利用随机提示生成的3D形状的合成场景。这些场景丰富了自动生成的点级标注，为训练检测算法提供了强大的监督框架。为了增强从虚拟到真实数据集的知识转移，我们引入了一个原型提议特征对齐（PPFA）模块。与sota BR相比，我们的方法在mAP上分别通过VoteNet和GroupFree3D作为检测器改善了5.4%和8.7%，展示了我们提出的方法的有效性。
83,Weakly Misalignment-free Adaptive Feature Alignment for UAVs-based Multimodal Object Detection,CVPR,2024,"Chen Chen, Jiahao Qi, Xingyue Liu, Kangcheng Bin, Ruigang Fu, Xikun Hu, Ping Zhong",Visible-infrared (RGB-IR) image fusion has shown great potentials in object detection based on unmanned aerial vehicles (UAVs). However the weakly misalignment problem between multimodal image pairs limits its performance in object detection. Most existing methods often ignore the modality gap and emphasize a strict alignment resulting in an upper bound of alignment quality and an increase of implementation costs. To address these challenges we propose a novel method named Offset-guided Adaptive Feature Alignment (OAFA) which could adaptively adjust the relative positions between multimodal features. Considering the impact of modality gap on the cross-modality spatial matching a Cross-modality Spatial Offset Modeling (CSOM) module is designed to establish a common subspace to estimate the precise feature-level offsets. Then an Offset-guided Deformable Alignment and Fusion (ODAF) module is utilized to implicitly capture optimal fusion positions for detection task rather than conducting a strict alignment. Comprehensive experiments demonstrate that our method not only achieves state-of-the-art performance in the UAVs-based object detection task but also shows strong robustness to the weakly misalignment problem.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_Weakly_Misalignment-free_Adaptive_Feature_Alignment_for_UAVs-based_Multimodal_Object_Detection_CVPR_2024_paper.pdf,,1,可见-红外（RGB-IR）图像融合在基于无人机（UAVs）的对象检测中表现出巨大潜力。然而，多模态图像对之间存在的微弱不对齐问题限制了其在对象检测中的性能。大多数现有方法通常忽略模态差距，并强调严格对齐，导致对齐质量的上限和实施成本的增加。为了应对这些挑战，我们提出了一种名为偏移引导自适应特征对齐（OAFA）的新方法，该方法可以自适应地调整多模态特征之间的相对位置。考虑到模态差距对跨模态空间匹配的影响，设计了一个跨模态空间偏移建模（CSOM）模块，用于建立一个共同子空间以估计精确的特征级偏移。然后，利用偏移引导的可变对齐和融合（ODAF）模
84,Disentangled Pre-training for Human-Object Interaction Detection,CVPR,2024,"Zhuolong Li, Xingao Li, Changxing Ding, Xiangmin Xu",Detecting human-object interaction (HOI) has long been limited by the amount of supervised data available. Recent approaches address this issue by pre-training according to pseudo-labels which align object regions with HOI triplets parsed from image captions. However pseudo-labeling is tricky and noisy making HOI pre-training a complex process. Therefore we propose an efficient disentangled pre-training method for HOI detection (DP-HOI) to address this problem. First DP-HOI utilizes object detection and action recognition datasets to pre-train the detection and interaction decoder layers respectively. Then we arrange these decoder layers so that the pre-training architecture is consistent with the downstream HOI detection task. This facilitates efficient knowledge transfer. Specifically the detection decoder identifies reliable human instances in each action recognition dataset image generates one corresponding query and feeds it into the interaction decoder for verb classification. Next we combine the human instance verb predictions in the same image and impose image-level supervision. The DP-HOI structure can be easily adapted to the HOI detection task enabling effective model parameter initialization. Therefore it significantly enhances the performance of existing HOI detection models on a broad range of rare categories. The code and pre-trained weight are available at https://github.com/xingaoli/DP-HOI.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Li_Disentangled_Pre-training_for_Human-Object_Interaction_Detection_CVPR_2024_paper.pdf,https://github.com/xingaoli/DP-HOI,2,检测人-物体交互（HOI）长期以来受制于有监督数据的数量限制。 最近的方法通过根据从图像标题中解析的HOI三元组对齐对象区域的伪标签来解决这一问题。 然而，伪标签是棘手且嘈杂的，使得HOI预训练成为一个复杂的过程。 因此，我们提出了一种高效的解耦预训练方法，用于HOI检测（DP-HOI）来解决这个问题。 首先，DP-HOI利用对象检测和动作识别数据集来分别预训练检测和交互解码器层。 然后，我们排列这些解码器层，使得预训练架构与下游的HOI检测任务一致。 这有助于有效的知识传输。 具体来说，检测解码器在每个动作识别数据集图像中识别可靠的人类实例，生成一个相应的查询并将
85,Unbiased Faster R-CNN for Single-source Domain Generalized Object Detection,CVPR,2024,"Yajing Liu, Shijun Zhou, Xiyao Liu, Chunhui Hao, Baojie Fan, Jiandong Tian",Single-source domain generalization (SDG) for object detection is a challenging yet essential task as the distribution bias of the unseen domain degrades the algorithm performance significantly. However existing methods attempt to extract domain-invariant features neglecting that the biased data leads the network to learn biased features that are non-causal and poorly generalizable. To this end we propose an Unbiased Faster R-CNN (UFR) for generalizable feature learning. Specifically we formulate SDG in object detection from a causal perspective and construct a Structural Causal Model (SCM) to analyze the data bias and feature bias in the task which are caused by scene confounders and object attribute confounders. Based on the SCM we design a Global-Local Transformation module for data augmentation which effectively simulates domain diversity and mitigates the data bias. Additionally we introduce a Causal Attention Learning module that incorporates a designed attention invariance loss to learn image-level features that are robust to scene confounders. Moreover we develop a Causal Prototype Learning module with an explicit instance constraint and an implicit prototype constraint which further alleviates the negative impact of object attribute confounders. Experimental results on five scenes demonstrate the prominent generalization ability of our method with an improvement of 3.9% mAP on the Night-Clear scene.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Unbiased_Faster_R-CNN_for_Single-source_Domain_Generalized_Object_Detection_CVPR_2024_paper.pdf,,1,单源领域泛化（SDG）对目标检测来说是一项具有挑战性但又必不可少的任务，因为未知领域的分布偏差会显著降低算法性能。然而，现有方法试图提取领域不变特征，忽视了有偏数据导致网络学习有偏特征的问题，这些特征是非因果的且普适性差。因此，我们提出了一种用于泛化特征学习的无偏Faster R-CNN（UFR）。具体来说，我们从因果角度制定了目标检测中的SDG，并构建了一个结构因果模型（SCM）来分析由场景混淆因素和物体属性混淆因素导致的数据偏差和特征偏差。基于SCM，我们设计了一个全局-局部变换模块用于数据增强，有效模拟领域多样性并减轻数据偏差。此外，我们还引
86,Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding,CVPR,2024,"Zhihao Yuan, Jinke Ren, Chun-Mei Feng, Hengshuang Zhao, Shuguang Cui, Zhen Li",3D Visual Grounding (3DVG) aims at localizing 3D object based on textual descriptions. Conventional supervised methods for 3DVG often necessitate extensive annotations and a predefined vocabulary which can be restrictive. To address this issue we propose a novel visual programming approach for zero-shot open-vocabulary 3DVG leveraging the capabilities of large language models (LLMs). Our approach begins with a unique dialog-based method engaging with LLMs to establish a foundational understanding of zero-shot 3DVG. Building on this we design a visual program that consists of three types of modules i.e. view-independent view-dependent and functional modules. Furthermore we develop an innovative language-object correlation module to extend the scope of existing 3D object detectors into open-vocabulary scenarios. Extensive experiments demonstrate that our zero-shot approach can outperform some supervised baselines marking a significant stride towards effective 3DVG. Code is available at https://curryyuan.github.io/ZSVG3D.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Yuan_Visual_Programming_for_Zero-shot_Open-Vocabulary_3D_Visual_Grounding_CVPR_2024_paper.pdf,,2,3D 视觉定位（3DVG）旨在基于文本描述定位 3D 物体。传统的监督式 3DVG 方法通常需要大量注释和预定义词汇，这可能具有限制性。为了解决这个问题，我们提出了一种新颖的视觉编程方法，用于零样本开放词汇 3DVG，利用了大型语言模型（LLMs）的能力。我们的方法始于一种独特的对话式方法，与LLMs 互动，建立零样本 3DVG 的基础理解。在此基础上，我们设计了一个视觉程序，包括三种类型的模块，即独立于视角的、依赖于视角的和功能型模块。此外，我们开发了一种创新的语言-对象相关模块，将现有 3D 物体检测器的范围扩展到开放词汇的场景。大量实验证明，我们的零样本方法可以胜过一些监督式基线，标
87,The Devil is in the Fine-Grained Details: Evaluating Open-Vocabulary Object Detectors for Fine-Grained Understanding,CVPR,2024,"Lorenzo Bianchi, Fabio Carrara, Nicola Messina, Claudio Gennaro, Fabrizio Falchi",Recent advancements in large vision-language models enabled visual object detection in open-vocabulary scenarios where object classes are defined in free-text formats during inference. In this paper we aim to probe the state-of-the-art methods for open-vocabulary object detection to determine to what extent they understand fine-grained properties of objects and their parts. To this end we introduce an evaluation protocol based on dynamic vocabulary generation to test whether models detect discern and assign the correct fine-grained description to objects in the presence of hard-negative classes. We contribute with a benchmark suite of increasing difficulty and probing different properties like color pattern and material. We further enhance our investigation by evaluating several state-of-the-art open-vocabulary object detectors using the proposed protocol and find that most existing solutions which shine in standard open-vocabulary benchmarks struggle to accurately capture and distinguish finer object details. We conclude the paper by highlighting the limitations of current methodologies and exploring promising research directions to overcome the discovered drawbacks. Data and code are available at https://lorebianchi98.github.io/FG-OVD .,,https://openaccess.thecvf.com/content/CVPR2024/papers/Bianchi_The_Devil_is_in_the_Fine-Grained_Details_Evaluating_Open-Vocabulary_Object_CVPR_2024_paper.pdf,,2,本文讨论了大型视觉-语言模型在开放词汇场景下实现视觉对象检测的最新进展，其中对象类别在推理过程中以自由文本格式定义。研究旨在探究开放词汇对象检测的最新方法，以确定其在多大程度上理解对象及其部分的细粒度属性。通过引入基于动态词汇生成的评估协议，测试模型在存在难识别类别的情况下是否能正确检测、识别并分配对象的正确细粒度描述。作者提出了一系列不断增加难度并探索不同属性（如颜色、图案和材料）的基准套件。通过使用提出的协议评估几种最先进的开放词汇对象检测器，发现大多数现有解决方案在标准开放词汇基准测试中表现出色，但难以准确捕捉和区分更细节的对象细节。文章总
88,Edge-Aware 3D Instance Segmentation Network with Intelligent Semantic Prior,CVPR,2024,"Wonseok Roh, Hwanhee Jung, Giljoo Nam, Jinseop Yeom, Hyunje Park, Sang Ho Yoon, Sangpil Kim",While recent 3D instance segmentation approaches show promising results based on transformer architectures they often fail to correctly identify instances with similar appearances. They also ambiguously determine edges leading to multiple misclassifications of adjacent edge points. In this work we introduce a novel framework called EASE to overcome these challenges and improve the perception of complex 3D instances. We first propose a semantic guidance network to leverage rich semantic knowledge from a language model as intelligent priors enhancing the functional understanding of real-world instances beyond relying solely on geometrical information. We explicitly instruct the basic instance queries using text embeddings of each instance to learn deep semantic details. Further we utilize the edge prediction module encouraging the segmentation network to be edge-aware. We extract voxel-wise edge maps from point features and use them as auxiliary information for learning edge cues. In our extensive experiments on large-scale benchmarks ScanNetV2 ScanNet200 S3DIS and STPLS3D our EASE outperforms existing state-of-the-art models demonstrating its superior performance.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Roh_Edge-Aware_3D_Instance_Segmentation_Network_with_Intelligent_Semantic_Prior_CVPR_2024_paper.pdf,,2,最近的3D实例分割方法基于变压器架构取得了有希望的结果，但它们经常无法正确识别外观相似的实例。它们也有模糊地确定边缘，导致多个相邻边缘点的错误分类。在这项工作中，我们引入了一种名为EASE的新型框架，以克服这些挑战，提高对复杂3D实例的感知能力。我们首先提出了一个语义引导网络，利用语言模型中的丰富语义知识作为智能先验，增强对真实世界实例的功能理解，不再仅依赖几何信息。我们明确指导基本实例查询，使用每个实例的文本嵌入来学习深层语义细节。此外，我们利用边缘预测模块鼓励分割网络具有边缘意识。我们从点特征中提取逐体素边缘图，并将其用作
89,COCONut: Modernizing COCO Segmentation,CVPR,2024,"Xueqing Deng, Qihang Yu, Peng Wang, Xiaohui Shen, Liang-Chieh Chen",In recent decades the vision community has witnessed remarkable progress in visual recognition partially owing to advancements in dataset benchmarks. Notably the established COCO benchmark has propelled the development of modern detection and segmentation systems. However the COCO segmentation benchmark has seen comparatively slow improvement over the last decade. Originally equipped with coarse polygon annotations for thing instances it gradually incorporated coarse superpixel annotations for stuff regions which were subsequently heuristically amalgamated to yield panoptic segmentation annotations. These annotations executed by different groups of raters have resulted not only in coarse segmentation masks but also in inconsistencies between segmentation types. In this study we undertake a comprehensive reevaluation of the COCO segmentation annotations. By enhancing the annotation quality and expanding the dataset to encompass 383K images with more than 5.18M panoptic masks we introduce COCONut the COCO Next Universal segmenTation dataset. COCONut harmonizes segmentation annotations across semantic instance and panoptic segmentation with meticulously crafted high-quality masks and establishes a robust benchmark for all segmentation tasks. To our knowledge COCONut stands as the inaugural large-scale universal segmentation dataset verified by human raters. We anticipate that the release of COCONut will significantly contribute to the community's ability to assess the progress of novel neural networks.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Deng_COCONut_Modernizing_COCO_Segmentation_CVPR_2024_paper.pdf,,1,最近几十年来，视觉社区在视觉识别方面取得了显著进展，部分归功于数据集基准的发展。著名的COCO基准推动了现代检测和分割系统的发展。然而，COCO分割基准在过去十年里进展相对较慢。最初，它为物体实例提供了粗略的多边形注释，逐渐融入了粗略的超像素注释用于区域，随后又启发性地合并为panoptic分割注释。本研究对COCO分割注释进行了全面重新评估。通过提高注释质量，扩大数据集范围至383K张图像，并增加超过5.18M个panoptic掩模，我们引入了COCONut，即COCO下一代通用分割数据集。COCONut通过精心制作高质量掩模，在语义实例和panoptic分割之间实现分割注释的协
90,Traffic Scene Parsing through the TSP6K Dataset,CVPR,2024,"Peng-Tao Jiang, Yuqi Yang, Yang Cao, Qibin Hou, Ming-Ming Cheng, Chunhua Shen",Traffic scene perception in computer vision is a critically important task to achieve intelligent cities. To date most existing datasets focus on autonomous driving scenes. We observe that the models trained on those driving datasets often yield unsatisfactory results on traffic monitoring scenes. However little effort has been put into improving the traffic monitoring scene understanding mainly due to the lack of specific datasets. To fill this gap we introduce a specialized traffic monitoring dataset termed TSP6K containing images from the traffic monitoring scenario with high-quality pixel-level and instance-level annotations. The TSP6K dataset captures more crowded traffic scenes with several times more traffic participants than the existing driving scenes. We perform a detailed analysis of the dataset and comprehensively evaluate previous popular scene parsing methods instance segmentation methods and unsupervised domain adaption methods. Furthermore considering the vast difference in instance sizes we propose a detail refining decoder for scene parsing which recovers the details of different semantic regions in traffic scenes owing to the proposed TSP6K dataset. Experiments show its effectiveness in parsing the traffic monitoring scenes. Code and dataset are available at https://github.com/PengtaoJiang/TSP6K.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Jiang_Traffic_Scene_Parsing_through_the_TSP6K_Dataset_CVPR_2024_paper.pdf,https://github.com/PengtaoJiang/TSP6K,2,交通场景在计算机视觉中是一个至关重要的任务，以实现智能城市。大多数现有数据集侧重于自动驾驶场景。然而，我们观察到，那些在驾驶数据集上训练的模型在交通监控场景上经常产生不理想的结果。然而，由于缺乏特定数据集，对改进交通监控场景理解的工作很少。为了填补这一空白，我们引入了一个名为 TSP6K 的专门交通监控数据集，其中包含来自交通监控场景的图像，并具有高质量的像素级和实例级注释。TSP6K 数据集捕获了比现有驾驶场景更拥挤的交通场景，交通参与者数量是现有场景的几倍。我们对数据集进行了详细分析，并全面评估了以往流行的场景解析方法、实例分割方法和无监督域自适应方法。此外，考虑到
91,LASA: Instance Reconstruction from Real Scans using A Large-scale Aligned Shape Annotation Dataset,CVPR,2024,"Haolin Liu, Chongjie Ye, Yinyu Nie, Yingfan He, Xiaoguang Han",Instance shape reconstruction from a 3D scene involves recovering the full geometries of multiple objects at the semantic instance level. Many methods leverage data-driven learning due to the intricacies of scene complexity and significant indoor occlusions. Training these methods often requires a large-scale high-quality dataset with aligned and paired shape annotations with real-world scans. Existing datasets are either synthetic or misaligned restricting the performance of data-driven methods on real data. To this end we introduce LASA a Large-scale Aligned Shape Annotation Dataset comprising 10412 high-quality CAD annotations aligned with 920 real-world scene scans from ArkitScenes created manually by professional artists. On this top we propose a novel Diffusion-based Cross-Modal Shape Reconstruction (DisCo) method. It is empowered by a hybrid feature aggregation design to fuse multi-modal inputs and recover high-fidelity object geometries. Besides we present an Occupancy-Guided 3D Object Detection (OccGOD) method and demonstrate that our shape annotations provide scene occupancy clues that can further improve 3D object detection. Supported by LASA extensive experiments show that our methods achieve state-of-the-art performance in both instance-level scene reconstruction and 3D object detection tasks.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_LASA_Instance_Reconstruction_from_Real_Scans_using_A_Large-scale_Aligned_CVPR_2024_paper.pdf,,1,实例形状重建涉及在语义实例级别上恢复多个对象的完整几何形状。许多方法利用数据驱动学习，由于场景复杂性和显著的室内遮挡。训练这些方法通常需要一个大规模高质量的数据集，其中包含与真实世界扫描对齐和配对的形状注释。现有数据集要么是合成的，要么是不对齐的，从而限制了数据驱动方法在真实数据上的性能。为此，我们引入了一个大规模对齐形状注释数据集LASA，包括10412个高质量CAD注释，与通过专业艺术家手动创建的920个ArkitScenes真实场景扫描对齐。在这一基础上，我们提出了一种新颖的基于扩散的跨模态形状重建（DisCo）方法。它借助混合特征聚合设计来融合多模态输入并恢复
92,FISBe: A Real-World Benchmark Dataset for Instance Segmentation of Long-Range Thin Filamentous Structures,CVPR,2024,"Lisa Mais, Peter Hirsch, Claire Managan, Ramya Kandarpa, Josef Lorenz Rumberger, Annika Reinke, Lena Maier-Hein, Gudrun Ihrke, Dagmar Kainmueller",Instance segmentation of neurons in volumetric light microscopy images of nervous systems enables groundbreaking research in neuroscience by facilitating joint functional and morphological analyses of neural circuits at cellular resolution. Yet said multi-neuron light microscopy data exhibits extremely challenging properties for the task of instance segmentation: Individual neurons have long-ranging thin filamentous and widely branching morphologies multiple neurons are tightly inter-weaved and partial volume effects uneven illumination and noise inherent to light microscopy severely impede local disentangling as well as long-range tracing of individual neurons. These properties reflect a current key challenge in machine learning research namely to effectively capture long-range dependencies in the data. While respective methodological research is buzzing to date methods are typically benchmarked on synthetic datasets. To address this gap we release the FlyLight Instance Segmentation Benchmark (FISBe) dataset the first publicly available multi-neuron light microscopy dataset with pixel-wise annotations. In addition we define a set of instance segmentation metrics for benchmarking that we designed to be meaningful with regard to downstream analyses. Lastly we provide three baselines to kick off a competition that we envision to both advance the field of machine learning regarding methodology for capturing long-range data dependencies and facilitate scientific discovery in basic neuroscience.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Mais_FISBe_A_Real-World_Benchmark_Dataset_for_Instance_Segmentation_of_Long-Range_CVPR_2024_paper.pdf,,1,神经系统体积光学显微图像中的神经元实例分割促进了神经科学中的创新研究，通过在细胞分辨率下促进神经回路的功能和形态分析。然而，多神经元光学显微数据对实例分割任务具有极具挑战性的特性：单个神经元具有长程细长丝状和广泛分支的形态，多个神经元紧密交织，局部体积效应不均匀的照明和光学显微固有的噪声严重阻碍了单个神经元的局部解缠和长程追踪。这些特性反映了当前机器学习研究中的一个关键挑战，即有效捕获数据中的长程依赖关系。虽然目前方法论研究颇受关注，但通常是在合成数据集上进行基准测试。为了填补这一空
93,MaskClustering: View Consensus based Mask Graph Clustering for Open-Vocabulary 3D Instance Segmentation,CVPR,2024,"Mi Yan, Jiazhao Zhang, Yan Zhu, He Wang",Open-vocabulary 3D instance segmentation is cutting-edge for its ability to segment 3D instances without predefined categories. However progress in 3D lags behind its 2D counterpart due to limited annotated 3D data. To address this recent works first generate 2D open-vocabulary masks through 2D models and then merge them into 3D instances based on metrics calculated between two neighboring frames. In contrast to these local metrics we propose a novel metric view consensus rate to enhance the utilization of multi-view observations. The key insight is that two 2D masks should be deemed part of the same 3D instance if a significant number of other 2D masks from different views contain both these two masks. Using this metric as edge weight we construct a global mask graph where each mask is a node. Through iterative clustering of masks showing high view consensus we generate a series of clusters each representing a distinct 3D instance. Notably our model is training-free. Through extensive experiments on publicly available datasets including ScanNet++ ScanNet200 and MatterPort3D we demonstrate that our method achieves state-of-the-art performance in open-vocabulary 3D instance segmentation. Our project page is at \href https://pku-epic.github.io/MaskClustering/  https://pku-epic.github.io/MaskClustering .,,https://openaccess.thecvf.com/content/CVPR2024/papers/Yan_MaskClustering_View_Consensus_based_Mask_Graph_Clustering_for_Open-Vocabulary_3D_CVPR_2024_paper.pdf,,2,为了解决3D实例分割中的数据限制问题，最近的研究首先通过2D模型生成2D开放词汇掩码，然后基于两个相邻帧之间计算的指标将它们合并为3D实例。与这些局部指标相对，他们提出了一种新颖的度量视图一致率，以增强多视角观察的利用。通过迭代聚类显示高视图一致性的掩码，产生一系列代表不同3D实例的聚类。值得注意的是，他们的模型是无需训练的。通过在公开数据集上进行大量实验，包括ScanNet++、ScanNet200和MatterPort3D，展示了他们的方法在开放词汇3D实例分割中实现了最先进性能。
94,DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection,CVPR,2024,"Lewei Yao, Renjie Pi, Jianhua Han, Xiaodan Liang, Hang Xu, Wei Zhang, Zhenguo Li, Dan Xu",Existing open-vocabulary object detectors typically require a predefined set of categories from users significantly confining their application scenarios. In this paper we introduce DetCLIPv3 a high-performing detector that excels not only at both open-vocabulary object detection but also generating hierarchical labels for detected objects. DetCLIPv3 is characterized by three core designs: 1. Versatile model architecture: we derive a robust open-set detection framework which is further empowered with generation ability via the integration of a caption head. 2. High information density data: we develop an auto-annotation pipeline leveraging visual large language model to refine captions for large-scale image-text pairs providing rich multi-granular object labels to enhance the training. 3. Efficient training strategy: we employ a pre-training stage with low-resolution inputs that enables the object captioner to efficiently learn a broad spectrum of visual concepts from extensive image-text paired data. This is followed by a fine-tuning stage that leverages a small number of high-resolution samples to further enhance detection performance. With these effective designs DetCLIPv3 demonstrates superior open-vocabulary detection performance e.g. our Swin-T backbone model achieves a notable 47.0 zero-shot fixed AP on the LVIS minival benchmark outperforming GLIPv2 GroundingDINO and DetCLIPv2 by 18.0/19.6/6.6 AP respectively. DetCLIPv3 also achieves a state-of-the-art 19.7 AP in dense captioning task on VG dataset showcasing its strong generative capability.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Yao_DetCLIPv3_Towards_Versatile_Generative_Open-vocabulary_Object_Detection_CVPR_2024_paper.pdf,,1,本文介绍了DetCLIPv3，一个表现出色的检测器，不仅在开放词汇对象检测方面表现出色，还能为检测到的对象生成分层标签。DetCLIPv3的特点包括三个核心设计：1. 多功能模型架构：我们提出了一个强大的开放集检测框架，通过整合字幕头部使其具有生成能力。2. 高信息密度数据：我们开发了一个自动注释流水线，利用视觉大语言模型为大规模图像文本对精细化字幕，提供丰富的多粒度对象标签以增强训练。3. 高效训练策略：我们采用低分辨率输入的预训练阶段，使对象字幕生成器能够有效地从大量图像文本配对数据中学习到广泛的视觉概念。然后通过精调阶段利用少量高分辨率样本进一步提升检测性能
95,ConCon-Chi: Concept-Context Chimera Benchmark for Personalized Vision-Language Tasks,CVPR,2024,"Andrea Rosasco, Stefano Berti, Giulia Pasquale, Damiano Malafronte, Shogo Sato, Hiroyuki Segawa, Tetsugo Inada, Lorenzo Natale",While recent Vision-Language (VL) models excel at open-vocabulary tasks it is unclear how to use them with specific or uncommon concepts. Personalized Text-to-Image Retrieval (TIR) or Generation (TIG) are recently introduced tasks that represent this challenge where the VL model has to learn a concept from few images and respectively discriminate or generate images of the target concept in arbitrary contexts. We identify the ability to learn new meanings and their compositionality with known ones as two key properties of a personalized system. We show that the available benchmarks offer a limited validation of personalized textual concept learning from images with respect to the above properties and introduce ConCon-Chi as a benchmark for both personalized TIR and TIG designed to fill this gap. We modelled the new-meaning concepts by crafting chimeric objects and formulating a large varied set of contexts where we photographed each object. To promote the compositionality assessment of the learned concepts with known contexts we combined different contexts with the same concept and vice-versa. We carry out a thorough evaluation of state-of-the-art methods on the resulting dataset. Our study suggests that future work on personalized TIR and TIG methods should focus on the above key properties and we propose principles and a dataset for their performance assessment. Dataset: https://doi.org/10.48557/QJ1166 and code: https://github.com/hsp-iit/concon-chi_benchmark.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Rosasco_ConCon-Chi_Concept-Context_Chimera_Benchmark_for_Personalized_Vision-Language_Tasks_CVPR_2024_paper.pdf,https://github.com/hsp-iit/concon-chi_benchmark,3,最近的视觉语言（VL）模型在开放词汇的任务上表现出色，但如何将它们应用于特定或罕见的概念尚不清楚。个性化文本 - 图像检索（TIR）或生成（TIG）是最近引入的任务，代表了模型必须从少量图像中学习概念，然后在任意上下文中辨别或生成目标概念图像的挑战。我们确定学习新含义及其与已知含义的组合性是个性化系统的两个关键属性。我们展示了当前的基准测试在关于从图像学习个性化文本概念方面对上述属性的验证是有限的，并介绍了ConCon-Chi作为旨在填补这一空白的个性化TIR和TIG的基准测试。我们通过打造嵌合物体来建模新含义概念，并制定了一个大而多样的上下文集，在每个物体中拍摄照片。为了促进学习概
96,Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from Aerial Imagery,CVPR,2024,"Yuqi Zhang, Guanying Chen, Jiaxing Chen, Shuguang Cui",We present a neural radiance field method for urban-scale semantic and building-level instance segmentation from aerial images by lifting noisy 2D labels to 3D. This is a challenging problem due to two primary reasons. Firstly objects in urban aerial images exhibit substantial variations in size including buildings cars and roads which pose a significant challenge for accurate 2D segmentation. Secondly the 2D labels generated by existing segmentation methods suffer from the multi-view inconsistency problem especially in the case of aerial images where each image captures only a small portion of the entire scene. To overcome these limitations we first introduce a scale-adaptive semantic label fusion strategy that enhances the segmentation of objects of varying sizes by combining labels predicted from different altitudes harnessing the novel-view synthesis capabilities of NeRF. We then introduce a novel cross-view instance label grouping strategy based on the 3D scene representation to mitigate the multi-view inconsistency problem in the 2D instance labels. Furthermore we exploit multi-view reconstructed depth priors to improve the geometric quality of the reconstructed radiance field resulting in enhanced segmentation results. Experiments on multiple real-world urban-scale datasets demonstrate that our approach outperforms existing methods highlighting its effectiveness. The source code is available at https://github.com/zyqz97/Aerial_lifting.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Aerial_Lifting_Neural_Urban_Semantic_and_Building_Instance_Lifting_from_CVPR_2024_paper.pdf,https://github.com/zyqz97/Aerial_lifting,2,我们提出了一种神经光辐射场方法，用于从航空图像中进行城市规模的语义分割和建筑级实例分割，将嘈杂的2D标签提升到3D。这是一个具有挑战性的问题，主要原因有两个。首先，城市航空图像中的对象在大小上存在实质性变化，包括建筑物、汽车和道路，这给精确的2D分割带来了重大挑战。其次，现有分割方法生成的2D标签在多视角一致性问题上存在困难，尤其是在航空图像的情况下，每个图像仅捕获整个场景的一小部分。为了克服这些局限性，我们首先引入了一种尺度自适应语义标签融合策略，通过合并从不同高度预测的标签来增强不同尺寸对象的分割，利用NeRF的新视角合成能力。然后，
97,Learning Degradation-Independent Representations for Camera ISP Pipelines,CVPR,2024,"Yanhui Guo, Fangzhou Luo, Xiaolin Wu",Image signal processing (ISP) pipeline plays a fundamental role in digital cameras which converts raw Bayer sensor data to RGB images. However ISP-generated images usually suffer from imperfections due to the compounded degradations that stem from sensor noises demosaicing noises compression artifacts and possibly adverse effects of erroneous ISP hyperparameter settings such as ISO and gamma values. In a general sense these ISP imperfections can be considered as degradations. The highly complex mechanisms of ISP degradations some of which are even unknown pose great challenges to the generalization capability of deep neural networks (DNN) for image restoration and to their adaptability to downstream tasks. To tackle the issues we propose a novel DNN approach to learn degradation-independent representations (DiR) through the refinement of a self-supervised learned baseline representation. The proposed DiR learning technique has remarkable domain generalization capability and consequently it outperforms state-of-the-art methods across various downstream tasks including blind image restoration object detection and instance segmentation as verified in our experiments.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_Learning_Degradation-Independent_Representations_for_Camera_ISP_Pipelines_CVPR_2024_paper.pdf,,2,图像信号处理（ISP）流水线在数码相机中扮演着重要角色，将原始 Bayer 传感器数据转换为 RGB 图像。然而，ISP 生成的图像通常存在缺陷，这些缺陷源自传感器噪声、去马赛克噪声、压缩伪像以及可能来自 ISO 和 gamma 值等错误的 ISP 超参数设置带来的复合退化。在一般意义上，这些 ISP 缺陷可以被视为退化。ISP 退化的高度复杂机制，其中一些甚至是未知的，给深度神经网络（DNN）的图像恢复的泛化能力和适应能力带来了巨大挑战。为了解决这些问题，我们提出了一种新颖的 DNN 方法，通过自监督学习基线表示的精炼来学习与退化无关的表示（DiR）。所提出的 DiR 学习技术具有显著的领域泛化能力，
98,RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding,CVPR,2024,"Jihan Yang, Runyu Ding, Weipeng Deng, Zhe Wang, Xiaojuan Qi",We propose a lightweight and scalable Regional Point-Language Contrastive learning framework namely RegionPLC for open-world 3D scene understanding aiming to identify and recognize open-set objects and categories. Specifically based on our empirical studies we introduce a 3D-aware SFusion strategy that fuses 3D vision-language pairs derived from multiple 2D foundation models yielding high-quality dense region-level language descriptions without human 3D annotations. Subsequently we devise a region-aware point-discriminative contrastive learning objective to enable robust and effective 3D learning from dense regional language supervision. We carry out extensive experiments on ScanNet ScanNet200 and nuScenes datasets and our model outperforms prior 3D open-world scene understanding approaches by an average of 17.2% and 9.1% for semantic and instance segmentation respectively while maintaining greater scalability and lower resource demands. Furthermore our method has the flexibility to be effortlessly integrated with language models to enable open-ended grounded 3D reasoning without extra task-specific training. Code will be released.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_RegionPLC_Regional_Point-Language_Contrastive_Learning_for_Open-World_3D_Scene_Understanding_CVPR_2024_paper.pdf,,2,我们提出了一个轻量级且可扩展的区域点-语言对比学习框架，即RegionPLC，用于开放式三维场景理解，旨在识别和识别开放集对象和类别。具体来说，基于我们的实证研究，我们引入了一种3D感知的SFusion策略，该策略融合了从多个2D基础模型导出的3D视觉-语言对，产生高质量的密集区域级语言描述，而无需人工3D注释。随后，我们设计了一个区域感知的点-判别对比学习目标，以实现从密集区域语言监督中进行强大且有效的3D学习。我们在ScanNet、ScanNet200和nuScenes数据集上进行了大量实验，我们的模型在语义分割和实例分割方面的表现优于先前的3D开放式场景理解方法，分别平均提高了17.2%和9.1%，同时具有更
99,Towards Automatic Power Battery Detection: New Challenge Benchmark Dataset and Baseline,CVPR,2024,"Xiaoqi Zhao, Youwei Pang, Zhenyu Chen, Qian Yu, Lihe Zhang, Hanqi Liu, Jiaming Zuo, Huchuan Lu",We conduct a comprehensive study on a new task named power battery detection (PBD) which aims to localize the dense cathode and anode plates endpoints from X-ray images to evaluate the quality of power batteries. Existing manufacturers usually rely on human eye observation to complete PBD which makes it difficult to balance the accuracy and efficiency of detection. To address this issue and drive more attention into this meaningful task we first elaborately collect a dataset called X-ray PBD which has 1500 diverse X-ray images selected from thousands of power batteries of 5 manufacturers with 7 different visual interference. Then we propose a novel segmentation-based solution for PBD termed multi-dimensional collaborative network (MDCNet). With the help of line and counting predictors the representation of the point segmentation branch can be improved at both semantic and detail aspects. Besides we design an effective distance-adaptive mask generation strategy which can alleviate the visual challenge caused by the inconsistent distribution density of plates to provide MDCNet with stable supervision. Without any bells and whistles our segmentation-based MDCNet consistently outperforms various other corner detection crowd counting and general/tiny object detection-based solutions making it a strong baseline that can help facilitate future research in PBD. Finally we share some potential difficulties and works for future researches. The source code and datasets will be publicly available at \href https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD  X-ray PBD .,,https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_Towards_Automatic_Power_Battery_Detection_New_Challenge_Benchmark_Dataset_and_CVPR_2024_paper.pdf,https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD,2,本研究对一项名为动力电池检测（PBD）的新任务进行了全面研究，旨在从X射线图像中定位密集的阴极和阳极板端点，以评估动力电池的质量。现有制造商通常依赖于人眼观察来完成PBD，这使得很难平衡检测的准确性和效率。为了解决这一问题并将更多关注引入这一有意义的任务，我们首先精心收集了一个名为X射线PBD的数据集，其中包含来自5家制造商数千块动力电池中选取的1500个多样化的X射线图像，具有7种不同的视觉干扰。然后，我们提出了一种称为多维协作网络（MDCNet）的基于分割的PBD解决方案。通过线性和计数预测器的帮助，点分割分支的表示在语义和细节方面都得到改善
100,UnScene3D: Unsupervised 3D Instance Segmentation for Indoor Scenes,CVPR,2024,"David Rozenberszki, Or Litany, Angela Dai",3D instance segmentation is fundamental to geometric understanding of the world around us. Existing methods for instance segmentation of 3D scenes rely on supervision from expensive manual 3D annotations. We propose UnScene3D the first fully unsupervised 3D learning approach for class-agnostic 3D instance segmentation of indoor scans. UnScene3D first generates pseudo masks by leveraging self-supervised color and geometry features to find potential object regions. We operate on a basis of 3D segment primitives enabling efficient representation and learning on high-resolution 3D data. The coarse proposals are then refined through self-training our model on its predictions. Our approach improves over state-of-the-art unsupervised 3D instance segmentation methods by more than 300% Average Precision score demonstrating effective instance segmentation even in challenging cluttered 3D scenes.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Rozenberszki_UnScene3D_Unsupervised_3D_Instance_Segmentation_for_Indoor_Scenes_CVPR_2024_paper.pdf,,2,3D实例分割对于我们周围世界的几何理解至关重要。现有的3D场景实例分割方法依赖于昂贵的手动3D标注监督。我们提出UnScene3D，这是针对室内扫描的无监督3D实例分割的第一个完全无监督的方法。UnScene3D首先通过利用自监督的颜色和几何特征生成伪掩模，以找到潜在的对象区域。我们基于3D分段原语进行操作，实现在高分辨率3D数据上的高效表示和学习。然后通过自我训练细化粗提议，我们的方法相对于最先进的无监督3D实例分割方法提高了超过300%的平均精度分数，展示了有效的实例分割，即使在具有挑战性的混乱的3D场景中也是如此。
101,BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation,CVPR,2024,"Jiahao Lu, Jiacheng Deng, Tianzhu Zhang",3D instance segmentation (3DIS) is a crucial task but point-level annotations are tedious in fully supervised settings. Thus using bounding boxes (bboxes) as annotations has shown great potential. The current mainstream approach is a two-step process involving the generation of pseudo-labels from box annotations and the training of a 3DIS network with the pseudo-labels. However due to the presence of intersections among bboxes not every point has a determined instance label especially in overlapping areas. To generate higher quality pseudo-labels and achieve more precise weakly supervised 3DIS results we propose the Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation (BSNet) which devises a novel pseudo-labeler called Simulation-assisted Transformer. The labeler consists of two main components. The first is Simulation-assisted Mean Teacher which introduces Mean Teacher for the first time in this task and constructs simulated samples to assist the labeler in acquiring prior knowledge about overlapping areas. To better model local-global structure we also propose Local-Global Aware Attention as the decoder for teacher and student labelers. Extensive experiments conducted on the ScanNetV2 and S3DIS datasets verify the superiority of our designs.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_BSNet_Box-Supervised_Simulation-assisted_Mean_Teacher_for_3D_Instance_Segmentation_CVPR_2024_paper.pdf,,2,为了解决 3D 实例分割（3DIS）中标注困难的问题，提出了一种新的弱监督方法 BSNet，其中包括 Simulation-assisted Transformer 伪标签生成器和 Local-Global Aware Attention 解码器。通过引入 Mean Teacher 模型和构建模拟样本，BSNet 能够更准确地生成高质量的伪标签，提升 3DIS 结果的精度。在 ScanNetV2 和 S3DIS 数据集上的实验表明，BSNet 设计的优越性。
102,SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection,CVPR,2024,"Junsu Kim, Hoseong Cho, Jihyeon Kim, Yihalem Yimolal Tiruneh, Seungryul Baek",In the field of class incremental learning (CIL) generative replay has become increasingly prominent as a method to mitigate the catastrophic forgetting alongside the continuous improvements in generative models. However its application in class incremental object detection (CIOD) has been significantly limited primarily due to the complexities of scenes involving multiple labels. In this paper we propose a novel approach called stable diffusion deep generative replay (SDDGR) for CIOD. Our method utilizes a diffusion-based generative model with pre-trained text-to-image diffusion networks to generate realistic and diverse synthetic images. SDDGR incorporates an iterative refinement strategy to produce high-quality images encompassing old classes. Additionally we adopt an L2 knowledge distillation technique to improve the retention of prior knowledge in synthetic images. Furthermore our approach includes pseudo-labeling for old objects within new task images preventing misclassification as background elements. Extensive experiments on the COCO 2017 dataset demonstrate that SDDGR significantly outperforms existing algorithms achieving a new state-of-the-art in various CIOD scenarios.,,https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_SDDGR_Stable_Diffusion-based_Deep_Generative_Replay_for_Class_Incremental_Object_CVPR_2024_paper.pdf,,1,在类别增量学习（CIL）领域，生成式重放已经成为一种在生成模型持续改进的同时减轻灾难性遗忘的方法，然而，在类别增量目标检测（CIOD）中，由于涉及多个标签的场景复杂性，其应用受到显著限制。本文提出了一种名为稳定扩散深层生成式重放（SDDGR）的新方法用于CIOD。我们的方法利用基于扩散的生成模型和预训练的文本到图像扩散网络生成逼真且多样的合成图像。SDDGR采用迭代细化策略生成包含旧类别的高质量图像。此外，我们采用L2知识蒸馏技术来提高合成图像中先前知识的保留。此外，我们的方法在新任务图像中为旧对象包含伪标签，防止其被误分类为背景元素。对COCO 2017

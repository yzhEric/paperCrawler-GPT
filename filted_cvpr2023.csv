,title,conference,year,authors,abstract,pdf_path,pdf_url,code_url,relevant,tran_flag
8,Region-Aware Pretraining for Open-Vocabulary Object Detection With Vision Transformers,CVPR,2023,"Dahun Kim, Anelia Angelova, Weicheng Kuo","We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) -- a contrastive image-text pretraining recipe to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we propose to randomly crop and resize regions of positional embeddings instead of using the whole image positional embeddings. This better matches the use of positional embeddings at region-level in the detection finetuning phase. In addition, we replace the common softmax cross entropy loss in contrastive learning with focal loss to better learn the informative yet difficult examples. Finally, we leverage recent advances in novel object proposals to improve open-vocabulary detection finetuning. We evaluate our full model on the LVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer. RO-ViT achieves a state-of-the-art 32.1 APr on LVIS, surpassing the best existing approach by +5.8 points in addition to competitive zero-shot transfer detection. Surprisingly, RO-ViT improves the image-level representation as well and achieves the state of the art on 9 out of 12 metrics on COCO and Flickr image-text retrieval benchmarks, outperforming competitive approaches with larger models.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.pdf,,1,本文介绍了一种名为 RO-ViT 的区域感知开放词汇视觉变换器的对比图像-文本预训练方法，以弥合图像级预训练和开放词汇目标检测之间的差距。在预训练阶段，作者建议随机裁剪和调整位置嵌入的区域，而不是使用整个图像的位置嵌入，以更好地匹配检测微调阶段的区域级位置嵌入的使用。此外，作者用聚焦损失替换了对比学习中常见的 softmax 交叉熵损失，以更好地学习信息丰富但难以处理的示例。最后，作者利用最近的新对象提议技术来改进开放词汇检测微调。作者在 LVIS 和 COCO 开放词汇检测基准测试和零-shot 转移上评估了完整模型。RO-ViT 在 LVIS 上实现了 32.1 APr 的最新成
13,Detecting Everything in the Open World: Towards Universal Object Detection,CVPR,2023,"Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, Hengshuang Zhao, Shengjin Wang","In this paper, we formally address universal object detection, which aims to detect every scene and predict every category. The dependence on human annotations, the limited visual information, and the novel categories in the open world severely restrict the universality of traditional detectors. We propose UniDetector, a universal object detector that has the ability to recognize enormous categories in the open world. The critical points for the universality of UniDetector are: 1) it leverages images of multiple sources and heterogeneous label spaces for training through the alignment of image and text spaces, which guarantees sufficient information for universal representations. 2) it generalizes to the open world easily while keeping the balance between seen and unseen classes, thanks to abundant information from both vision and language modalities. 3) it further promotes the generalization ability to novel categories through our proposed decoupling training manner and probability calibration. These contributions allow UniDetector to detect over 7k categories, the largest measurable category size so far, with only about 500 classes participating in training. Our UniDetector behaves the strong zero-shot generalization ability on large-vocabulary datasets like LVIS, ImageNetBoxes, and VisualGenome - it surpasses the traditional supervised baselines by more than 4% on average without seeing any corresponding images. On 13 public detection datasets with various scenes, UniDetector also achieves state-of-the-art performance with only a 3% amount of training data.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Detecting_Everything_in_the_Open_World_Towards_Universal_Object_Detection_CVPR_2023_paper.pdf,,1,本文正式探讨了通用物体检测，旨在检测每个场景并预测每个类别。传统检测器对人类注释的依赖、有限的视觉信息和开放世界中的新类别严重限制了其通用性。我们提出了UniDetector，一种通用物体检测器，具有识别开放世界中巨大类别的能力。UniDetector的通用性关键点是：1）通过图像和文本空间的对齐，利用多个来源和异构标签空间的图像进行训练，从而保证通用表示的充分信息。2）它易于推广到开放世界，同时保持已知和未知类别之间的平衡，得益于视觉和语言模态的丰富信息。3）通过我们提出的解耦训练方式和概率校准，进一步提高了对新类别的泛化能力。这些贡献使UniDetector能够检测超过7k个类别，是迄
14,Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection,CVPR,2023,"Luting Wang, Yi Liu, Penghui Du, Zihan Ding, Yue Liao, Qiaosong Qi, Biaolong Chen, Si Liu","Open-vocabulary object detection aims to provide object detectors trained on a fixed set of object categories with the generalizability to detect objects described by arbitrary text queries. Previous methods adopt knowledge distillation to extract knowledge from Pretrained Vision-and-Language Models (PVLMs) and transfer it to detectors. However, due to the non-adaptive proposal cropping and single-level feature mimicking processes, they suffer from information destruction during knowledge extraction and inefficient knowledge transfer. To remedy these limitations, we propose an Object-Aware Distillation Pyramid (OADP) framework, including an Object-Aware Knowledge Extraction (OAKE) module and a Distillation Pyramid (DP) mechanism. When extracting object knowledge from PVLMs, the former adaptively transforms object proposals and adopts object-aware mask attention to obtain precise and complete knowledge of objects. The latter introduces global and block distillation for more comprehensive knowledge transfer to compensate for the missing relation information in object distillation. Extensive experiments show that our method achieves significant improvement compared to current methods. Especially on the MS-COCO dataset, our OADP framework reaches 35.6 mAP^N_50, surpassing the current state-of-the-art method by 3.3 mAP^N_50. Code is anonymously provided in the supplementary materials.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.pdf,,1,本文提出了一种开放词汇目标检测方法，旨在为固定对象类别的对象检测器提供检测任意文本查询描述的对象的通用性。以往的方法采用知识蒸馏从预训练的视觉语言模型中提取知识并将其转移到检测器中。然而，由于非自适应的提议裁剪和单层特征模仿过程，它们在知识提取和知识转移过程中遭受信息破坏和效率低下的问题。为了解决这些限制，我们提出了一种对象感知蒸馏金字塔（OADP）框架，包括对象感知知识提取（OAKE）模块和蒸馏金字塔（DP）机制。在从PVLMs中提取对象知识时，前者自适应地转换对象提议并采用对象感知掩码注意力来获取精确和完整的对象知识。后者引入全局和块
20,PROB: Probabilistic Objectness for Open World Object Detection,CVPR,2023,"Orr Zohar, Kuan-Chieh Wang, Serena Yeung","Open World Object Detection (OWOD) is a new and challenging computer vision task that bridges the gap between classic object detection (OD) benchmarks and object detection in the real world. In addition to detecting and classifying seen/labeled objects, OWOD algorithms are expected to detect novel/unknown objects - which can be classified and incrementally learned. In standard OD, object proposals not overlapping with a labeled object are automatically classified as background. Therefore, simply applying OD methods to OWOD fails as unknown objects would be predicted as background. The challenge of detecting unknown objects stems from the lack of supervision in distinguishing unknown objects and background object proposals. Previous OWOD methods have attempted to overcome this issue by generating supervision using pseudo-labeling - however, unknown object detection has remained low. Probabilistic/generative models may provide a solution for this challenge. Herein, we introduce a novel probabilistic framework for objectness estimation, where we alternate between probability distribution estimation and objectness likelihood maximization of known objects in the embedded feature space - ultimately allowing us to estimate the objectness probability of different proposals. The resulting Probabilistic Objectness transformer-based open-world detector, PROB, integrates our framework into traditional object detection models, adapting them for the open-world setting. Comprehensive experiments on OWOD benchmarks show that PROB outperforms all existing OWOD methods in both unknown object detection ( 2x unknown recall) and known object detection (  mAP). Our code is available at https://github.com/orrzohar/PROB.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Zohar_PROB_Probabilistic_Objectness_for_Open_World_Object_Detection_CVPR_2023_paper.pdf,https://github.com/orrzohar/PROB,1,Open World Object Detection (OWOD) 是一项新的、具有挑战性的计算机视觉任务，它弥合了经典目标检测 (OD) 基准和现实世界中的目标检测之间的差距。除了检测和分类已知的对象，OWOD 算法还需要检测新的/未知的对象 - 这些对象可以被分类和逐步学习。在标准的 OD 中，与标记对象不重叠的对象提议会自动分类为背景。因此，简单地将 OD 方法应用于 OWOD 会失败，因为未知的对象会被预测为背景。检测未知对象的挑战源于在区分未知对象和背景对象提议时缺乏监督。以前的 OWOD 方法尝试通过生成伪标签来克服这个问题，但未知对象的检测仍然很低。概率/生成模型可能为这个挑战提供解决方案。在这里，我们介绍了一种新颖的概率
24,Annealing-Based Label-Transfer Learning for Open World Object Detection,CVPR,2023,"Yuqing Ma, Hainan Li, Zhange Zhang, Jinyang Guo, Shanghang Zhang, Ruihao Gong, Xianglong Liu","Open world object detection (OWOD) has attracted extensive attention due to its practicability in the real world. Previous OWOD works manually designed unknown-discover strategies to select unknown proposals from the background, suffering from uncertainties without appropriate priors. In this paper, we claim the learning of object detection could be seen as an object-level feature-entanglement process, where unknown traits are propagated to the known proposals through convolutional operations and could be distilled to benefit unknown recognition without manual selection. Therefore, we propose a simple yet effective Annealing-based Label-Transfer framework, which sufficiently explores the known proposals to alleviate the uncertainties. Specifically, a Label-Transfer Learning paradigm is introduced to decouple the known and unknown features, while a Sawtooth Annealing Scheduling strategy is further employed to rebuild the decision boundaries of the known and unknown classes, thus promoting both known and unknown recognition. Moreover, previous OWOD works neglected the trade-off of known and unknown performance, and we thus introduce a metric called Equilibrium Index to comprehensively evaluate the effectiveness of the OWOD models. To the best of our knowledge, this is the first OWOD work without manual unknown selection. Extensive experiments conducted on the common-used benchmark validate that our model achieves superior detection performance (200% unknown mAP improvement with the even higher known detection performance) compared to other state-of-the-art methods. Our code is available at https://github.com/DIG-Beihang/ALLOW.git.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_Annealing-Based_Label-Transfer_Learning_for_Open_World_Object_Detection_CVPR_2023_paper.pdf,https://github.com/DIG-Beihang/ALLOW.git,1,本文提出了一种基于学习的目标检测方法，称之为Annealing-based Label-Transfer framework。该方法通过卷积操作将未知特征传播到已知提案中，并通过Label-Transfer Learning范式来解耦已知和未知特征，进而通过Sawtooth Annealing Scheduling策略重建已知和未知类别的决策边界，从而促进已知和未知识别。此外，本文还引入了一个称为Equilibrium Index的度量来全面评估OWOD模型的有效性。实验结果表明，该模型相比其他最先进的方法具有更好的检测性能。
25,Aligning Bag of Regions for Open-Vocabulary Object Detection,CVPR,2023,"Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, Chen Change Loy","Pre-trained vision-language models (VLMs) learn to align vision and language representations on large-scale datasets, where each image-text pair usually contains a bag of semantic concepts. However, existing open-vocabulary object detectors only align region embeddings individually with the corresponding features extracted from the VLMs. Such a design leaves the compositional structure of semantic concepts in a scene under-exploited, although the structure may be implicitly learned by the VLMs. In this work, we propose to align the embedding of bag of regions beyond individual regions. The proposed method groups contextually interrelated regions as a bag. The embeddings of regions in a bag are treated as embeddings of words in a sentence, and they are sent to the text encoder of a VLM to obtain the bag-of-regions embedding, which is learned to be aligned to the corresponding features extracted by a frozen VLM. Applied to the commonly used Faster R-CNN, our approach surpasses the previous best results by 4.6 box AP 50 and 2.8 mask AP on novel categories of open-vocabulary COCO and LVIS benchmarks, respectively. Code and models are available at https://github.com/wusize/ovdet.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Aligning_Bag_of_Regions_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.pdf,https://github.com/wusize/ovdet,1,本文提出了一种新的方法，将场景中语义概念的组合结构与视觉和语言表示对齐。现有的开放词汇对象检测器只能将区域嵌入与VLMs提取的相应特征单独对齐，而不能充分利用场景中语义概念的组合结构。本文提出的方法将上下文相关的区域分组为一个包，将包中的区域嵌入视为句子中的单词嵌入，并将其发送到VLM的文本编码器中，以获得区域包嵌入，该嵌入被学习与冻结的VLM提取的相应特征对齐。在常用的Faster R-CNN上应用本方法，我们的方法在开放词汇COCO和LVIS基准测试的新类别上超过了以前的最佳结果，分别为4.6个盒子AP 50和2.8个掩码AP。代码和模型可在https://github.com/wusize/ovdet上
28,ScaleDet: A Scalable Multi-Dataset Object Detector,CVPR,2023,"Yanbei Chen, Manchen Wang, Abhay Mittal, Zhenlin Xu, Paolo Favaro, Joseph Tighe, Davide Modolo","Multi-dataset training provides a viable solution for exploiting heterogeneous large-scale datasets without extra annotation cost. In this work, we propose a scalable multi-dataset detector (ScaleDet) that can scale up its generalization across datasets when increasing the number of training datasets. Unlike existing multi-dataset learners that mostly rely on manual relabelling efforts or sophisticated optimizations to unify labels across datasets, we introduce a simple yet scalable formulation to derive a unified semantic label space for multi-dataset training. ScaleDet is trained by visual-textual alignment to learn the label assignment with label semantic similarities across datasets. Once trained, ScaleDet can generalize well on any given upstream and downstream datasets with seen and unseen classes. We conduct extensive experiments using LVIS, COCO, Objects365, OpenImages as upstream datasets, and 13 datasets from Object Detection in the Wild (ODinW) as downstream datasets. Our results show that ScaleDet achieves compelling strong model performance with an mAP of 50.7 on LVIS, 58.8 on COCO, 46.8 on Objects365, 76.2 on OpenImages, and 71.8 on ODinW, surpassing state-of-the-art detectors with the same backbone.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_ScaleDet_A_Scalable_Multi-Dataset_Object_Detector_CVPR_2023_paper.pdf,,1,本文提出了一种可扩展的多数据集检测器（ScaleDet），通过视觉-文本对齐训练来学习标签分配，从而实现跨数据集的泛化。与现有的多数据集学习器不同，ScaleDet引入了一种简单但可扩展的公式，用于推导多数据集训练的统一语义标签空间。一旦训练完成，ScaleDet可以在任何给定的上游和下游数据集上进行良好的泛化，包括已知和未知类别。实验结果表明，ScaleDet在多个数据集上均取得了优异的性能，超过了具有相同骨干的最先进检测器。
29,Dense Distinct Query for End-to-End Object Detection,CVPR,2023,"Shilong Zhang, Xinjiang Wang, Jiaqi Wang, Jiangmiao Pang, Chengqi Lyu, Wenwei Zhang, Ping Luo, Kai Chen","One-to-one label assignment in object detection has successfully obviated the need of non-maximum suppression (NMS) as a postprocessing and makes the pipeline end-to-end. However, it triggers a new dilemma as the widely used sparse queries cannot guarantee a high recall, while dense queries inevitably bring more similar queries and encounters optimization difficulty. As both sparse and dense queries are problematic, then what are the expected queries in end-to-end object detection? This paper shows that the solution should be Dense Distinct Queries (DDQ). Concretely, we first lay dense queries like traditional detectors and then select distinct ones for one-to-one assignments. DDQ blends the advantages of traditional and recent end-to-end detectors and significantly improves the performance of various detectors including FCN, R-CNN, and DETRs. Most impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12 epochs using a ResNet-50 backbone, outperforming all existing detectors in the same setting. DDQ also shares the benefit of end-to-end detectors in crowded scenes and achieves 93.8 AP on CrowdHuman. We hope DDQ can inspire researchers to consider the complementarity between traditional methods and end-to-end detectors. The source code can be found at https://github.com/jshilong/DDQ.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Dense_Distinct_Query_for_End-to-End_Object_Detection_CVPR_2023_paper.pdf,https://github.com/jshilong/DDQ,1,本文介绍了在目标检测中一对一标签分配的成功应用，成功地避免了后处理中的非极大值抑制（NMS），使得流程成为端到端。然而，这也引发了一个新的问题，即广泛使用的稀疏查询无法保证高召回率，而密集查询则不可避免地带来更多相似的查询和遇到优化困难。既然稀疏和密集查询都有问题，那么端到端目标检测中应该使用什么样的查询呢？本文表明，解决方案应该是密集不同查询（DDQ）。具体来说，我们首先像传统检测器一样放置密集查询，然后为一对一分配选择不同的查询。DDQ融合了传统和最近的端到端检测器的优点，并显著提高了各种检测器的性能，包括FCN、R-CNN和DETRs。最令人印象深刻的是，DDQ-DE
35,DynamicDet: A Unified Dynamic Architecture for Object Detection,CVPR,2023,"Zhihao Lin, Yongtao Wang, Jinhe Zhang, Xiaojie Chu","Dynamic neural network is an emerging research topic in deep learning. With adaptive inference, dynamic models can achieve remarkable accuracy and computational efficiency. However, it is challenging to design a powerful dynamic detector, because of no suitable dynamic architecture and exiting criterion for object detection. To tackle these difficulties, we propose a dynamic framework for object detection, named DynamicDet. Firstly, we carefully design a dynamic architecture based on the nature of the object detection task. Then, we propose an adaptive router to analyze the multi-scale information and to decide the inference route automatically. We also present a novel optimization strategy with an exiting criterion based on the detection losses for our dynamic detectors. Last, we present a variable-speed inference strategy, which helps to realize a wide range of accuracy-speed trade-offs with only one dynamic detector. Extensive experiments conducted on the COCO benchmark demonstrate that the proposed DynamicDet achieves new state-of-the-art accuracy-speed trade-offs. For instance, with comparable accuracy, the inference speed of our dynamic detector Dy-YOLOv7-W6 surpasses YOLOv7-E6 by 12%, YOLOv7-D6 by 17%, and YOLOv7-E6E by 39%. The code is available at https://github.com/VDIGPKU/DynamicDet.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_DynamicDet_A_Unified_Dynamic_Architecture_for_Object_Detection_CVPR_2023_paper.pdf,https://github.com/VDIGPKU/DynamicDet,1,本文介绍了一种名为DynamicDet的动态目标检测框架，该框架基于自适应推理，能够实现卓越的准确性和计算效率。作者首先根据目标检测任务的特点，精心设计了动态架构，然后提出了一种自适应路由器来分析多尺度信息并自动决定推理路线。此外，作者还提出了一种基于检测损失的新型优化策略和退出标准，用于动态检测器。最后，作者提出了一种可变速推理策略，可以在只有一个动态检测器的情况下实现广泛的准确度和速度权衡。实验结果表明，DynamicDet在COCO基准测试中实现了新的最先进的准确度和速度权衡。例如，与相似的准确度相比，我们的动态检测器Dy-YOLOv7-W6的推理速度比YOLOv7-E6快12％，比
40,CORA: Adapting CLIP for Open-Vocabulary Detection With Region Prompting and Anchor Pre-Matching,CVPR,2023,"Xiaoshi Wu, Feng Zhu, Rui Zhao, Hongsheng Li","Open-vocabulary detection (OVD) is an object detection task aiming at detecting objects from novel categories beyond the base categories on which the detector is trained. Recent OVD methods rely on large-scale visual-language pre-trained models, such as CLIP, for recognizing novel objects. We identify the two core obstacles that need to be tackled when incorporating these models into detector training: (1) the distribution mismatch that happens when applying a VL-model trained on whole images to region recognition tasks; (2) the difficulty of localizing objects of unseen classes. To overcome these obstacles, we propose CORA, a DETR-style framework that adapts CLIP for Open-vocabulary detection by Region prompting and Anchor pre-matching. Region prompting mitigates the whole-to-region distribution gap by prompting the region features of the CLIP-based region classifier. Anchor pre-matching helps learning generalizable object localization by a class-aware matching mechanism. We evaluate CORA on the COCO OVD benchmark, where we achieve 41.7 AP50 on novel classes, which outperforms the previous SOTA by 2.4 AP50 even without resorting to extra training data. When extra training data is available, we train CORA+ on both ground-truth base-category annotations and additional pseudo bounding box labels computed by CORA. CORA+ achieves 43.1 AP50 on the COCO OVD benchmark and 28.1 box APr on the LVIS OVD benchmark. The code is available at https://github.com/tgxs002/CORA.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_CORA_Adapting_CLIP_for_Open-Vocabulary_Detection_With_Region_Prompting_and_CVPR_2023_paper.pdf,https://github.com/tgxs002/CORA,1,Open-vocabulary detection (OVD) 是一项目标检测任务，旨在检测训练检测器的基本类别之外的新类别的对象。最近的 OVD 方法依赖于大规模的视觉语言预训练模型，例如 CLIP，用于识别新对象。当将这些模型纳入检测器训练时，我们确定需要解决的两个核心障碍：（1）将在整个图像上训练的 VL 模型应用于区域识别任务时发生的分布不匹配；（2）定位未见过类别的对象的困难。为了克服这些障碍，我们提出了 CORA，一种 DETR 风格的框架，通过区域提示和锚点预匹配来适应 CLIP 用于开放词汇检测。区域提示通过提示基于 CLIP 的区域分类器的区域特征来减轻整体到区域分布差距。锚点预匹配通过一种类别感知的
43,Learning To Detect and Segment for Open Vocabulary Object Detection,CVPR,2023,Tao Wang,"Open vocabulary object detection has been greately advanced by the recent development of vision-language pre-trained model, which helps recognizing the novel objects with only semantic categories. The prior works mainly focus on knowledge transferring to the object proposal classification and employ class-agnostic box and mask prediction. In this work, we propose CondHead, a principled dynamic network design to better generalize the box regression and mask segmentation for open vocabulary setting. The core idea is to conditionally parametrize the network heads on semantic embedding and thus the model is guided with class-specific knowledge to better detect novel categories. Specifically, CondHead is composed of two streams of network heads, the dynamically aggregated heads and dynamically generated heads. The former is instantiated with a set of static heads that are conditionally aggregated, these heads are optimized as experts and are expected to learn sophisticated prediction. The Latter is instantiated with dynamically generated parameters and encodes general class-specific information. With such conditional design, the detection model is bridged by the semantic embedding to offer strongly generalizable class-wise box and mask prediction. Our method brings significant improvement to the prior state-of-the-art open vocabulary object detection methods with very minor overhead, e.g., it surpasses a RegionClip model by 3.0 detection AP on novel categories, with only 1.1% more computation.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Learning_To_Detect_and_Segment_for_Open_Vocabulary_Object_Detection_CVPR_2023_paper.pdf,,1,最近视觉语言预训练模型的发展极大地推进了开放词汇物体检测，有助于仅通过语义类别识别新颖物体。先前的工作主要集中在知识转移到对象提议分类上，并采用类不可知的框和掩码预测。在这项工作中，我们提出了CondHead，一种基于原则的动态网络设计，以更好地推广开放词汇设置的框回归和掩码分割。核心思想是在语义嵌入上有条件地参数化网络头，从而模型在类特定知识的指导下更好地检测新颖类别。具体而言，CondHead由两个网络头流组成，即动态聚合头和动态生成头。前者由一组静态头实例化，这些头被有条件地聚合，这些头被优化为专家，并期望学习复杂的预测。后者由动态生成的参数实例化，并编码一般的类特定信息。
45,CLIP the Gap: A Single Domain Generalization Approach for Object Detection,CVPR,2023,"Vidit Vidit, Martin Engilberge, Mathieu Salzmann","Single Domain Generalization (SDG) tackles the problem of training a model on a single source domain so that it generalizes to any unseen target domain. While this has been well studied for image classification, the literature on SDG object detection remains almost non-existent. To address the challenges of simultaneously learning robust object localization and representation, we propose to leverage a pre-trained vision-language model to introduce semantic domain concepts via textual prompts. We achieve this via a semantic augmentation strategy acting on the features extracted by the detector backbone, as well as a text-based classification loss. Our experiments evidence the benefits of our approach, outperforming by 10% the only existing SDG object detection method, Single-DGOD[49], on their own diverse weather-driving benchmark.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Vidit_CLIP_the_Gap_A_Single_Domain_Generalization_Approach_for_Object_CVPR_2023_paper.pdf,,1,本文介绍了单域泛化（SDG）的概念，即在单个源域上训练模型，使其能够推广到任何未见过的目标域。虽然这在图像分类方面已经得到了广泛研究，但在SDG目标检测方面的文献几乎不存在。为了解决同时学习鲁棒的目标定位和表示的挑战，我们提出利用预训练的视觉语言模型通过文本提示引入语义域概念。我们通过对检测器骨干提取的特征进行语义增强策略以及基于文本的分类损失来实现这一点。我们的实验表明了我们方法的优势，在自己的多样化天气驾驶基准测试中，我们的方法比唯一现有的SDG目标检测方法Single-DGOD[49]表现提高了10%。
47,OneFormer: One Transformer To Rule Universal Image Segmentation,CVPR,2023,"Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi","Universal Image Segmentation is not a new concept.Past attempts to unify image segmentation include scene parsing, panoptic segmentation, and, more recently, new panoptic architectures. However, such panoptic architectures do not truly unify image segmentation because they need to be trained individually on the semantic, instance, or panoptic segmentation to achieve the best performance. Ideally, a truly universal framework should be trained only once and achieve SOTA performance across all three image segmentation tasks. To that end, we propose OneFormer, a universal image segmentation framework that unifies segmentation with a multi-task train-once design. We first propose a task-conditioned joint training strategy that enables training on ground truths of each domain (semantic, instance, and panoptic segmentation) within a single multi-task training process. Secondly, we introduce a task token to condition our model on the task at hand, making our model task-dynamic to support multi-task training and inference. Thirdly, we propose using a query-text contrastive loss during training to establish better inter-task and inter-class distinctions. Notably, our single OneFormer model outperforms specialized Mask2Former models across all three segmentation tasks on ADE20k, Cityscapes, and COCO, despite the latter being trained on each task individually. We believe OneFormer is a significant step towards making image segmentation more universal and accessible.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_OneFormer_One_Transformer_To_Rule_Universal_Image_Segmentation_CVPR_2023_paper.pdf,,1,本文介绍了一种名为 OneFormer 的通用图像分割框架，旨在通过多任务训练一次来实现真正的图像分割统一。该框架采用任务条件联合训练策略，引入任务令牌来支持多任务训练和推理，并在训练过程中使用查询文本对比损失来建立更好的任务和类别区分。实验结果表明，OneFormer 模型在 ADE20k、Cityscapes 和 COCO 数据集上的表现优于专门的 Mask2Former 模型，尽管后者是在每个任务上单独训练的。作者认为，OneFormer 是使图像分割更加通用和易于使用的重要一步。
57,Mask-Free OVIS: Open-Vocabulary Instance Segmentation Without Manual Mask Annotations,CVPR,2023,"Vibashan VS, Ning Yu, Chen Xing, Can Qin, Mingfei Gao, Juan Carlos Niebles, Vishal M. Patel, Ran Xu","Existing instance segmentation models learn task-specific information using manual mask annotations from base (training) categories. These mask annotations require tremendous human effort, limiting the scalability to annotate novel (new) categories. To alleviate this problem, Open-Vocabulary (OV) methods leverage large-scale image-caption pairs and vision-language models to learn novel categories. In summary, an OV method learns task-specific information using strong supervision from base annotations and novel category information using weak supervision from image-captions pairs. This difference between strong and weak supervision leads to overfitting on base categories, resulting in poor generalization towards novel categories. In this work, we overcome this issue by learning both base and novel categories from pseudo-mask annotations generated by the vision-language model in a weakly supervised manner using our proposed Mask-free OVIS pipeline. Our method automatically generates pseudo-mask annotations by leveraging the localization ability of a pre-trained vision-language model for objects present in image-caption pairs. The generated pseudo-mask annotations are then used to supervise an instance segmentation model, freeing the entire pipeline from any labour-expensive instance-level annotations and overfitting. Our extensive experiments show that our method trained with just pseudo-masks significantly improves the mAP scores on the MS-COCO dataset and OpenImages dataset compared to the recent state-of-the-art methods trained with manual masks. Codes and models are provided in https://vibashan.github.io/ovis-web/.",,https://openaccess.thecvf.com/content/CVPR2023/papers/VS_Mask-Free_OVIS_Open-Vocabulary_Instance_Segmentation_Without_Manual_Mask_Annotations_CVPR_2023_paper.pdf,,1,本文介绍了一种名为 Mask-free OVIS 的实例分割方法，该方法使用预训练的视觉语言模型生成伪掩码注释，从而实现了对基础和新颖类别的弱监督学习。通过使用伪掩码注释，该方法不需要昂贵的实例级注释，并且可以避免在基础类别上过拟合，从而提高了对新颖类别的泛化能力。实验结果表明，该方法在 MS-COCO 数据集和 OpenImages 数据集上的 mAP 得分显著优于使用手动掩码训练的最新技术方法。
60,FastInst: A Simple Query-Based Model for Real-Time Instance Segmentation,CVPR,2023,"Junjie He, Pengyu Li, Yifeng Geng, Xuansong Xie","Recent attention in instance segmentation has focused on query-based models. Despite being non-maximum suppression (NMS)-free and end-to-end, the superiority of these models on high-accuracy real-time benchmarks has not been well demonstrated. In this paper, we show the strong potential of query-based models on efficient instance segmentation algorithm designs. We present FastInst, a simple, effective query-based framework for real-time instance segmentation. FastInst can execute at a real-time speed (i.e., 32.5 FPS) while yielding an AP of more than 40 (i.e., 40.5 AP) on COCO test-dev without bells and whistles. Specifically, FastInst follows the meta-architecture of recently introduced Mask2Former. Its key designs include instance activation-guided queries, dual-path update strategy, and ground truth mask-guided learning, which enable us to use lighter pixel decoders, fewer Transformer decoder layers, while achieving better performance. The experiments show that FastInst outperforms most state-of-the-art real-time counterparts, including strong fully convolutional baselines, in both speed and accuracy. Code can be found at https://github.com/junjiehe96/FastInst.",,https://openaccess.thecvf.com/content/CVPR2023/papers/He_FastInst_A_Simple_Query-Based_Model_for_Real-Time_Instance_Segmentation_CVPR_2023_paper.pdf,https://github.com/junjiehe96/FastInst,1,本文介绍了一种基于查询的实例分割算法框架FastInst，该框架具有实时性和高准确度的特点。FastInst采用实例激活引导查询、双路径更新策略和地面真实掩码引导学习等关键设计，使得其在使用更轻量级的像素解码器和更少的Transformer解码器层的同时，能够实现更好的性能。实验结果表明，FastInst在速度和准确度方面均优于大多数最先进的实时对应物，包括强大的全卷积基线。该算法的代码可在https://github.com/junjiehe96/FastInst找到。
66,DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-Training via Word-Region Alignment,CVPR,2023,"Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Hang Xu","This paper presents DetCLIPv2, an efficient and scalable training framework that incorporates large-scale image-text pairs to achieve open-vocabulary object detection (OVD). Unlike previous OVD frameworks that typically rely on a pre-trained vision-language model (e.g., CLIP) or exploit image-text pairs via a pseudo labeling process, DetCLIPv2 directly learns the fine-grained word-region alignment from massive image-text pairs in an end-to-end manner. To accomplish this, we employ a maximum word-region similarity between region proposals and textual words to guide the contrastive objective. To enable the model to gain localization capability while learning broad concepts, DetCLIPv2 is trained with a hybrid supervision from detection, grounding and image-text pair data under a unified data formulation. By jointly training with an alternating scheme and adopting low-resolution input for image-text pairs, DetCLIPv2 exploits image-text pair data efficiently and effectively: DetCLIPv2 utilizes 13x more image-text pairs than DetCLIP with a similar training time and improves performance. With 13M image-text pairs for pre-training, DetCLIPv2 demonstrates superior open-vocabulary detection performance, e.g., DetCLIPv2 with Swin-T backbone achieves 40.4% zero-shot AP on the LVIS benchmark, which outperforms previous works GLIP/GLIPv2/DetCLIP by 14.4/11.4/4.5% AP, respectively, and even beats its fully-supervised counterpart by a large margin.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_DetCLIPv2_Scalable_Open-Vocabulary_Object_Detection_Pre-Training_via_Word-Region_Alignment_CVPR_2023_paper.pdf,,1,DetCLIPv2是一个高效且可扩展的训练框架，它利用大规模的图像-文本对实现了开放词汇量的目标检测（OVD）。与以往的OVD框架不同，DetCLIPv2直接从海量的图像-文本对中学习细粒度的单词-区域对齐，而不是依赖于预训练的视觉-语言模型（例如CLIP）或通过伪标签过程利用图像-文本对。为了实现这一点，DetCLIPv2采用了区域提议和文本单词之间的最大单词-区域相似度来指导对比目标。为了使模型在学习广泛概念的同时获得定位能力，DetCLIPv2在统一的数据公式下通过检测、定位和图像-文本对数据的混合监督进行训练。通过联合训练和采用低分辨率输入的交替方案，DetCLIPv2
67,CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection,CVPR,2023,"Shuailei Ma, Yuefeng Wang, Ying Wei, Jiaqi Fan, Thomas H. Li, Hongli Liu, Fanbing Lv","Open-world object detection (OWOD), as a more general and challenging goal, requires the model trained from data on known objects to detect both known and unknown objects and incrementally learn to identify these unknown objects. The existing works which employ standard detection framework and fixed pseudo-labelling mechanism (PLM) have the following problems: (i) The inclusion of detecting unknown objects substantially reduces the model's ability to detect known ones. (ii) The PLM does not adequately utilize the priori knowledge of inputs. (iii) The fixed selection manner of PLM cannot guarantee that the model is trained in the right direction. We observe that humans subconsciously prefer to focus on all foreground objects and then identify each one in detail, rather than localize and identify a single object simultaneously, for alleviating the confusion. This motivates us to propose a novel solution called CAT: LoCalization and IdentificAtion Cascade Detection Transformer which decouples the detection process via the shared decoder in the cascade decoding way. In the meanwhile, we propose the self-adaptive pseudo-labelling mechanism which combines the model-driven with input-driven PLM and self-adaptively generates robust pseudo-labels for unknown objects, significantly improving the ability of CAT to retrieve unknown objects. Comprehensive experiments on two benchmark datasets, i.e., MS-COCO and PASCAL VOC, show that our model outperforms the state-of-the-art in terms of all metrics in the task of OWOD, incremental object detection (IOD) and open-set detection.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_CAT_LoCalization_and_IdentificAtion_Cascade_Detection_Transformer_for_Open-World_Object_CVPR_2023_paper.pdf,,1,本文提出了一种名为CAT的新型解决方案，即LoCalization and IdentificAtion Cascade Detection Transformer，用于开放世界目标检测（OWOD）。该方案通过级联解码方式将检测过程解耦，并提出了自适应伪标签机制，结合模型驱动和输入驱动的PLM，自适应生成强健的未知对象伪标签，显著提高了CAT检索未知对象的能力。在MS-COCO和PASCAL VOC两个基准数据集上进行的全面实验表明，我们的模型在OWOD、增量目标检测（IOD）和开放集检测任务的所有指标方面均优于现有技术水平。
70,ScaleKD: Distilling Scale-Aware Knowledge in Small Object Detector,CVPR,2023,"Yichen Zhu, Qiqi Zhou, Ning Liu, Zhiyuan Xu, Zhicai Ou, Xiaofeng Mou, Jian Tang","Despite the prominent success of general object detection, the performance and efficiency of Small Object Detection (SOD) are still unsatisfactory. Unlike existing works that struggle to balance the trade-off between inference speed and SOD performance, in this paper, we propose a novel Scale-aware Knowledge Distillation (ScaleKD), which transfers knowledge of a complex teacher model to a compact student model. We design two novel modules to boost the quality of knowledge transfer in distillation for SOD: 1) a scale-decoupled feature distillation module that disentangled teacher's feature representation into multi-scale embedding that enables explicit feature mimicking of the student model on small objects. 2) a cross-scale assistant to refine the noisy and uninformative bounding boxes prediction student models, which can mislead the student model and impair the efficacy of knowledge distillation. A multi-scale cross-attention layer is established to capture the multi-scale semantic information to improve the student model. We conduct experiments on COCO and VisDrone datasets with diverse types of models, i.e., two-stage and one-stage detectors, to evaluate our proposed method. Our ScaleKD achieves superior performance on general detection performance and obtains spectacular improvement regarding the SOD performance.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_ScaleKD_Distilling_Scale-Aware_Knowledge_in_Small_Object_Detector_CVPR_2023_paper.pdf,,1,本文提出了一种新的尺度感知知识蒸馏方法（Scale-aware Knowledge Distillation，ScaleKD），将复杂的教师模型的知识转移给紧凑的学生模型，以提高小目标检测（SOD）的性能和效率。与现有的工作不同，该方法不仅能够平衡推理速度和SOD性能之间的权衡，还设计了两个新模块来提高知识蒸馏的质量：1）尺度解耦特征蒸馏模块，将教师的特征表示解耦成多尺度嵌入，使得学生模型能够明确地模仿小目标的特征。2）跨尺度辅助模块，用于改善学生模型的嘈杂和无信息的边界框预测，以提高知识蒸馏的效果。实验结果表明，该方法在COCO和VisDrone数据集上的性能优于现有方法，特别是在SOD
52,Enhanced Training of Query-Based Object Detection via Selective Query Recollection,CVPR,2023,"Fangyi Chen, Han Zhang, Kai Hu, Yu-Kai Huang, Chenchen Zhu, Marios Savvides","This paper investigates a phenomenon where query-based object detectors mispredict at the last decoding stage while predicting correctly at an intermediate stage. We review the training process and attribute the overlooked phenomenon to two limitations: lack of training emphasis and cascading errors from decoding sequence. We design and present Selective Query Recollection (SQR), a simple and effective training strategy for query-based object detectors. It cumulatively collects intermediate queries as decoding stages go deeper and selectively forwards the queries to the downstream stages aside from the sequential structure. Such-wise, SQR places training emphasis on later stages and allows later stages to work with intermediate queries from earlier stages directly. SQR can be easily plugged into various query-based object detectors and significantly enhances their performance while leaving the inference pipeline unchanged. As a result, we apply SQR on Adamixer, DAB-DETR, and Deformable-DETR across various settings (backbone, number of queries, schedule) and consistently brings 1.4   2.8 AP improvement.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Enhanced_Training_of_Query-Based_Object_Detection_via_Selective_Query_Recollection_CVPR_2023_paper.pdf,,1,本文研究了一种现象，即在预测中间阶段正确的情况下，在最后解码阶段查询基础目标检测器会出现误判。我们回顾了训练过程，并将被忽视的现象归因于两个限制：缺乏训练重点和解码序列中的级联错误。我们设计并提出了选择性查询回收（SQR），这是一种简单而有效的查询基础目标检测器训练策略。它在解码阶段越深入时累积收集中间查询，并选择性地将查询转发到下游阶段，而不是按顺序结构。因此，SQR将训练重点放在后期阶段，并允许后期阶段直接使用来自早期阶段的中间查询。SQR可以轻松地插入各种查询基础目标检测器中，并显著提高它们的性能，同时不改变推理流程。因此，我们在Adamixer、DAB
0,Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors,CVPR,2023,"Gongjie Zhang, Zhipeng Luo, Zichen Tian, Jingyi Zhang, Xiaoqin Zhang, Shijian Lu","Multi-scale features have been proven highly effective for object detection but often come with huge and even prohibitive extra computation costs, especially for the recent Transformer-based detectors. In this paper, we propose Iterative Multi-scale Feature Aggregation (IMFA) - a generic paradigm that enables efficient use of multi-scale features in Transformer-based object detectors. The core idea is to exploit sparse multi-scale features from just a few crucial locations, and it is achieved with two novel designs. First, IMFA rearranges the Transformer encoder-decoder pipeline so that the encoded features can be iteratively updated based on the detection predictions. Second, IMFA sparsely samples scale-adaptive features for refined detection from just a few keypoint locations under the guidance of prior detection predictions. As a result, the sampled multi-scale features are sparse yet still highly beneficial for object detection. Extensive experiments show that the proposed IMFA boosts the performance of multiple Transformer-based object detectors significantly yet with only slight computational overhead.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Towards_Efficient_Use_of_Multi-Scale_Features_in_Transformer-Based_Object_Detectors_CVPR_2023_paper.pdf,,1,本文提出了一种名为迭代多尺度特征聚合（IMFA）的通用范例，旨在使基于Transformer的目标检测器能够高效地利用多尺度特征。IMFA的核心思想是利用来自仅有几个关键位置的稀疏多尺度特征，并通过两个新颖的设计实现。首先，IMFA重新排列Transformer编码器-解码器管道，以便根据检测预测迭代地更新编码特征。其次，IMFA在先前的检测预测指导下，从仅有几个关键点位置稀疏地采样尺度自适应特征以进行精细检测。因此，采样的多尺度特征是稀疏的，但仍对目标检测非常有益。大量实验表明，所提出的IMFA显著提高了多个基于Transformer的目标检测器的性能，但只有轻微的计算开销。
1,Adaptive Sparse Convolutional Networks With Global Context Enhancement for Faster Object Detection on Drone Images,CVPR,2023,"Bowei Du, Yecheng Huang, Jiaxin Chen, Di Huang","Object detection on drone images with low-latency is an important but challenging task on the resource-constrained unmanned aerial vehicle (UAV) platform. This paper investigates optimizing the detection head based on the sparse convolution, which proves effective in balancing the accuracy and efficiency. Nevertheless, it suffers from inadequate integration of contextual information of tiny objects as well as clumsy control of the mask ratio in the presence of foreground with varying scales. To address the issues above, we propose a novel global context-enhanced adaptive sparse convolutional network (CEASC). It first develops a context-enhanced group normalization (CE-GN) layer, by replacing the statistics based on sparsely sampled features with the global contextual ones, and then designs an adaptive multi-layer masking strategy to generate optimal mask ratios at distinct scales for compact foreground coverage, promoting both the accuracy and efficiency. Extensive experimental results on two major benchmarks, i.e. VisDrone and UAVDT, demonstrate that CEASC remarkably reduces the GFLOPs and accelerates the inference procedure when plugging into the typical state-of-the-art detection frameworks (e.g. RetinaNet and GFL V1) with competitive performance. Code is available at https://github.com/Cuogeihong/CEASC.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Adaptive_Sparse_Convolutional_Networks_With_Global_Context_Enhancement_for_Faster_CVPR_2023_paper.pdf,https://github.com/Cuogeihong/CEASC,1,本文研究了在资源受限的无人机平台上进行低延迟目标检测的优化方法。通过基于稀疏卷积的检测头优化，可以在准确性和效率之间取得平衡。然而，由于微小物体的上下文信息集成不足以及前景尺度变化时掩模比例的笨拙控制，该方法存在一些问题。为了解决这些问题，本文提出了一种新颖的全局上下文增强自适应稀疏卷积网络（CEASC）。它首先开发了一个上下文增强组归一化（CE-GN）层，通过用全局上下文替换基于稀疏采样特征的统计信息，从而生成最佳掩模比例，以促进准确性和效率。在两个主要基准测试中，即VisDrone和UAVDT上的广泛实验结果表明，CEASC显著降低了GFLOPs并加速了推理
2,Boosting Low-Data Instance Segmentation by Unsupervised Pre-Training With Saliency Prompt,CVPR,2023,"Hao Li, Dingwen Zhang, Nian Liu, Lechao Cheng, Yalun Dai, Chao Zhang, Xinggang Wang, Junwei Han","Recently, inspired by DETR variants, query-based end-to-end instance segmentation (QEIS) methods have outperformed CNN-based models on large-scale datasets. Yet they would lose efficacy when only a small amount of training data is available since it's hard for the crucial queries/kernels to learn localization and shape priors. To this end, this work offers a novel unsupervised pre-training solution for low-data regimes. Inspired by the recent success of the Prompting technique, we introduce a new pre-training method that boosts QEIS models by giving Saliency Prompt for queries/kernels. Our method contains three parts: 1) Saliency Masks Proposal is responsible for generating pseudo masks from unlabeled images based on the saliency mechanism. 2) Prompt-Kernel Matching transfers pseudo masks into prompts and injects the corresponding localization and shape priors to the best-matched kernels. 3) Kernel Supervision is applied to supply supervision at the kernel level for robust learning. From a practical perspective, our pre-training method helps QEIS models achieve a similar convergence speed and comparable performance with CNN-based models in low-data regimes. Experimental results show that our method significantly boosts several QEIS models on three datasets.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Boosting_Low-Data_Instance_Segmentation_by_Unsupervised_Pre-Training_With_Saliency_Prompt_CVPR_2023_paper.pdf,,1,最近，受到DETR变体的启发，基于查询的端到端实例分割（QEIS）方法在大规模数据集上表现优于基于CNN的模型。然而，当只有少量训练数据可用时，它们会失去效力，因为关键查询/核心很难学习定位和形状先验知识。为此，本文提供了一种新颖的无监督预训练解决方案，适用于低数据情况。受Prompting技术的最近成功启发，我们引入了一种新的预训练方法，通过为查询/核心提供显著性提示来增强QEIS模型。我们的方法包含三个部分：1）显著性掩模提议负责基于显著性机制从未标记图像生成伪掩模。2）提示核心匹配将伪掩模转换为提示，并将相应的定位和形状先验知识注入到最佳匹配的核心中。3）核心监督应
11,What Can Human Sketches Do for Object Detection?,CVPR,2023,"Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Subhadeep Koley, Tao Xiang, Yi-Zhe Song","Sketches are highly expressive, inherently capturing subjective and fine-grained visual cues. The exploration of such innate properties of human sketches has, however, been limited to that of image retrieval. In this paper, for the first time, we cultivate the expressiveness of sketches but for the fundamental vision task of object detection. The end result is a sketch-enabled object detection framework that detects based on what you sketch -- that ""zebra"" (e.g., one that is eating the grass) in a herd of zebras (instance-aware detection), and only the part (e.g., ""head"" of a ""zebra"") that you desire (part-aware detection). We further dictate that our model works without (i) knowing which category to expect at testing (zero-shot) and (ii) not requiring additional bounding boxes (as per fully supervised) and class labels (as per weakly supervised). Instead of devising a model from the ground up, we show an intuitive synergy between foundation models (e.g., CLIP) and existing sketch models build for sketch-based image retrieval (SBIR), which can already elegantly solve the task -- CLIP to provide model generalisation, and SBIR to bridge the (sketch->photo) gap. In particular, we first perform independent prompting on both sketch and photo branches of an SBIR model to build highly generalisable sketch and photo encoders on the back of the generalisation ability of CLIP. We then devise a training paradigm to adapt the learned encoders for object detection, such that the region embeddings of detected boxes are aligned with the sketch and photo embeddings from SBIR. Evaluating our framework on standard object detection datasets like PASCAL-VOC and MS-COCO outperforms both supervised (SOD) and weakly-supervised object detectors (WSOD) on zero-shot setups. Project Page: https://pinakinathc.github.io/sketch-detect",,https://openaccess.thecvf.com/content/CVPR2023/papers/Chowdhury_What_Can_Human_Sketches_Do_for_Object_Detection_CVPR_2023_paper.pdf,,1,本文介绍了一种基于素描的物体检测框架，该框架可以根据您所画的素描来检测物体，包括实例感知检测和部位感知检测。该模型不需要知道测试时期望的类别，也不需要额外的边界框和类标签。作者通过将基础模型（如CLIP）和现有的基于素描的图像检索（SBIR）模型相结合，构建了高度通用的素描和图像编码器，并通过训练范式将学习到的编码器用于物体检测。在PASCAL-VOC和MS-COCO等标准物体检测数据集上评估该框架，结果表明在零样本设置下，该框架的性能优于有监督和弱监督物体检测器。
21,SAP-DETR: Bridging the Gap Between Salient Points and Queries-Based Transformer Detector for Fast Model Convergency,CVPR,2023,"Yang Liu, Yao Zhang, Yixin Wang, Yang Zhang, Jiang Tian, Zhongchao Shi, Jianping Fan, Zhiqiang He","Recently, the dominant DETR-based approaches apply central-concept spatial prior to accelerating Transformer detector convergency. These methods gradually refine the reference points to the center of target objects and imbue object queries with the updated central reference information for spatially conditional attention. However, centralizing reference points may severely deteriorate queries' saliency and confuse detectors due to the indiscriminative spatial prior. To bridge the gap between the reference points of salient queries and Transformer detectors, we propose SAlient Point-based DETR (SAP-DETR) by treating object detection as a transformation from salient points to instance objects. In SAP-DETR, we explicitly initialize a query-specific reference point for each object query, gradually aggregate them into an instance object, and then predict the distance from each side of the bounding box to these points. By rapidly attending to query-specific reference region and other conditional extreme regions from the image features, SAP-DETR can effectively bridge the gap between the salient point and the query-based Transformer detector with a significant convergency speed. Our extensive experiments have demonstrated that SAP-DETR achieves 1.4 times convergency speed with competitive performance. Under the standard training scheme, SAP-DETR stably promotes the SOTA approaches by  1.0 AP. Based on ResNet-DC-101, SAP-DETR achieves 46.9 AP. The code will be released at https://github.com/liuyang-ict/SAP-DETR.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SAP-DETR_Bridging_the_Gap_Between_Salient_Points_and_Queries-Based_Transformer_CVPR_2023_paper.pdf,https://github.com/liuyang-ict/SAP-DETR,1,最近，基于DETR的方法应用中心概念空间先验来加速Transformer检测器的收敛。这些方法逐渐将参考点细化到目标对象的中心，并为空间条件注意力赋予了更新的中心参考信息。然而，集中参考点可能会严重降低查询的显著性，并由于不可区分的空间先验而使检测器混淆。为了弥合显著查询的参考点和Transformer检测器之间的差距，我们提出了基于显著点的DETR（SAP-DETR），将目标检测视为从显著点到实例对象的转换。在SAP-DETR中，我们明确为每个对象查询初始化一个特定于查询的参考点，逐渐将它们聚合成一个实例对象，然后预测边界框每侧到这些点的距离。通过快速关注来自图像特征的特定于查询的参考区域和其他条件极端区域，SAP
22,DynaMask: Dynamic Mask Selection for Instance Segmentation,CVPR,2023,"Ruihuang Li, Chenhang He, Shuai Li, Yabin Zhang, Lei Zhang","The representative instance segmentation methods mostly segment different object instances with a mask of the fixed resolution, e.g., 28x 28 grid. However, a low-resolution mask loses rich details, while a high-resolution mask incurs quadratic computation overhead. It is a challenging task to predict the optimal binary mask for each instance. In this paper, we propose to dynamically select suitable masks for different object proposals. First, a dual-level Feature Pyramid Network (FPN) with adaptive feature aggregation is developed to gradually increase the mask grid resolution, ensuring high-quality segmentation of objects. Specifically, an efficient region-level top-down path (r-FPN) is introduced to incorporate complementary contextual and detailed information from different stages of image-level FPN (i-FPN). Then, to alleviate the increase of computation and memory costs caused by using large masks, we develop a Mask Switch Module (MSM) with negligible computational cost to select the most suitable mask resolution for each instance, achieving high efficiency while maintaining high segmentation accuracy. Without bells and whistles, the proposed method, namely DynaMask, brings consistent and noticeable performance improvements over other state-of-the-arts at a moderate computation overhead. The source code: https://github.com/lslrh/DynaMask.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DynaMask_Dynamic_Mask_Selection_for_Instance_Segmentation_CVPR_2023_paper.pdf,https://github.com/lslrh/DynaMask,1,本文提出了一种名为DynaMask的实例分割方法，该方法通过动态选择适合不同目标提议的掩模来解决低分辨率掩模丢失细节和高分辨率掩模带来的计算开销问题。首先，使用自适应特征聚合的双层特征金字塔网络（FPN）逐步增加掩模网格分辨率，确保高质量的对象分割。其次，引入了一种有效的区域级自顶向下路径（r-FPN）来整合来自不同阶段的图像级FPN（i-FPN）的互补上下文和详细信息。然后，为了减轻使用大型掩模带来的计算和内存成本的增加，开发了一个掩模切换模块（MSM），以选择每个实例最适合的掩模分辨率，实现高效率同时保持高分割准确性。实验结果表明，DynaMask方法在适度的计算开销下，相
23,The Devil Is in the Points: Weakly Semi-Supervised Instance Segmentation via Point-Guided Mask Representation,CVPR,2023,"Beomyoung Kim, Joonhyun Jeong, Dongyoon Han, Sung Ju Hwang","In this paper, we introduce a novel learning scheme named weakly semi-supervised instance segmentation (WSSIS) with point labels for budget-efficient and high-performance instance segmentation. Namely, we consider a dataset setting consisting of a few fully-labeled images and a lot of point-labeled images. Motivated by the main challenge of semi-supervised approaches mainly derives from the trade-off between false-negative and false-positive instance proposals, we propose a method for WSSIS that can effectively leverage the budget-friendly point labels as a powerful weak supervision source to resolve the challenge. Furthermore, to deal with the hard case where the amount of fully-labeled data is extremely limited, we propose a MaskRefineNet that refines noise in rough masks. We conduct extensive experiments on COCO and BDD100K datasets, and the proposed method achieves promising results comparable to those of the fully-supervised model, even with 50% of the fully labeled COCO data (38.8% vs. 39.7%). Moreover, when using as little as 5% of fully labeled COCO data, our method shows significantly superior performance over the state-of-the-art semi-supervised learning method (33.7% vs. 24.9%). The code is available at https://github.com/clovaai/PointWSSIS.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_The_Devil_Is_in_the_Points_Weakly_Semi-Supervised_Instance_Segmentation_CVPR_2023_paper.pdf,https://github.com/clovaai/PointWSSIS,1,本文介绍了一种名为弱半监督实例分割（WSSIS）的新型学习方案，该方案使用点标签实现预算高效和高性能的实例分割。具体而言，我们考虑一个数据集设置，其中包含少量完全标记的图像和大量点标记的图像。由于半监督方法的主要挑战主要源于误报和漏报实例提议之间的权衡，因此我们提出了一种WSSIS方法，可以有效地利用预算友好的点标签作为强大的弱监督源来解决这一挑战。此外，为了处理完全标记数据量极少的困难情况，我们提出了一个MaskRefineNet来细化粗糙掩模中的噪声。我们在COCO和BDD100K数据集上进行了广泛的实验，所提出的方法取得了有希望的结果，与完全监督模型相当，即使使用COCO完全标记数据的50％（38.
18,DoNet: Deep De-Overlapping Network for Cytology Instance Segmentation,CVPR,2023,"Hao Jiang, Rushan Zhang, Yanning Zhou, Yumeng Wang, Hao Chen","Cell instance segmentation in cytology images has significant importance for biology analysis and cancer screening, while remains challenging due to 1) the extensive overlapping translucent cell clusters that cause the ambiguous boundaries, and 2) the confusion of mimics and debris as nuclei. In this work, we proposed a De-overlapping Network (DoNet) in a decompose-and-recombined strategy. A Dual-path Region Segmentation Module (DRM) explicitly decomposes the cell clusters into intersection and complement regions, followed by a Semantic Consistency-guided Recombination Module (CRM) for integration. To further introduce the containment relationship of the nucleus in the cytoplasm, we design a Mask-guided Region Proposal Strategy (MRP) that integrates the cell attention maps for inner-cell instance prediction. We validate the proposed approach on ISBI2014 and CPS datasets. Experiments show that our proposed DoNet significantly outperforms other state-of-the-art (SOTA) cell instance segmentation methods. The code is available at https://github.com/DeepDoNet/DoNet.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_DoNet_Deep_De-Overlapping_Network_for_Cytology_Instance_Segmentation_CVPR_2023_paper.pdf,https://github.com/DeepDoNet/DoNet,2,细胞实例分割在生物学分析和癌症筛查中具有重要意义，但由于1）广泛重叠的半透明细胞簇导致模糊的边界，以及2）模拟物和碎片与细胞核混淆，因此仍然具有挑战性。本文提出了一种分解和重组策略的去重叠网络（DoNet）。双路径区域分割模块（DRM）明确将细胞簇分解为交集和补集区域，随后是语义一致性引导的重组模块（CRM）进行集成。为了进一步介绍细胞质中细胞核的包含关系，我们设计了一种掩码引导的区域提议策略（MRP），将细胞注意力图集成到内部细胞实例预测中。我们在ISBI2014和CPS数据集上验证了所提出的方法。实验表明，我们提出的DoNet明
76,DeGPR: Deep Guided Posterior Regularization for Multi-Class Cell Detection and Counting,CVPR,2023,"Aayush Kumar Tyagi, Chirag Mohapatra, Prasenjit Das, Govind Makharia, Lalita Mehra, Prathosh AP, Mausam","Multi-class cell detection and counting is an essential task for many pathological diagnoses. Manual counting is tedious and often leads to inter-observer variations among pathologists. While there exist multiple, general-purpose, deep learning-based object detection and counting methods, they may not readily transfer to detecting and counting cells in medical images, due to the limited data, presence of tiny overlapping objects, multiple cell types, severe class-imbalance, minute differences in size/shape of cells, etc. In response, we propose guided posterior regularization DeGPR, which assists an object detector by guiding it to exploit discriminative features among cells. The features may be pathologist-provided or inferred directly from visual data. We validate our model on two publicly available datasets (CoNSeP and MoNuSAC), and on MuCeD, a novel dataset that we contribute. MuCeD consists of 55 biopsy images of the human duodenum for predicting celiac disease. We perform extensive experimentation with three object detection baselines on three datasets to show that DeGPR is model-agnostic, and consistently improves baselines obtaining up to 9% (absolute) mAP gains.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Tyagi_DeGPR_Deep_Guided_Posterior_Regularization_for_Multi-Class_Cell_Detection_and_CVPR_2023_paper.pdf,,2,多类细胞检测和计数是许多病理诊断中必不可少的任务。手动计数繁琐，往往导致病理学家之间的观察者差异。虽然存在多种基于深度学习的通用目标检测和计数方法，但由于数据有限、存在微小重叠对象、多种细胞类型、严重的类别不平衡、细胞大小/形状的微小差异等原因，它们可能不容易转移到医学图像中检测和计数细胞。为此，我们提出了引导后验正则化DeGPR，通过引导目标检测器利用细胞之间的区分特征来辅助它。这些特征可以是病理学家提供的，也可以直接从视觉数据中推断出来。我们在两个公开数据集（CoNSeP和MoNuSAC）以及我们贡献的一个新数据集MuCeD上验证了我们的模型。MuCeD包
4,Beyond mAP: Towards Better Evaluation of Instance Segmentation,CVPR,2023,"Rohit Jena, Lukas Zhornyak, Nehal Doiphode, Pratik Chaudhari, Vivek Buch, James Gee, Jianbo Shi","Correctness of instance segmentation constitutes counting the number of objects, correctly localizing all predictions and classifying each localized prediction. Average Precision is the de-facto metric used to measure all these constituents of segmentation. However, this metric does not penalize duplicate predictions in the high-recall range, and cannot distinguish instances that are localized correctly but categorized incorrectly. This weakness has inadvertently led to network designs that achieve significant gains in AP but also introduce a large number of false positives. We therefore cannot rely on AP to choose a model that provides an optimal tradeoff between false positives and high recall. To resolve this dilemma, we review alternative metrics in the literature and propose two new measures to explicitly measure the amount of both spatial and categorical duplicate predictions. We also propose a Semantic Sorting and NMS module to remove these duplicates based on a pixel occupancy matching scheme. Experiments show that modern segmentation networks have significant gains in AP, but also contain a considerable amount of duplicates. Our Semantic Sorting and NMS can be added as a plug-and-play module to mitigate hedged predictions and preserve AP.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Jena_Beyond_mAP_Towards_Better_Evaluation_of_Instance_Segmentation_CVPR_2023_paper.pdf,,2,实例分割的正确性包括计算对象数量、正确定位所有预测和对每个定位预测进行分类。平均精度是用于衡量分割所有这些组成部分的事实上的度量标准。然而，这个度量标准不会惩罚高召回率范围内的重复预测，并且无法区分定位正确但分类错误的实例。这个弱点无意中导致了网络设计在AP方面取得了显著的增益，但也引入了大量的假阳性。因此，我们不能依靠AP来选择提供假阳性和高召回率之间最佳权衡的模型。为了解决这个困境，我们回顾了文献中的替代度量标准，并提出了两个新的度量标准来明确测量空间和分类重复预测的数量。我们还提出了一个语义排序和NMS模块，基于像素占用匹配方案来删除这些重复项。实验表明，
15,Exemplar-FreeSOLO: Enhancing Unsupervised Instance Segmentation With Exemplars,CVPR,2023,"Taoseef Ishtiak, Qing En, Yuhong Guo","Instance segmentation seeks to identify and segment each object from images, which often relies on a large number of dense annotations for model training. To alleviate this burden, unsupervised instance segmentation methods have been developed to train class-agnostic instance segmentation models without any annotation. In this paper, we propose a novel unsupervised instance segmentation approach, Exemplar-FreeSOLO, to enhance unsupervised instance segmentation by exploiting a limited number of unannotated and unsegmented exemplars. The proposed framework offers a new perspective on directly perceiving top-down information without annotations. Specifically, Exemplar-FreeSOLO introduces a novel exemplarknowledge abstraction module to acquire beneficial top-down guidance knowledge for instances using unsupervised exemplar object extraction. Moreover, a new exemplar embedding contrastive module is designed to enhance the discriminative capability of the segmentation model by exploiting the contrastive exemplar-based guidance knowledge in the embedding space. To evaluate the proposed ExemplarFreeSOLO, we conduct comprehensive experiments and perform in-depth analyses on three image instance segmentation datasets. The experimental results demonstrate that the proposed approach is effective and outperforms the state-of-the-art methods.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Ishtiak_Exemplar-FreeSOLO_Enhancing_Unsupervised_Instance_Segmentation_With_Exemplars_CVPR_2023_paper.pdf,,2,本文提出了一种新的无监督实例分割方法Exemplar-FreeSOLO，通过利用有限数量的未注释和未分割的样本来增强无监督实例分割。该方法引入了一种新的样本知识抽象模块，通过无监督样本对象提取来获取有益的自上而下的指导知识，从而直接感知自上而下的信息。此外，还设计了一种新的样本嵌入对比模块，通过在嵌入空间中利用对比样本为基础的指导知识来增强分割模型的区分能力。实验结果表明，该方法在三个图像实例分割数据集上表现出色，有效性优于现有的最先进方法。
19,Ambiguity-Resistant Semi-Supervised Learning for Dense Object Detection,CVPR,2023,"Chang Liu, Weiming Zhang, Xiangru Lin, Wei Zhang, Xiao Tan, Junyu Han, Xiaomao Li, Errui Ding, Jingdong Wang","With basic Semi-Supervised Object Detection (SSOD) techniques, one-stage detectors generally obtain limited promotions compared with two-stage clusters. We experimentally find that the root lies in two kinds of ambiguities: (1) Selection ambiguity that selected pseudo labels are less accurate, since classification scores cannot properly represent the localization quality. (2) Assignment ambiguity that samples are matched with improper labels in pseudo-label assignment, as the strategy is misguided by missed objects and inaccurate pseudo boxes. To tackle these problems, we propose a Ambiguity-Resistant Semi-supervised Learning (ARSL) for one-stage detectors. Specifically, to alleviate the selection ambiguity, Joint-Confidence Estimation (JCE) is proposed to jointly quantifies the classification and localization quality of pseudo labels. As for the assignment ambiguity, Task-Separation Assignment (TSA) is introduced to assign labels based on pixel-level predictions rather than unreliable pseudo boxes. It employs a 'divide-and-conquer' strategy and separately exploits positives for the classification and localization task, which is more robust to the assignment ambiguity. Comprehensive experiments demonstrate that ARSL effectively mitigates the ambiguities and achieves state-of-the-art SSOD performance on MS COCO and PASCAL VOC. Codes can be found at https://github.com/PaddlePaddle/PaddleDetection.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Ambiguity-Resistant_Semi-Supervised_Learning_for_Dense_Object_Detection_CVPR_2023_paper.pdf,https://github.com/PaddlePaddle/PaddleDetection,2,本文提出了一种抗模糊半监督学习方法（ARSL）用于一阶段目标检测器。研究发现，与两阶段检测器相比，基本的半监督目标检测技术只能获得有限的提升。这是由于存在两种模糊性：（1）选择模糊性，即由于分类分数不能正确表示定位质量，所选伪标签的准确性较低；（2）分配模糊性，即由于策略被错过的对象和不准确的伪框所误导，样本被分配给不当的伪标签。为了解决这些问题，ARSL提出了两种方法：联合置信度估计（JCE）和任务分离分配（TSA）。JCE用于缓解选择模糊性，它联合量化伪标签的分类和定位质量。TSA用于解决分配模糊性，它基于像素级预测而不是不可靠
27,Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects,CVPR,2023,"Wenteng Liang, Feng Xue, Yihao Liu, Guofeng Zhong, Anlong Ming","The recently proposed open-world object and open-set detection have achieved a breakthrough in finding never-seen-before objects and distinguishing them from known ones. However, their studies on knowledge transfer from known classes to unknown ones are not deep enough, resulting in the scanty capability for detecting unknowns hidden in the background. In this paper, we propose the unknown sniffer (UnSniffer) to find both unknown and known objects. Firstly, the generalized object confidence (GOC) score is introduced, which only uses known samples for supervision and avoids improper suppression of unknowns in the background. Significantly, such confidence score learned from known objects can be generalized to unknown ones. Additionally, we propose a negative energy suppression loss to further suppress the non-object samples in the background. Next, the best box of each unknown is hard to obtain during inference due to lacking their semantic information in training. To solve this issue, we introduce a graph-based determination scheme to replace hand-designed non-maximum suppression (NMS) post-processing. Finally, we present the Unknown Object Detection Benchmark, the first publicly benchmark that encompasses precision evaluation for unknown detection to our knowledge. Experiments show that our method is far better than the existing state-of-the-art methods. Code is available at: https://github.com/Went-Liang/UnSniffer.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_Unknown_Sniffer_for_Object_Detection_Dont_Turn_a_Blind_Eye_CVPR_2023_paper.pdf,https://github.com/Went-Liang/UnSniffer,2,本文提出了一种名为“未知嗅探器（UnSniffer）”的方法，可以发现未知和已知对象。首先，引入了广义对象置信度（GOC）分数，仅使用已知样本进行监督，并避免了对背景中未知对象的不当抑制。此外，还提出了一种负能量抑制损失，以进一步抑制背景中的非对象样本。接下来，为了解决推理过程中无法获得每个未知对象的最佳框的问题，引入了基于图的确定方案来替代手动设计的非极大值抑制（NMS）后处理。最后，提出了未知对象检测基准，这是我们所知道的第一个公开基准，包括对未知检测的精度评估。实验表明，我们的方法比现有的最先进方法要好得多。代码可在以下网址找到：https://github.com/Went-Liang/UnSniffer。
33,MixTeacher: Mining Promising Labels With Mixed Scale Teacher for Semi-Supervised Object Detection,CVPR,2023,"Liang Liu, Boshen Zhang, Jiangning Zhang, Wuhao Zhang, Zhenye Gan, Guanzhong Tian, Wenbing Zhu, Yabiao Wang, Chengjie Wang","Scale variation across object instances is one of the key challenges in object detection. Although modern detection models have achieved remarkable progress in dealing with the scale variation, it still brings trouble in the semi-supervised case. Most existing semi-supervised object detection methods rely on strict conditions to filter out high-quality pseudo labels from the network predictions. However, we observe that objects with extreme scale tend to have low confidence, which makes the positive supervision missing for these objects. In this paper, we delve into the scale variation problem, and propose a novel framework by introducing a mixed scale teacher to improve the pseudo labels generation and scale invariant learning. In addition, benefiting from the better predictions from mixed scale features, we propose to mine pseudo labels with the score promotion of predictions across scales. Extensive experiments on MS COCO and PASCAL VOC benchmarks under various semi-supervised settings demonstrate that our method achieves new state-of-the-art performance. The code and models will be made publicly available.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_MixTeacher_Mining_Promising_Labels_With_Mixed_Scale_Teacher_for_Semi-Supervised_CVPR_2023_paper.pdf,,2,目标检测中，物体实例的尺度变化是一个关键挑战之一。尽管现代检测模型在处理尺度变化方面取得了显著进展，但在半监督情况下仍然存在问题。大多数现有的半监督目标检测方法依赖于严格的条件来过滤网络预测中的高质量伪标签。然而，我们观察到尺度极端的物体往往具有较低的置信度，这使得这些物体缺乏正面监督。本文深入探讨了尺度变化问题，并通过引入混合尺度教师提出了一种新的框架，以改善伪标签生成和尺度不变学习。此外，受益于混合尺度特征的更好预测，我们提出了通过跨尺度预测得分提升来挖掘伪标签的方法。在各种半监督设置下，对 MS
36,Open-Vocabulary Attribute Detection,CVPR,2023,"María A. Bravo, Sudhanshu Mittal, Simon Ging, Thomas Brox","Vision-language modeling has enabled open-vocabulary tasks where predictions can be queried using any text prompt in a zero-shot manner. Existing open-vocabulary tasks focus on object classes, whereas research on object attributes is limited due to the lack of a reliable attribute-focused evaluation benchmark. This paper introduces the Open-Vocabulary Attribute Detection (OVAD) task and the corresponding OVAD benchmark. The objective of the novel task and benchmark is to probe object-level attribute information learned by vision-language models. To this end, we created a clean and densely annotated test set covering 117 attribute classes on the 80 object classes of MS COCO. It includes positive and negative annotations, which enables open-vocabulary evaluation. Overall, the benchmark consists of 1.4 million annotations. For reference, we provide a first baseline method for open-vocabulary attribute detection. Moreover, we demonstrate the benchmark's value by studying the attribute detection performance of several foundation models.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Bravo_Open-Vocabulary_Attribute_Detection_CVPR_2023_paper.pdf,,2,本文介绍了开放词汇属性检测（OVAD）任务及相应的OVAD基准。该任务和基准的目标是探究视觉语言模型学习的对象级属性信息。为此，我们创建了一个干净且密集注释的测试集，涵盖了MS COCO的80个对象类别中的117个属性类别。它包括正面和负面注释，可以进行开放词汇评估。总体而言，基准包括140万个注释。我们提供了一个开放词汇属性检测的第一个基准方法。此外，我们通过研究几个基础模型的属性检测性能，展示了基准的价值。
38,BoxTeacher: Exploring High-Quality Pseudo Labels for Weakly Supervised Instance Segmentation,CVPR,2023,"Tianheng Cheng, Xinggang Wang, Shaoyu Chen, Qian Zhang, Wenyu Liu","Labeling objects with pixel-wise segmentation requires a huge amount of human labor compared to bounding boxes. Most existing methods for weakly supervised instance segmentation focus on designing heuristic losses with priors from bounding boxes. While, we find that box-supervised methods can produce some fine segmentation masks and we wonder whether the detectors could learn from these fine masks while ignoring low-quality masks. To answer this question, we present BoxTeacher, an efficient and end-to-end training framework for high-performance weakly supervised instance segmentation, which leverages a sophisticated teacher to generate high-quality masks as pseudo labels. Considering the massive noisy masks hurt the training, we present a mask-aware confidence score to estimate the quality of pseudo masks and propose the noise-aware pixel loss and noise-reduced affinity loss to adaptively optimize the student with pseudo masks. Extensive experiments can demonstrate the effectiveness of the proposed BoxTeacher. Without bells and whistles, BoxTeacher remarkably achieves 35.0 mask AP and 36.5 mask AP with ResNet-50 and ResNet-101 respectively on the challenging COCO dataset, which outperforms the previous state-of-the-art methods by a significant margin and bridges the gap between box-supervised and mask-supervised methods. The code and models will be available later.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_BoxTeacher_Exploring_High-Quality_Pseudo_Labels_for_Weakly_Supervised_Instance_Segmentation_CVPR_2023_paper.pdf,,2,本文提出了一种高效的、端到端的训练框架BoxTeacher，用于弱监督实例分割。该框架利用复杂的教师生成高质量的伪标签，以适应性地优化学生。为了解决噪声标签的问题，提出了噪声感知像素损失和噪声减少亲和力损失。实验结果表明，BoxTeacher在COCO数据集上取得了显著的性能提升，达到了35.0和36.5的mask AP，超过了之前的最先进方法。该方法填补了盒子监督和掩模监督方法之间的差距。
39,Feature Shrinkage Pyramid for Camouflaged Object Detection With Transformers,CVPR,2023,"Zhou Huang, Hang Dai, Tian-Zhu Xiang, Shuo Wang, Huai-Xin Chen, Jie Qin, Huan Xiong","Vision transformers have recently shown strong global context modeling capabilities in camouflaged object detection. However, they suffer from two major limitations: less effective locality modeling and insufficient feature aggregation in decoders, which are not conducive to camouflaged object detection that explores subtle cues from indistinguishable backgrounds. To address these issues, in this paper, we propose a novel transformer-based Feature Shrinkage Pyramid Network (FSPNet), which aims to hierarchically decode locality-enhanced neighboring transformer features through progressive shrinking for camouflaged object detection. Specifically, we propose a non-local token enhancement module (NL-TEM) that employs the non-local mechanism to interact neighboring tokens and explore graph-based high-order relations within tokens to enhance local representations of transformers. Moreover, we design a feature shrinkage decoder (FSD) with adjacent interaction modules (AIM), which progressively aggregates adjacent transformer features through a layer-by-layer shrinkage pyramid to accumulate imperceptible but effective cues as much as possible for object information decoding. Extensive quantitative and qualitative experiments demonstrate that the proposed model significantly outperforms the existing 24 competitors on three challenging COD benchmark datasets under six widely-used evaluation metrics. Our code is publicly available at https://github.com/ZhouHuang23/FSPNet.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Feature_Shrinkage_Pyramid_for_Camouflaged_Object_Detection_With_Transformers_CVPR_2023_paper.pdf,https://github.com/ZhouHuang23/FSPNet,2,本文提出了一种基于Transformer的特征收缩金字塔网络（FSPNet），旨在通过逐步收缩来分层解码增强邻近Transformer特征，以便于探索来自难以区分背景的微妙线索的伪装物体检测。具体来说，我们提出了一个非局部令牌增强模块（NL-TEM），它利用非局部机制相互作用邻近令牌，并探索令牌内基于图形的高阶关系，以增强Transformer的局部表示。此外，我们设计了一个特征收缩解码器（FSD），其中包含相邻交互模块（AIM），通过逐层收缩金字塔来逐步聚合相邻的Transformer特征，以尽可能地积累不可感知但有效的线索，以便于对象信息解码。大量的定量和定性实验表明，所提出的模型在六个广泛使用的评估指标下，在三个具有挑
41,SIM: Semantic-Aware Instance Mask Generation for Box-Supervised Instance Segmentation,CVPR,2023,"Ruihuang Li, Chenhang He, Yabin Zhang, Shuai Li, Liyi Chen, Lei Zhang","Weakly supervised instance segmentation using only bounding box annotations has recently attracted much research attention. Most of the current efforts leverage low-level image features as extra supervision without explicitly exploiting the high-level semantic information of the objects, which will become ineffective when the foreground objects have similar appearances to the background or other objects nearby. We propose a new box-supervised instance segmentation approach by developing a Semantic-aware Instance Mask (SIM) generation paradigm. Instead of heavily relying on local pair-wise affinities among neighboring pixels, we construct a group of category-wise feature centroids as prototypes to identify foreground objects and assign them semantic-level pseudo labels. Considering that the semantic-aware prototypes cannot distinguish different instances of the same semantics, we propose a self-correction mechanism to rectify the falsely activated regions while enhancing the correct ones. Furthermore, to handle the occlusions between objects, we tailor the Copy-Paste operation for the weakly-supervised instance segmentation task to augment challenging training data. Extensive experimental results demonstrate the superiority of our proposed SIM approach over other state-of-the-art methods. The source code: https://github.com/lslrh/SIM.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SIM_Semantic-Aware_Instance_Mask_Generation_for_Box-Supervised_Instance_Segmentation_CVPR_2023_paper.pdf,https://github.com/lslrh/SIM,2,本文介绍了一种新的基于边界框注释的弱监督实例分割方法，通过开发语义感知实例掩模生成范式，构建了一组类别特征质心作为原型来识别前景对象并分配语义级伪标签。该方法不仅考虑了语义感知原型无法区分相同语义的不同实例的问题，还提出了自我校正机制来纠正错误激活区域并增强正确区域。此外，为了处理对象之间的遮挡，还针对弱监督实例分割任务量身定制了复制-粘贴操作以增强具有挑战性的训练数据。实验结果表明，该方法在弱监督实例分割任务中具有优越性。
44,Unbalanced Optimal Transport: A Unified Framework for Object Detection,CVPR,2023,"Henri De Plaen, Pierre-François De Plaen, Johan A. K. Suykens, Marc Proesmans, Tinne Tuytelaars, Luc Van Gool","During training, supervised object detection tries to correctly match the predicted bounding boxes and associated classification scores to the ground truth. This is essential to determine which predictions are to be pushed towards which solutions, or to be discarded. Popular matching strategies include matching to the closest ground truth box (mostly used in combination with anchors), or matching via the Hungarian algorithm (mostly used in anchor-free methods). Each of these strategies comes with its own properties, underlying losses, and heuristics. We show how Unbalanced Optimal Transport unifies these different approaches and opens a whole continuum of methods in between. This allows for a finer selection of the desired properties. Experimentally, we show that training an object detection model with Unbalanced Optimal Transport is able to reach the state-of-the-art both in terms of Average Precision and Average Recall as well as to provide a faster initial convergence. The approach is well suited for GPU implementation, which proves to be an advantage for large-scale models.",,https://openaccess.thecvf.com/content/CVPR2023/papers/De_Plaen_Unbalanced_Optimal_Transport_A_Unified_Framework_for_Object_Detection_CVPR_2023_paper.pdf,,2,在监督目标检测的训练过程中，需要正确匹配预测的边界框和相关的分类分数与真实值。这是确定哪些预测应该被推向哪些解决方案或被丢弃的关键。流行的匹配策略包括匹配到最接近的真实框（大多数与锚点结合使用），或通过匈牙利算法进行匹配（大多数用于无锚点方法）。每种策略都有其自身的属性、基础损失和启发式方法。我们展示了如何使用不平衡最优传输来统一这些不同的方法，并在其中开启整个方法的连续性。这允许更精细地选择所需的属性。实验表明，使用不平衡最优传输训练目标检测模型能够达到最先进的平均精度和平均召回率，并提供更快的初始收敛。该方法非常适合GPU实现，这对于大规模模型
49,Lite DETR: An Interleaved Multi-Scale Encoder for Efficient DETR,CVPR,2023,"Feng Li, Ailing Zeng, Shilong Liu, Hao Zhang, Hongyang Li, Lei Zhang, Lionel M. Ni","Recent DEtection TRansformer-based (DETR) models have obtained remarkable performance. Its success cannot be achieved without the re-introduction of multi-scale feature fusion in the encoder. However, the excessively increased tokens in multi-scale features, especially for about 75% of low-level features, are quite computationally inefficient, which hinders real applications of DETR models. In this paper, we present Lite DETR, a simple yet efficient end-to-end object detection framework that can effectively reduce the GFLOPs of the detection head by 60% while keeping 99% of the original performance. Specifically, we design an efficient encoder block to update high-level features (corresponding to small-resolution feature maps) and low-level features (corresponding to large-resolution feature maps) in an interleaved way. In addition, to better fuse cross-scale features, we develop a key-aware deformable attention to predict more reliable attention weights. Comprehensive experiments validate the effectiveness and efficiency of the proposed Lite DETR, and the efficient encoder strategy can generalize well across existing DETR-based models. The code will be released after the blind review.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Lite_DETR_An_Interleaved_Multi-Scale_Encoder_for_Efficient_DETR_CVPR_2023_paper.pdf,,2,本文介绍了一种名为 Lite DETR 的简单而高效的端到端目标检测框架，可以有效地减少检测头的 GFLOPs，同时保持原始性能的99%。为了更好地融合跨尺度特征，作者开发了一种关键点感知的可变形注意力机制来预测更可靠的注意力权重。全面的实验验证了 Lite DETR 的有效性和效率，并且高效的编码器策略可以很好地推广到现有的基于 DETR 的模型中。在盲审后，代码将被发布。
50,PanoSwin: A Pano-Style Swin Transformer for Panorama Understanding,CVPR,2023,"Zhixin Ling, Zhen Xing, Xiangdong Zhou, Manliang Cao, Guichun Zhou","In panorama understanding, the widely used equirectangular projection (ERP) entails boundary discontinuity and spatial distortion. It severely deteriorates the conventional CNNs and vision Transformers on panoramas. In this paper, we propose a simple yet effective architecture named PanoSwin to learn panorama representations with ERP. To deal with the challenges brought by equirectangular projection, we explore a pano-style shift windowing scheme and novel pitch attention to address the boundary discontinuity and the spatial distortion, respectively. Besides, based on spherical distance and Cartesian coordinates, we adapt absolute positional encodings and relative positional biases for panoramas to enhance panoramic geometry information. Realizing that planar image understanding might share some common knowledge with panorama understanding, we devise a novel two-stage learning framework to facilitate knowledge transfer from the planar images to panoramas. We conduct experiments against the state-of-the-art on various panoramic tasks, i.e., panoramic object detection, panoramic classification, and panoramic layout estimation. The experimental results demonstrate the effectiveness of PanoSwin in panorama understanding.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Ling_PanoSwin_A_Pano-Style_Swin_Transformer_for_Panorama_Understanding_CVPR_2023_paper.pdf,,2,本文提出了一种名为PanoSwin的简单而有效的架构，用于学习具有等距投影（ERP）的全景表示。为了应对等距投影带来的挑战，我们探索了一种全景式移位窗口方案和新颖的俯仰注意力，以分别解决边界不连续性和空间失真。此外，基于球面距离和笛卡尔坐标，我们适应了绝对位置编码和相对位置偏差，以增强全景几何信息。我们还设计了一种新颖的两阶段学习框架，以促进从平面图像到全景图像的知识转移。实验结果表明，PanoSwin在各种全景任务中都具有很好的效果。
54,Weak-Shot Object Detection Through Mutual Knowledge Transfer,CVPR,2023,"Xuanyi Du, Weitao Wan, Chong Sun, Chen Li","Weak-shot Object Detection methods exploit a fully-annotated source dataset to facilitate the detection performance on the target dataset which only contains image-level labels for novel categories. To bridge the gap between these two datasets, we aim to transfer the object knowledge between the source (S) and target (T) datasets in a bi-directional manner. We propose a novel Knowledge Transfer (KT) loss which simultaneously distills the knowledge of objectness and class entropy from a proposal generator trained on the S dataset to optimize a multiple instance learning module on the T dataset. By jointly optimizing the classification loss and the proposed KT loss, the multiple instance learning module effectively learns to classify object proposals into novel categories in the T dataset with the transferred knowledge from base categories in the S dataset. Noticing the predicted boxes on the T dataset can be regarded as an extension for the original annotations on the S dataset to refine the proposal generator in return, we further propose a novel Consistency Filtering (CF) method to reliably remove inaccurate pseudo labels by evaluating the stability of the multiple instance learning module upon noise injections. Via mutually transferring knowledge between the S and T datasets in an iterative manner, the detection performance on the target dataset is significantly improved. Extensive experiments on public benchmarks validate that the proposed method performs favourably against the state-of-the-art methods without increasing the model parameters or inference computational complexity.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Weak-Shot_Object_Detection_Through_Mutual_Knowledge_Transfer_CVPR_2023_paper.pdf,,2,本文提出了一种新的知识转移损失（KT）和一种一致性过滤（CF）方法，以在只包含新类别图像级标签的目标数据集上提高弱监督目标检测方法的性能。该方法通过在源（S）和目标（T）数据集之间双向转移目标知识，将S数据集上的提议生成器的目标性和类熵的知识同时提取到T数据集上的多实例学习模块中，从而有效地学习将目标提议分类为T数据集中的新类别。同时，该方法还通过一致性过滤方法可靠地去除不准确的伪标签。实验结果表明，该方法在不增加模型参数或推理计算复杂度的情况下，与现有方法相比具有更好的性能。
55,Semi-DETR: Semi-Supervised Object Detection With Detection Transformers,CVPR,2023,"Jiacheng Zhang, Xiangru Lin, Wei Zhang, Kuo Wang, Xiao Tan, Junyu Han, Errui Ding, Jingdong Wang, Guanbin Li","We analyze the DETR-based framework on semi-supervised object detection (SSOD) and observe that (1) the one-to-one assignment strategy generates incorrect matching when the pseudo ground-truth bounding box is inaccurate, leading to training inefficiency; (2) DETR-based detectors lack deterministic correspondence between the input query and its prediction output, which hinders the applicability of the consistency-based regularization widely used in current SSOD methods. We present Semi-DETR, the first transformer-based end-to-end semi-supervised object detector, to tackle these problems. Specifically, we propose a Stage-wise Hybrid Matching strategy that com- bines the one-to-many assignment and one-to-one assignment strategies to improve the training efficiency of the first stage and thus provide high-quality pseudo labels for the training of the second stage. Besides, we introduce a Cross-view Query Consistency method to learn the semantic feature invariance of object queries from different views while avoiding the need to find deterministic query correspondence. Furthermore, we propose a Cost-based Pseudo Label Mining module to dynamically mine more pseudo boxes based on the matching cost of pseudo ground truth bounding boxes for consistency training. Extensive experiments on all SSOD settings of both COCO and Pascal VOC benchmark datasets show that our Semi-DETR method outperforms all state-of-the-art methods by clear margins.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Semi-DETR_Semi-Supervised_Object_Detection_With_Detection_Transformers_CVPR_2023_paper.pdf,,2,本文分析了基于DETR框架的半监督目标检测(SSOD)方法，并发现(1)当伪标注边界框不准确时，一对一分配策略会产生错误匹配，导致训练效率低下；(2)基于DETR的检测器缺乏输入查询和预测输出之间的确定性对应关系，这阻碍了当前SSOD方法中广泛使用的基于一致性的正则化的适用性。我们提出了Semi-DETR，这是第一个基于Transformer的端到端半监督目标检测器，以解决这些问题。具体来说，我们提出了一个阶段混合匹配策略，将一对多分配和一对一分配策略相结合，以提高第一阶段的训练效率，从而为第二阶段的训练提供高质量的伪标签。此外，我们引入了一个跨视图查询一致性方法，学习不同视图中的对象
56,Discriminating Known From Unknown Objects via Structure-Enhanced Recurrent Variational AutoEncoder,CVPR,2023,"Aming Wu, Cheng Deng","Discriminating known from unknown objects is an important essential ability for human beings. To simulate this ability, a task of unsupervised out-of-distribution object detection (OOD-OD) is proposed to detect the objects that are never-seen-before during model training, which is beneficial for promoting the safe deployment of object detectors. Due to lacking unknown data for supervision, for this task, the main challenge lies in how to leverage the known in-distribution (ID) data to improve the detector's discrimination ability. In this paper, we first propose a method of Structure-Enhanced Recurrent Variational AutoEncoder (SR-VAE), which mainly consists of two dedicated recurrent VAE branches. Specifically, to boost the performance of object localization, we explore utilizing the classical Laplacian of Gaussian (LoG) operator to enhance the structure information in the extracted low-level features. Meanwhile, we design a VAE branch that recurrently generates the augmentation of the classification features to strengthen the discrimination ability of the object classifier. Finally, to alleviate the impact of lacking unknown data, another cycle-consistent conditional VAE branch is proposed to synthesize virtual OOD features that deviate from the distribution of ID features, which improves the capability of distinguishing OOD objects. In the experiments, our method is evaluated on OOD-OD, open-vocabulary detection, and incremental object detection. The significant performance gains over baselines show the superiorities of our method. The code will be released at https://github.com/AmingWu/SR-VAE.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Discriminating_Known_From_Unknown_Objects_via_Structure-Enhanced_Recurrent_Variational_AutoEncoder_CVPR_2023_paper.pdf,https://github.com/AmingWu/SR-VAE,2,本文提出了一种无监督的超出分布物体检测（OOD-OD）任务，以检测在模型训练期间从未见过的物体，模拟人类区分已知和未知物体的能力。由于缺乏未知数据进行监督，因此该任务的主要挑战在于如何利用已知的分布（ID）数据来提高检测器的区分能力。为此，本文首先提出了一种结构增强的循环变分自编码器（SR-VAE）方法，主要由两个专用的循环VAE分支组成。具体而言，为了提高物体定位的性能，我们探索利用经典的高斯拉普拉斯算子（LoG）来增强提取的低级特征中的结构信息。同时，我们设计了一个VAE分支，循环生成分类特征的增强，以增强物体分类器的区分能力。最后，为了减轻缺乏未知数据的影响，提出了另一个
59,DETRs With Hybrid Matching,CVPR,2023,"Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu, Weihong Lin, Lei Sun, Chao Zhang, Han Hu","One-to-one set matching is a key design for DETR to establish its end-to-end capability, so that object detection does not require a hand-crafted NMS (non-maximum suppression) to remove duplicate detections. This end-to-end signature is important for the versatility of DETR, and it has been generalized to broader vision tasks. However, we note that there are few queries assigned as positive samples and the one-to-one set matching significantly reduces the training efficacy of positive samples. We propose a simple yet effective method based on a hybrid matching scheme that combines the original one-to-one matching branch with an auxiliary one-to-many matching branch during training. Our hybrid strategy has been shown to significantly improve accuracy. In inference, only the original one-to-one match branch is used, thus maintaining the end-to-end merit and the same inference efficiency of DETR. The method is named H-DETR, and it shows that a wide range of representative DETR methods can be consistently improved across a wide range of visual tasks, including Deformable-DETR, PETRv2, PETR, and TransTrack, among others.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Jia_DETRs_With_Hybrid_Matching_CVPR_2023_paper.pdf,,2,DETR 的关键设计是一对一的集合匹配，以建立其端到端的能力，使得目标检测不需要手工制作的 NMS（非极大值抑制）来消除重复检测。这种端到端的特征对于 DETR 的多功能性非常重要，并且已经推广到更广泛的视觉任务中。然而，我们注意到分配为正样本的查询很少，而一对一的集合匹配显著降低了正样本的训练效果。我们提出了一种基于混合匹配方案的简单而有效的方法，在训练期间将原始的一对一匹配分支与辅助的一对多匹配分支相结合。我们的混合策略已被证明可以显著提高准确性。在推理中，仅使用原始的一对一匹配分支，从而保持 DETR 的端到端优点和相同的推理效率。该方法被命名为 H-DETR，并表明一系列代表性
61,Continual Detection Transformer for Incremental Object Detection,CVPR,2023,"Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, Christian Rupprecht","Incremental object detection (IOD) aims to train an object detector in phases, each with annotations for new object categories. As other incremental settings, IOD is subject to catastrophic forgetting, which is often addressed by techniques such as knowledge distillation (KD) and exemplar replay (ER). However, KD and ER do not work well if applied directly to state-of-the-art transformer-based object detectors such as Deformable DETR and UP-DETR. In this paper, we solve these issues by proposing a ContinuaL DEtection TRansformer (CL-DETR), a new method for transformer-based IOD which enables effective usage of KD and ER in this context. First, we introduce a Detector Knowledge Distillation (DKD) loss, focusing on the most informative and reliable predictions from old versions of the model, ignoring redundant background predictions, and ensuring compatibility with the available ground-truth labels. We also improve ER by proposing a calibration strategy to preserve the label distribution of the training set, therefore better matching training and testing statistics. We conduct extensive experiments on COCO 2017 and demonstrate that CL-DETR achieves state-of-the-art results in the IOD setting.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Continual_Detection_Transformer_for_Incremental_Object_Detection_CVPR_2023_paper.pdf,,2,本文提出了一种基于Transformer的增量目标检测方法，称为ContinuaL DEtection TRansformer (CL-DETR)。该方法通过引入Detector Knowledge Distillation (DKD) loss和标签分布校准策略，使得知识蒸馏和样本重放等技术在增量目标检测中更加有效。实验结果表明，CL-DETR在COCO 2017数据集上取得了最先进的增量目标检测结果。
63,DropKey for Vision Transformer,CVPR,2023,"Bonan Li, Yinhan Hu, Xuecheng Nie, Congying Han, Xiangjian Jiang, Tiande Guo, Luoqi Liu","In this paper, we focus on analyzing and improving the dropout technique for self-attention layers of Vision Transformer, which is important while surprisingly ignored by prior works. In particular, we conduct researches on three core questions: First, what to drop in self-attention layers? Different from dropping attention weights in literature, we propose to move dropout operations forward ahead of attention matrix calculation and set the Key as the dropout unit, yielding a novel dropout-before-softmax scheme. We theoretically verify that this scheme helps keep both regularization and probability features of attention weights, alleviating the overfittings problem to specific patterns and enhancing the model to globally capture vital information; Second, how to schedule the drop ratio in consecutive layers? In contrast to exploit a constant drop ratio for all layers, we present a new decreasing schedule that gradually decreases the drop ratio along the stack of self-attention layers. We experimentally validate the proposed schedule can avoid overfittings in low-level features and missing in high-level semantics, thus improving the robustness and stableness of model training; Third, whether need to perform structured dropout operation as CNN? We attempt patch-based block-version of dropout operation and find that this useful trick for CNN is not essential for ViT. Given exploration on the above three questions, we present the novel DropKey method that regards Key as the drop unit and exploits decreasing schedule for drop ratio, improving ViTs in a general way. Comprehensive experiments demonstrate the effectiveness of DropKey for various ViT architectures, e.g. T2T, VOLO, CeiT and DeiT, as well as for various vision tasks, e.g., image classification, object detection, human-object interaction detection and human body shape recovery.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DropKey_for_Vision_Transformer_CVPR_2023_paper.pdf,,2,本文聚焦于分析和改进Vision Transformer自注意力层的dropout技术，这在之前的研究中被忽视了。具体而言，我们研究了三个核心问题：第一，自注意力层中应该丢弃什么？不同于文献中丢弃注意力权重的方法，我们提出将dropout操作提前到注意力矩阵计算之前，并将Key作为dropout单元，从而得到一种新的dropout-before-softmax方案。我们在理论上验证了这种方案有助于保持注意力权重的正则化和概率特征，缓解了过度拟合特定模式的问题，增强了模型对全局重要信息的捕捉能力；第二，如何在连续的层中安排丢弃比率？与利用恒定的丢弃比率不同，我们提出了一种新的递减调度，逐渐降低自注意力层的丢弃比率。我们在实验中验证了所提出的调度可以
64,Semantic-Promoted Debiasing and Background Disambiguation for Zero-Shot Instance Segmentation,CVPR,2023,"Shuting He, Henghui Ding, Wei Jiang","Zero-shot instance segmentation aims to detect and precisely segment objects of unseen categories without any training samples. Since the model is trained on seen categories, there is a strong bias that the model tends to classify all the objects into seen categories. Besides, there is a natural confusion between background and novel objects that have never shown up in training. These two challenges make novel objects hard to be raised in the final instance segmentation results. It is desired to rescue novel objects from background and dominated seen categories. To this end, we propose D^2Zero with Semantic-Promoted Debiasing and Background Disambiguation to enhance the performance of Zero-shot instance segmentation. Semantic-promoted debiasing utilizes inter-class semantic relationships to involve unseen categories in visual feature training and learns an input-conditional classifier to conduct dynamical classification based on the input image. Background disambiguation produces image-adaptive background representation to avoid mistaking novel objects for background. Extensive experiments show that we significantly outperform previous state-of-the-art methods by a large margin, e.g., 16.86% improvement on COCO.",,https://openaccess.thecvf.com/content/CVPR2023/papers/He_Semantic-Promoted_Debiasing_and_Background_Disambiguation_for_Zero-Shot_Instance_Segmentation_CVPR_2023_paper.pdf,,2,Zero-shot instance segmentation旨在在没有任何训练样本的情况下检测和精确分割未见过类别的对象。由于模型是在已知类别上训练的，存在强烈的偏见，模型倾向于将所有对象分类为已知类别。此外，背景和从未出现在训练中的新颖对象之间存在自然混淆。这两个挑战使得新颖对象难以在最终的实例分割结果中被提取出来。为了解决这个问题，我们提出了D^2Zero with Semantic-Promoted Debiasing and Background Disambiguation来增强Zero-shot实例分割的性能。语义促进去偏差利用类间语义关系将未见过的类别纳入视觉特征训练，并学习一个输入条件分类器来根据输入图像进行动态分类。背景消歧产生图像自适应背景表示，避免将新颖对象误认为背景。大量实验证明，我们的方法
68,AttentionShift: Iteratively Estimated Part-Based Attention Map for Pointly Supervised Instance Segmentation,CVPR,2023,"Mingxiang Liao, Zonghao Guo, Yuze Wang, Peng Yuan, Bailan Feng, Fang Wan","Pointly supervised instance segmentation (PSIS) learns to segment objects using a single point within the object extent as supervision. Challenged by the non-negligible semantic variance between object parts, however, the single supervision point causes semantic bias and false segmentation. In this study, we propose an AttentionShift method, to solve the semantic bias issue by iteratively decomposing the instance attention map to parts and estimating fine-grained semantics of each part. AttentionShift consists of two modules plugged on the vision transformer backbone: (i) token querying for pointly supervised attention map generation, and (ii) key-point shift, which re-estimates part-based attention maps by key-point filtering in the feature space. These two steps are iteratively performed so that the part-based attention maps are optimized spatially as well as in the feature space to cover full object extent. Experiments on PASCAL VOC and MS COCO 2017 datasets show that AttentionShift respectively improves the state-of-the-art of by 7.7% and 4.8% under mAP@0.5, setting a solid PSIS baseline using vision transformer. Code is enclosed in the supplementary material.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_AttentionShift_Iteratively_Estimated_Part-Based_Attention_Map_for_Pointly_Supervised_Instance_CVPR_2023_paper.pdf,,2,本文提出了一种名为AttentionShift的方法，用于解决点监督实例分割中的语义偏差问题。该方法通过迭代地将实例注意力图分解为部分，并估计每个部分的细粒度语义来优化部分注意力图，从而覆盖整个对象范围。AttentionShift由两个模块组成，分别是用于点监督注意力图生成的令牌查询和通过特征空间中的关键点过滤重新估计基于部分的注意力图的关键点移位。在PASCAL VOC和MS COCO 2017数据集上的实验表明，AttentionShift分别在mAP@0.5下将最新技术水平提高了7.7％和4.8％，为使用视觉变换器的PSIS奠定了坚实的基础。代码附在补充材料中。
69,Vision Transformers Are Good Mask Auto-Labelers,CVPR,2023,"Shiyi Lan, Xitong Yang, Zhiding Yu, Zuxuan Wu, Jose M. Alvarez, Anima Anandkumar","We propose Mask Auto-Labeler (MAL), a high-quality Transformer-based mask auto-labeling framework for instance segmentation using only box annotations. MAL takes box-cropped images as inputs and conditionally generates their mask pseudo-labels.We show that Vision Transformers are good mask auto-labelers. Our method significantly reduces the gap between auto-labeling and human annotation regarding mask quality. Instance segmentation models trained using the MAL-generated masks can nearly match the performance of their fully-supervised counterparts, retaining up to 97.4% performance of fully supervised models. The best model achieves 44.1% mAP on COCO instance segmentation (test-dev 2017), outperforming state-of-the-art box-supervised methods by significant margins. Qualitative results indicate that masks produced by MAL are, in some cases, even better than human annotations.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Lan_Vision_Transformers_Are_Good_Mask_Auto-Labelers_CVPR_2023_paper.pdf,,2,我们提出了 Mask Auto-Labeler (MAL)，这是一个基于 Transformer 的高质量实例分割自动标注框架，只使用框注释。MAL将框裁剪图像作为输入，并有条件地生成它们的掩码伪标签。我们展示了 Vision Transformers 是良好的掩码自动标注器。我们的方法显著减少了自动标注和人工标注之间的掩码质量差距。使用MAL生成的掩码训练的实例分割模型可以几乎匹配其完全监督的对应模型的性能，保留了高达97.4%的完全监督模型的性能。最佳模型在COCO实例分割（test-dev 2017）上实现了44.1%的mAP，优于最先进的基于框监督的方法。定性结果表明，MAL生成的掩码在某些情况下甚至比人工注释更好。
77,Multiclass Confidence and Localization Calibration for Object Detection,CVPR,2023,"Bimsara Pathiraja, Malitha Gunawardhana, Muhammad Haris Khan","Albeit achieving high predictive accuracy across many challenging computer vision problems, recent studies suggest that deep neural networks (DNNs) tend to make overconfident predictions, rendering them poorly calibrated. Most of the existing attempts for improving DNN calibration are limited to classification tasks and restricted to calibrating in-domain predictions. Surprisingly, very little to no attempts have been made in studying the calibration of object detection methods, which occupy a pivotal space in vision-based security-sensitive, and safety-critical applications. In this paper, we propose a new train-time technique for calibrating modern object detection methods. It is capable of jointly calibrating multiclass confidence and box localization by leveraging their predictive uncertainties. We perform extensive experiments on several in-domain and out-of-domain detection benchmarks. Results demonstrate that our proposed train-time calibration method consistently outperforms several baselines in reducing calibration error for both in-domain and out-of-domain predictions. Our code and models are available at https://github.com/bimsarapathiraja/MCCL",,https://openaccess.thecvf.com/content/CVPR2023/papers/Pathiraja_Multiclass_Confidence_and_Localization_Calibration_for_Object_Detection_CVPR_2023_paper.pdf,https://github.com/bimsarapathiraja/MCCL,2,本文提出了一种新的训练时技术，用于校准现代目标检测方法。该方法能够通过利用预测的不确定性来联合校准多类置信度和框定位。我们在几个领域内和领域外的检测基准上进行了广泛的实验。结果表明，我们提出的训练时校准方法在减少领域内和领域外预测的校准误差方面始终优于几个基线。我们的代码和模型可在 https://github.com/bimsarapathiraja/MCCL 上获得。
6,Toward RAW Object Detection: A New Benchmark and a New Model,CVPR,2023,"Ruikang Xu, Chang Chen, Jingyang Peng, Cheng Li, Yibin Huang, Fenglong Song, Youliang Yan, Zhiwei Xiong","In many computer vision applications (e.g., robotics and autonomous driving), high dynamic range (HDR) data is necessary for object detection algorithms to handle a variety of lighting conditions, such as strong glare. In this paper, we aim to achieve object detection on RAW sensor data, which naturally saves the HDR information from image sensors without extra equipment costs. We build a novel RAW sensor dataset, named ROD, for Deep Neural Networks (DNNs)-based object detection algorithms to be applied to HDR data. The ROD dataset contains a large amount of annotated instances of day and night driving scenes in 24-bit dynamic range. Based on the dataset, we first investigate the impact of dynamic range for DNNs-based detectors and demonstrate the importance of dynamic range adjustment for detection on RAW sensor data. Then, we propose a simple and effective adjustment method for object detection on HDR RAW sensor data, which is image adaptive and jointly optimized with the downstream detector in an end-to-end scheme. Extensive experiments demonstrate that the performance of detection on RAW sensor data is significantly superior to standard dynamic range (SDR) data in different situations. Moreover, we analyze the influence of texture information and pixel distribution of input data on the performance of the DNNs-based detector.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Toward_RAW_Object_Detection_A_New_Benchmark_and_a_New_CVPR_2023_paper.pdf,,3,本文旨在实现对RAW传感器数据的目标检测，以便在处理各种光照条件（如强烈的耀斑）时，目标检测算法能够处理高动态范围（HDR）数据，这在许多计算机视觉应用（如机器人和自动驾驶）中是必要的。我们建立了一个名为ROD的新型RAW传感器数据集，用于基于深度神经网络（DNN）的目标检测算法应用于HDR数据。ROD数据集包含大量24位动态范围的日间和夜间驾驶场景的注释实例。基于该数据集，我们首先研究了动态范围对基于DNN的检测器的影响，并证明了动态范围调整对于在RAW传感器数据上进行检测的重要性。然后，我们提出了一种简单有效的HDR RAW传感器数据目标检测调整方法，该方法是图像自适应的，并在端到端方案中与下游
7,Towards Building Self-Aware Object Detectors via Reliable Uncertainty Quantification and Calibration,CVPR,2023,"Kemal Oksuz, Tom Joy, Puneet K. Dokania","The current approach for testing the robustness of object detectors suffers from serious deficiencies such as improper methods of performing out-of-distribution detection and using calibration metrics which do not consider both localisation and classification quality. In this work, we address these issues, and introduce the Self Aware Object Detection (SAOD) task, a unified testing framework which respects and adheres to the challenges that object detectors face in safety-critical environments such as autonomous driving. Specifically, the SAOD task requires an object detector to be: robust to domain shift; obtain reliable uncertainty estimates for the entire scene; and provide calibrated confidence scores for the detections. We extensively use our framework, which introduces novel metrics and large scale test datasets, to test numerous object detectors in two different use-cases, allowing us to highlight critical insights into their robustness performance. Finally, we introduce a simple baseline for the SAOD task, enabling researchers to benchmark future proposed methods and move towards robust object detectors which are fit for purpose. Code is available at: https://github.com/fiveai/saod",,https://openaccess.thecvf.com/content/CVPR2023/papers/Oksuz_Towards_Building_Self-Aware_Object_Detectors_via_Reliable_Uncertainty_Quantification_and_CVPR_2023_paper.pdf,https://github.com/fiveai/saod,3,本文介绍了当前目标检测器鲁棒性测试方法存在的严重缺陷，如不当的域外检测方法和使用不考虑定位和分类质量的校准度量。为此，提出了自我感知目标检测（SAOD）任务，这是一个统一的测试框架，尊重并遵守目标检测器在自动驾驶等安全关键环境中面临的挑战。具体而言，SAOD任务要求目标检测器具有以下特点：对域漂移具有鲁棒性；为整个场景提供可靠的不确定性估计；并为检测提供校准的置信度分数。作者广泛使用了自己的框架，引入了新的度量和大规模测试数据集，测试了许多目标检测器在两种不同的用例中，从而突出了它们的鲁棒性表现的关键见解。最后，作者提出了SAOD任务的简单基线，使
10,Bridging Precision and Confidence: A Train-Time Loss for Calibrating Object Detection,CVPR,2023,"Muhammad Akhtar Munir, Muhammad Haris Khan, Salman Khan, Fahad Shahbaz Khan","Deep neural networks (DNNs) have enabled astounding progress in several vision-based problems. Despite showing high predictive accuracy, recently, several works have revealed that they tend to provide overconfident predictions and thus are poorly calibrated. The majority of the works addressing the miscalibration of DNNs fall under the scope of classification and consider only in-domain predictions. However, there is little to no progress in studying the calibration of DNN-based object detection models, which are central to many vision-based safety-critical applications. In this paper, inspired by the train-time calibration methods, we propose a novel auxiliary loss formulation that explicitly aims to align the class confidence of bounding boxes with the accurateness of predictions (i.e. precision). Since the original formulation of our loss depends on the counts of true positives and false positives in a minibatch, we develop a differentiable proxy of our loss that can be used during training with other application-specific loss functions. We perform extensive experiments on challenging in-domain and out-domain scenarios with six benchmark datasets including MS-COCO, Cityscapes, Sim10k, and BDD100k. Our results reveal that our train-time loss surpasses strong calibration baselines in reducing calibration error for both in and out-domain scenarios. Our source code and pre-trained models are available at https://github.com/akhtarvision/bpc_calibration",,https://openaccess.thecvf.com/content/CVPR2023/papers/Munir_Bridging_Precision_and_Confidence_A_Train-Time_Loss_for_Calibrating_Object_CVPR_2023_paper.pdf,https://github.com/akhtarvision/bpc_calibration,3,深度神经网络（DNN）在多个基于视觉的问题中取得了惊人的进展。尽管表现出高预测准确性，但最近的几项研究表明，它们往往提供过于自信的预测，因此校准不佳。大多数解决DNN校准问题的工作都属于分类范畴，并仅考虑域内预测。然而，在研究基于DNN的物体检测模型的校准方面几乎没有进展，而这对许多基于视觉的安全关键应用至关重要。本文受到训练时校准方法的启发，提出了一种新的辅助损失公式，明确旨在将边界框的类置信度与预测的准确性（即精度）对齐。由于我们损失的原始公式取决于小批量中真正的正例和假正例的数量，因此我们开发了一个可微
12,MetaFusion: Infrared and Visible Image Fusion via Meta-Feature Embedding From Object Detection,CVPR,2023,"Wenda Zhao, Shigeng Xie, Fan Zhao, You He, Huchuan Lu","Fusing infrared and visible images can provide more texture details for subsequent object detection task. Conversely, detection task furnishes object semantic information to improve the infrared and visible image fusion. Thus, a joint fusion and detection learning to use their mutual promotion is attracting more attention. However, the feature gap between these two different-level tasks hinders the progress. Addressing this issue, this paper proposes an infrared and visible image fusion via meta-feature embedding from object detection. The core idea is that meta-feature embedding model is designed to generate object semantic features according to fusion network ability, and thus the semantic features are naturally compatible with fusion features. It is optimized by simulating a meta learning. Moreover, we further implement a mutual promotion learning between fusion and detection tasks to improve their performances. Comprehensive experiments on three public datasets demonstrate the effectiveness of our method. Code and model are available at: https://github.com/wdzhao123/MetaFusion.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_MetaFusion_Infrared_and_Visible_Image_Fusion_via_Meta-Feature_Embedding_From_CVPR_2023_paper.pdf,https://github.com/wdzhao123/MetaFusion,3,本文提出了一种基于目标检测的元特征嵌入的红外和可见光图像融合方法。该方法通过设计元特征嵌入模型来生成对象语义特征，从而使语义特征与融合特征自然兼容。此外，我们还进一步实现了融合和检测任务之间的相互促进学习，以提高它们的性能。在三个公共数据集上的综合实验表明了我们方法的有效性。代码和模型可在 https://github.com/wdzhao123/MetaFusion 上获得。
34,Gaussian Label Distribution Learning for Spherical Image Object Detection,CVPR,2023,"Hang Xu, Xinyuan Liu, Qiang Zhao, Yike Ma, Chenggang Yan, Feng Dai","Spherical image object detection emerges in many applications from virtual reality to robotics and automatic driving, while many existing detectors use ln-norms loss for regression of spherical bounding boxes. There are two intrinsic flaws for ln-norms loss, i.e., independent optimization of parameters and inconsistency between metric (dominated by IoU) and loss. These problems are common in planar image detection but more significant in spherical image detection. Solution for these problems has been extensively discussed in planar image detection by using IoU loss and related variants. However, these solutions cannot be migrated to spherical image object detection due to the undifferentiable of the Spherical IoU (SphIoU). In this paper, we design a simple but effective regression loss based on Gaussian Label Distribution Learning (GLDL) for spherical image object detection. Besides, we observe that the scale of the object in a spherical image varies greatly. The huge differences among objects from different categories make the sample selection strategy based on SphIoU challenging. Therefore, we propose GLDL-ATSS as a better training sample selection strategy for objects of the spherical image, which can alleviate the drawback of IoU threshold-based strategy of scale-sample imbalance. Extensive results on various two datasets with different baseline detectors show the effectiveness of our approach.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Gaussian_Label_Distribution_Learning_for_Spherical_Image_Object_Detection_CVPR_2023_paper.pdf,,3,本文介绍了球形图像目标检测在虚拟现实、机器人和自动驾驶等领域的应用，而许多现有的检测器使用ln-norms损失来回归球形边界框。然而，ln-norms损失存在两个固有缺陷，即参数的独立优化和度量（由IoU主导）与损失之间的不一致性。这些问题在平面图像检测中很常见，但在球形图像检测中更为显著。本文提出了一种基于高斯标签分布学习（GLDL）的简单而有效的回归损失，用于球形图像目标检测。此外，我们观察到球形图像中物体的尺度差异很大。不同类别的物体之间的巨大差异使得基于SphIoU的样本选择策略具有挑战性。因此，我们提出了GLDL-ATSS作为更好的球形图像对象的
37,Generalized UAV Object Detection via Frequency Domain Disentanglement,CVPR,2023,"Kunyu Wang, Xueyang Fu, Yukun Huang, Chengzhi Cao, Gege Shi, Zheng-Jun Zha","When deploying the Unmanned Aerial Vehicles object detection (UAV-OD) network to complex and unseen real-world scenarios, the generalization ability is usually reduced due to the domain shift. To address this issue, this paper proposes a novel frequency domain disentanglement method to improve the UAV-OD generalization. Specifically, we first verified that the spectrum of different bands in the image has different effects to the UAV-OD generalization. Based on this conclusion, we design two learnable filters to extract domain-invariant spectrum and domain-specific spectrum, respectively. The former can be used to train the UAV-OD network and improve its capacity for generalization. In addition, we design a new instance-level contrastive loss to guide the network training. This loss enables the network to concentrate on extracting domain-invariant spectrum and domain-specific spectrum, so as to achieve better disentangling results. Experimental results on three unseen target domains demonstrate that our method has better generalization ability than both the baseline method and state-of-the-art methods.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Generalized_UAV_Object_Detection_via_Frequency_Domain_Disentanglement_CVPR_2023_paper.pdf,,3,本文提出了一种新颖的频域解缠方法，以提高无人机目标检测（UAV-OD）网络在复杂和未知的实际场景中的泛化能力。首先验证了图像中不同频段的频谱对UAV-OD泛化的影响不同，基于此设计了两个可学习的滤波器，分别提取域不变频谱和域特定频谱。前者可用于训练UAV-OD网络并提高其泛化能力。此外，设计了一种新的实例级对比损失来指导网络训练，使网络集中于提取域不变频谱和域特定频谱，以实现更好的解缠结果。在三个未知目标域上的实验结果表明，我们的方法比基线方法和最先进的方法具有更好的泛化能力。
42,Q-DETR: An Efficient Low-Bit Quantized Detection Transformer,CVPR,2023,"Sheng Xu, Yanjing Li, Mingbao Lin, Peng Gao, Guodong Guo, Jinhu Lü, Baochang Zhang","The recent detection transformer (DETR) has advanced object detection, but its application on resource-constrained devices requires massive computation and memory resources. Quantization stands out as a solution by representing the network in low-bit parameters and operations. However, there is a significant performance drop when performing low-bit quantized DETR (Q-DETR) with existing quantization methods. We find that the bottlenecks of Q-DETR come from the query information distortion through our empirical analyses. This paper addresses this problem based on a distribution rectification distillation (DRD). We formulate our DRD as a bi-level optimization problem, which can be derived by generalizing the information bottleneck (IB) principle to the learning of Q-DETR. At the inner level, we conduct a distribution alignment for the queries to maximize the self-information entropy. At the upper level, we introduce a new foreground-aware query matching scheme to effectively transfer the teacher information to distillation-desired features to minimize the conditional information entropy. Extensive experimental results show that our method performs much better than prior arts. For example, the 4-bit Q-DETR can theoretically accelerate DETR with ResNet-50 backbone by 6.6x and achieve 39.4% AP, with only 2.6% performance gaps than its real-valued counterpart on the COCO dataset.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Q-DETR_An_Efficient_Low-Bit_Quantized_Detection_Transformer_CVPR_2023_paper.pdf,,3,本文介绍了一种基于分布矫正蒸馏（DRD）的低比特量化检测变压器（Q-DETR）方法，以解决现有量化方法在执行Q-DETR时性能下降的问题。通过将信息瓶颈（IB）原则推广到Q-DETR的学习中，我们将DRD制定为一个双层优化问题。在内部层面上，我们对查询进行分布对齐，以最大化自信息熵。在上层，我们引入了一种新的前景感知查询匹配方案，以有效地将教师信息传输到蒸馏所需的特征中，以最小化条件信息熵。实验结果表明，我们的方法比现有方法表现更好。例如，4比特Q-DETR可以在COCO数据集上实现39.4％的AP，理论上可以将带有ResNet-50骨干的DETR加速6.6倍，与其实值对应物仅有2.6％的性能差距。
58,Angelic Patches for Improving Third-Party Object Detector Performance,CVPR,2023,"Wenwen Si, Shuo Li, Sangdon Park, Insup Lee, Osbert Bastani","Deep learning models have shown extreme vulnerability to simple perturbations and spatial transformations. In this work, we explore whether we can adopt the characteristics of adversarial attack methods to help improve perturbation robustness for object detection. We study a class of realistic object detection settings wherein the target objects have control over their appearance. To this end, we propose a reversed Fast Gradient Sign Method (FGSM) to obtain these angelic patches that significantly increase the detection probability, even without pre-knowledge of the perturbations. In detail, we apply the patch to each object instance simultaneously, strengthen not only classification but also bounding box accuracy. Experiments demonstrate the efficacy of the partial-covering patch in solving the complex bounding box problem. More importantly, the performance is also transferable to different detection models even under severe affine transformations and deformable shapes. To our knowledge, we are the first (object detection) patch that achieves both cross-model and multiple-patch efficacy. We observed average accuracy improvements of 30% in the real-world experiments, which brings large social value. Our code is available at: https://github.com/averysi224/angelic_patches.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Si_Angelic_Patches_for_Improving_Third-Party_Object_Detector_Performance_CVPR_2023_paper.pdf,https://github.com/averysi224/angelic_patches,3,本文研究了深度学习模型对简单扰动和空间变换的极度脆弱性，并探讨了是否可以采用对抗攻击方法的特征来帮助提高目标检测的扰动鲁棒性。研究了一类现实的目标检测设置，其中目标对象可以控制其外观。为此，提出了一种反向快速梯度符号方法（FGSM）来获取这些天使贴片，即使没有预先了解扰动，也能显著提高检测概率。具体来说，我们将贴片同时应用于每个对象实例，不仅增强了分类，还提高了边界框的准确性。实验表明，部分覆盖贴片在解决复杂边界框问题方面具有很好的效果。更重要的是，该性能在严重的仿射变换和可变形形状下也具有可转移性。据我们所知，我们是第
73,Camouflaged Instance Segmentation via Explicit De-Camouflaging,CVPR,2023,"Naisong Luo, Yuwen Pan, Rui Sun, Tianzhu Zhang, Zhiwei Xiong, Feng Wu","Camouflaged Instance Segmentation (CIS) aims at predicting the instance-level masks of camouflaged objects, which are usually the animals in the wild adapting their appearance to match the surroundings. Previous instance segmentation methods perform poorly on this task as they are easily disturbed by the deceptive camouflage. To address these challenges, we propose a novel De-camouflaging Network (DCNet) including a pixel-level camouflage decoupling module and an instance-level camouflage suppression module. The proposed DCNet enjoys several merits. First, the pixel-level camouflage decoupling module can extract camouflage characteristics based on the Fourier transformation. Then a difference attention mechanism is proposed to eliminate the camouflage characteristics while reserving target object characteristics in the pixel feature. Second, the instance-level camouflage suppression module can aggregate rich instance information from pixels by use of instance prototypes. To mitigate the effect of background noise during segmentation, we introduce some reliable reference points to build a more robust similarity measurement. With the aid of these two modules, our DCNet can effectively model de-camouflaging and achieve accurate segmentation for camouflaged instances. Extensive experimental results on two benchmarks demonstrate that our DCNet performs favorably against state-of-the-art CIS methods, e.g., with more than 5% performance gains on COD10K and NC4K datasets in average precision.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Camouflaged_Instance_Segmentation_via_Explicit_De-Camouflaging_CVPR_2023_paper.pdf,,3,本文介绍了一种名为 Camouflaged Instance Segmentation (CIS) 的方法，旨在预测伪装对象的实例级掩模，这些对象通常是野生动物，它们会调整外观以匹配周围环境。以往的实例分割方法在这个任务上表现不佳，因为它们很容易被欺骗性的伪装所干扰。为了解决这些挑战，作者提出了一种新颖的 De-camouflaging Network (DCNet)，包括像素级伪装解耦模块和实例级伪装抑制模块。DCNet 具有以下几个优点。首先，像素级伪装解耦模块可以基于傅里叶变换提取伪装特征。然后，提出了一种差异注意机制，以消除伪装特征，同时保留像素特征中的目标对象特征。其次，实例级伪装抑制模块可以通过使用实例原型从像素中聚
74,Learning Transformations To Reduce the Geometric Shift in Object Detection,CVPR,2023,"Vidit Vidit, Martin Engilberge, Mathieu Salzmann","The performance of modern object detectors drops when the test distribution differs from the training one. Most of the methods that address this focus on object appearance changes caused by, e.g., different illumination conditions, or gaps between synthetic and real images. Here, by contrast, we tackle geometric shifts emerging from variations in the image capture process, or due to the constraints of the environment causing differences in the apparent geometry of the content itself. We introduce a self-training approach that learns a set of geometric transformations to minimize these shifts without leveraging any labeled data in the new domain, nor any information about the cameras. We evaluate our method on two different shifts, i.e., a camera's field of view (FoV) change and a viewpoint change. Our results evidence that learning geometric transformations helps detectors to perform better in the target domains.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Vidit_Learning_Transformations_To_Reduce_the_Geometric_Shift_in_Object_Detection_CVPR_2023_paper.pdf,,3,现代目标检测器的性能会因为测试分布与训练分布不同而下降。大多数解决这个问题的方法都集中在处理由于物体外观变化引起的问题，例如不同的照明条件或合成图像与真实图像之间的差距。相比之下，我们处理由于图像捕获过程中的变化或环境约束引起的几何变换。我们引入了一种自我训练方法，学习一组几何变换，以最小化这些变换，而不需要利用新域中的任何标记数据或相机信息。我们在两种不同的变换，即摄像机视野（FoV）变化和视角变化上评估了我们的方法。我们的结果表明，学习几何变换有助于检测器在目标域中表现更好。
3,Visual Recognition by Request,CVPR,2023,"Chufeng Tang, Lingxi Xie, Xiaopeng Zhang, Xiaolin Hu, Qi Tian","Humans have the ability of recognizing visual semantics in an unlimited granularity, but existing visual recognition algorithms cannot achieve this goal. In this paper, we establish a new paradigm named visual recognition by request (ViRReq) to bridge the gap. The key lies in decomposing visual recognition into atomic tasks named requests and leveraging a knowledge base, a hierarchical and text-based dictionary, to assist task definition. ViRReq allows for (i) learning complicated whole-part hierarchies from highly incomplete annotations and (ii) inserting new concepts with minimal efforts. We also establish a solid baseline by integrating language-driven recognition into recent semantic and instance segmentation methods, and demonstrate its flexible recognition ability on CPP and ADE20K, two datasets with hierarchical whole-part annotations.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Visual_Recognition_by_Request_CVPR_2023_paper.pdf,,3,本文提出了一种名为“请求驱动的视觉识别”（ViRReq）的新范式，旨在弥合现有视觉识别算法无法实现无限粒度识别的差距。该方法将视觉识别分解为原子任务，即请求，并利用知识库（一种层次化的基于文本的字典）来辅助任务定义。ViRReq 允许从高度不完整的注释中学习复杂的整体-部分层次结构，并以最小的努力插入新概念。本文还将语言驱动的识别与最近的语义和实例分割方法相结合，建立了一个坚实的基线，并在具有层次化整体-部分注释的 CPP 和 ADE20K 两个数据集上展示了其灵活的识别能力。
5,SimpSON: Simplifying Photo Cleanup With Single-Click Distracting Object Segmentation Network,CVPR,2023,"Chuong Huynh, Yuqian Zhou, Zhe Lin, Connelly Barnes, Eli Shechtman, Sohrab Amirghodsi, Abhinav Shrivastava","In photo editing, it is common practice to remove visual distractions to improve the overall image quality and highlight the primary subject. However, manually selecting and removing these small and dense distracting regions can be a laborious and time-consuming task. In this paper, we propose an interactive distractor selection method that is optimized to achieve the task with just a single click. Our method surpasses the precision and recall achieved by the traditional method of running panoptic segmentation and then selecting the segments containing the clicks. We also showcase how a transformer-based module can be used to identify more distracting regions similar to the user's click position. Our experiments demonstrate that the model can effectively and accurately segment unknown distracting objects interactively and in groups. By significantly simplifying the photo cleaning and retouching process, our proposed model provides inspiration for exploring rare object segmentation and group selection with a single click.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Huynh_SimpSON_Simplifying_Photo_Cleanup_With_Single-Click_Distracting_Object_Segmentation_Network_CVPR_2023_paper.pdf,,3,本文提出了一种交互式干扰物选择方法，旨在通过单击实现任务。该方法优于传统的全景分割方法，并展示了如何使用基于转换器的模块识别更多类似于用户点击位置的干扰区域。实验表明，该模型可以有效地和准确地交互式地分割未知的干扰物体，并且可以简化照片清理和修饰过程，为探索单击实现稀有对象分割和组选择提供了启示。
9,"M6Doc: A Large-Scale Multi-Format, Multi-Type, Multi-Layout, Multi-Language, Multi-Annotation Category Dataset for Modern Document Layout Analysis",CVPR,2023,"Hiuyi Cheng, Peirong Zhang, Sihang Wu, Jiaxin Zhang, Qiyuan Zhu, Zecheng Xie, Jing Li, Kai Ding, Lianwen Jin","Document layout analysis is a crucial prerequisite for document understanding, including document retrieval and conversion. Most public datasets currently contain only PDF documents and lack realistic documents. Models trained on these datasets may not generalize well to real-world scenarios. Therefore, this paper introduces a large and diverse document layout analysis dataset called M^6-Doc. The M^6 designation represents six properties: (1) Multi-Format (including scanned, photographed, and PDF documents); (2) Multi-Type (such as scientific articles, textbooks, books, test papers, magazines, newspapers, and notes); (3) Multi-Layout (rectangular, Manhattan, non-Manhattan, and multi-column Manhattan); (4) Multi-Language (Chinese and English); (5) Multi-Annotation Category (74 types of annotation labels with 237,116 annotation instances in 9,080 manually annotated pages); and (6) Modern documents. Additionally, we propose a transformer-based document layout analysis method called TransDLANet, which leverages an adaptive element matching mechanism that enables query embedding to better match ground truth to improve recall, and constructs a segmentation branch for more precise document image instance segmentation. We conduct a comprehensive evaluation of M^6-Doc with various layout analysis methods and demonstrate its effectiveness. TransDLANet achieves state-of-the-art performance on M^6-Doc with 64.5% mAP. The M^6-Doc dataset will be available at https://github.com/HCIILAB/M6Doc.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_M6Doc_A_Large-Scale_Multi-Format_Multi-Type_Multi-Layout_Multi-Language_Multi-Annotation_Category_Dataset_CVPR_2023_paper.pdf,https://github.com/HCIILAB/M6Doc,3,"本文介绍了一个名为M^6-Doc的大型多样化文档布局分析数据集，其中M^6代表六个属性：多格式（包括扫描、拍照和PDF文档）、多类型（如科学文章、教科书、书籍、试卷、杂志、报纸和笔记）、多布局（矩形、曼哈顿、非曼哈顿和多列曼哈顿）、多语言（中文和英文）、多注释类别（74种注释标签，涵盖9,080个手动注释页面中的237,116个注释实例）和现代文档。此外，我们提出了一种基于Transformer的文档布局分析方法TransDLANet，它利用自适应元素匹配机制，使查询嵌入更好地匹配基准真实值以提高召回率，并构建了一个分割分支，以实现更精确的文档图像实例分割。我们使用各种布局分析方法对M^6-Doc进行了全面评估，并"
16,Freestyle Layout-to-Image Synthesis,CVPR,2023,"Han Xue, Zhiwu Huang, Qianru Sun, Li Song, Wenjun Zhang","Typical layout-to-image synthesis (LIS) models generate images for a closed set of semantic classes, e.g., 182 common objects in COCO-Stuff. In this work, we explore the freestyle capability of the model, i.e., how far can it generate unseen semantics (e.g., classes, attributes, and styles) onto a given layout, and call the task Freestyle LIS (FLIS). Thanks to the development of large-scale pre-trained language-image models, a number of discriminative models (e.g., image classification and object detection) trained on limited base classes are empowered with the ability of unseen class prediction. Inspired by this, we opt to leverage large-scale pre-trained text-to-image diffusion models to achieve the generation of unseen semantics. The key challenge of FLIS is how to enable the diffusion model to synthesize images from a specific layout which very likely violates its pre-learned knowledge, e.g., the model never sees ""a unicorn sitting on a bench"" during its pre-training. To this end, we introduce a new module called Rectified Cross-Attention (RCA) that can be conveniently plugged in the diffusion model to integrate semantic masks. This ""plug-in"" is applied in each cross-attention layer of the model to rectify the attention maps between image and text tokens. The key idea of RCA is to enforce each text token to act on the pixels in a specified region, allowing us to freely put a wide variety of semantics from pre-trained knowledge (which is general) onto the given layout (which is specific). Extensive experiments show that the proposed diffusion network produces realistic and freestyle layout-to-image generation results with diverse text inputs, which has a high potential to spawn a bunch of interesting applications. Code is available at https://github.com/essunny310/FreestyleNet.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_Freestyle_Layout-to-Image_Synthesis_CVPR_2023_paper.pdf,https://github.com/essunny310/FreestyleNet,3,本文研究了布局到图像合成（LIS）模型的自由风格能力，即它能够在给定布局上生成未见过的语义（例如类别、属性和风格）的能力，并称之为自由风格LIS（FLIS）。通过利用大规模预训练的文本到图像扩散模型，本文实现了未见语义的生成。FLIS的关键挑战是如何使扩散模型能够从特定布局合成图像，而这很可能违反了其预先学习的知识。为此，本文引入了一种新的模块，称为矫正交叉注意力（RCA），可以方便地插入扩散模型中以集成语义掩码。RCA应用于模型的每个交叉注意力层中，以矫正图像和文本令牌之间的注意力映射。RCA的关键思想是强制每个文本令牌作用于指定区域的像素，使我们能
17,Mobile User Interface Element Detection via Adaptively Prompt Tuning,CVPR,2023,"Zhangxuan Gu, Zhuoer Xu, Haoxing Chen, Jun Lan, Changhua Meng, Weiqiang Wang","Recent object detection approaches rely on pretrained vision-language models for image-text alignment. However, they fail to detect the Mobile User Interface (MUI) element since it contains additional OCR information, which describes its content and function but is often ignored. In this paper, we develop a new MUI element detection dataset named MUI-zh and propose an Adaptively Prompt Tuning (APT) module to take advantage of discriminating OCR information. APT is a lightweight and effective module to jointly optimize category prompts across different modalities. For every element, APT uniformly encodes its visual features and OCR descriptions to dynamically adjust the representation of frozen category prompts. We evaluate the effectiveness of our plug-and-play APT upon several existing CLIP-based detectors for both standard and open-vocabulary MUI element detection. Extensive experiments show that our method achieves considerable improvements on two datasets. The datasets is available at github.com/antmachineintelligence/MUI-zh.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_Mobile_User_Interface_Element_Detection_via_Adaptively_Prompt_Tuning_CVPR_2023_paper.pdf,,3,本文介绍了一种新的 Mobile User Interface (MUI) 元素检测数据集 MUI-zh，并提出了一种自适应提示调整 (APT) 模块，以利用区分 OCR 信息。APT 是一种轻量级且有效的模块，可联合优化不同模态的类别提示。对于每个元素，APT 均匀编码其视觉特征和 OCR 描述，以动态调整冻结类别提示的表示。作者在多个现有的基于 CLIP 的检测器上评估了我们的即插即用 APT，用于标准和开放词汇 MUI 元素检测。广泛的实验表明，我们的方法在两个数据集上取得了相当大的改进。该数据集可在 github.com/antmachineintelligence/MUI-zh 上获得。
26,Multi-View Adversarial Discriminator: Mine the Non-Causal Factors for Object Detection in Unseen Domains,CVPR,2023,"Mingjun Xu, Lingyun Qin, Weijie Chen, Shiliang Pu, Lei Zhang","Domain shift degrades the performance of object detection models in practical applications. To alleviate the influence of domain shift, plenty of previous work try to decouple and learn the domain-invariant (common) features from source domains via domain adversarial learning (DAL). However, inspired by causal mechanisms, we find that previous methods ignore the implicit insignificant non-causal factors hidden in the common features. This is mainly due to the single-view nature of DAL. In this work, we present an idea to remove non-causal factors from common features by multi-view adversarial training on source domains, because we observe that such insignificant non-causal factors may still be significant in other latent spaces (views) due to the multi-mode structure of data. To summarize, we propose a Multi-view Adversarial Discriminator (MAD) based domain generalization model, consisting of a Spurious Correlations Generator (SCG) that increases the diversity of source domain by random augmentation and a Multi-View Domain Classifier (MVDC) that maps features to multiple latent spaces, such that the non-causal factors are removed and the domain-invariant features are purified. Extensive experiments on six benchmarks show our MAD obtains state-of-the-art performance.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Multi-View_Adversarial_Discriminator_Mine_the_Non-Causal_Factors_for_Object_Detection_CVPR_2023_paper.pdf,,3,本文研究了领域偏移对目标检测模型在实际应用中的性能影响。为了缓解领域偏移的影响，之前的研究尝试通过领域对抗学习（DAL）从源域中解耦并学习领域不变（共同）特征。然而，受因果机制的启发，我们发现之前的方法忽略了隐藏在共同特征中的隐含的不重要的非因果因素。这主要是由于DAL的单视图性质。本文提出了一种通过源域的多视图对抗训练来消除共同特征中的非因果因素的想法，因为我们观察到这种不重要的非因果因素可能仍然在其他潜在空间（视图）中具有重要性，这是由于数据的多模态结构。总之，我们提出了一种基于多视图对抗鉴别器（MAD）的领域泛化模型
30,Benchmarking Self-Supervised Learning on Diverse Pathology Datasets,CVPR,2023,"Mingu Kang, Heon Song, Seonwook Park, Donggeun Yoo, Sérgio Pereira","Computational pathology can lead to saving human lives, but models are annotation hungry and pathology images are notoriously expensive to annotate. Self-supervised learning has shown to be an effective method for utilizing unlabeled data, and its application to pathology could greatly benefit its downstream tasks. Yet, there are no principled studies that compare SSL methods and discuss how to adapt them for pathology. To address this need, we execute the largest-scale study of SSL pre-training on pathology image data, to date. Our study is conducted using 4 representative SSL methods on diverse downstream tasks. We establish that large-scale domain-aligned pre-training in pathology consistently out-performs ImageNet pre-training in standard SSL settings such as linear and fine-tuning evaluations, as well as in low-label regimes. Moreover, we propose a set of domain-specific techniques that we experimentally show leads to a performance boost. Lastly, for the first time, we apply SSL to the challenging task of nuclei instance segmentation and show large and consistent performance improvements under diverse settings.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Benchmarking_Self-Supervised_Learning_on_Diverse_Pathology_Datasets_CVPR_2023_paper.pdf,,3,计算机病理学可以拯救人类生命，但模型需要大量注释，而病理图像的注释费用昂贵。自监督学习已被证明是利用未标记数据的有效方法，其在病理学中的应用可以极大地有益于其下游任务。然而，目前没有原则性的研究比较自监督学习方法，并讨论如何将其应用于病理学。为了解决这个问题，我们进行了迄今为止规模最大的自监督学习预训练病理图像数据的研究。我们使用4种代表性的自监督学习方法进行研究，并在不同的下游任务中进行了评估。我们确定，在病理学中进行大规模的领域对齐预训练始终优于在标准自监督学习设置中进行的ImageNet预训练，如线性和微调评估，以及在低标签制度下。此外，我们
31,Panoptic Compositional Feature Field for Editable Scene Rendering With Network-Inferred Labels via Metric Learning,CVPR,2023,"Xinhua Cheng, Yanmin Wu, Mengxi Jia, Qian Wang, Jian Zhang","Despite neural implicit representations demonstrating impressive high-quality view synthesis capacity, decomposing such representations into objects for instance-level editing is still challenging. Recent works learn object-compositional representations supervised by ground truth instance annotations and produce promising scene editing results. However, ground truth annotations are manually labeled and expensive in practice, which limits their usage in real-world scenes. In this work, we attempt to learn an object-compositional neural implicit representation for editable scene rendering by leveraging labels inferred from the off-the-shelf 2D panoptic segmentation networks instead of the ground truth annotations. We propose a novel framework named Panoptic Compositional Feature Field (PCFF), which introduces an instance quadruplet metric learning to build a discriminating panoptic feature space for reliable scene editing. In addition, we propose semantic-related strategies to further exploit the correlations between semantic and appearance attributes for achieving better rendering results. Experiments on multiple scene datasets including ScanNet, Replica, and ToyDesk demonstrate that our proposed method achieves superior performance for novel view synthesis and produces convincing real-world scene editing results. The code will be available.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_Panoptic_Compositional_Feature_Field_for_Editable_Scene_Rendering_With_Network-Inferred_CVPR_2023_paper.pdf,,3,本文提出了一种名为 Panoptic Compositional Feature Field (PCFF) 的新框架，通过利用来自现成 2D 全景分割网络的标签来学习可编辑场景渲染的对象组合神经隐式表示。该框架引入了实例四元组度量学习，以构建一个可靠的全景特征空间，以实现可靠的场景编辑。此外，我们提出了语义相关策略，以进一步利用语义和外观属性之间的相关性，以实现更好的渲染结果。在包括 ScanNet、Replica 和 ToyDesk 在内的多个场景数据集上的实验表明，我们提出的方法在新视角合成方面具有卓越的性能，并产生了令人信服的真实世界场景编辑结果。代码将会公开。
32,Tree Instance Segmentation With Temporal Contour Graph,CVPR,2023,"Adnan Firoze, Cameron Wingren, Raymond A. Yeh, Bedrich Benes, Daniel Aliaga","We present a novel approach to perform instance segmentation, and counting, for densely packed self-similar trees using a top-view RGB image sequence. We propose a solution that leverages pixel content, shape, and self-occlusion. First, we perform an initial over-segmentation of the image sequence and aggregate structural characteristics into a contour graph with temporal information incorporated. Second, using a graph convolutional network and its inherent local messaging passing abilities, we merge adjacent tree crown patches into a final set of tree crowns. Per various studies and comparisons, our method is superior to all prior methods and results in high-accuracy instance segmentation and counting, despite the trees being tightly packed. Finally, we provide various forest image sequence datasets suitable for subsequent benchmarking and evaluation captured at different altitudes and leaf conditions.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Firoze_Tree_Instance_Segmentation_With_Temporal_Contour_Graph_CVPR_2023_paper.pdf,,3,本文提出了一种新颖的方法，利用顶视 RGB 图像序列对密集自相似树进行实例分割和计数。我们提出了一种解决方案，利用像素内容、形状和自遮挡。首先，我们对图像序列进行初始过分割，并将结构特征聚合到带有时间信息的轮廓图中。其次，利用图卷积网络及其固有的局部消息传递能力，将相邻的树冠补丁合并成最终的树冠集合。根据各种研究和比较，我们的方法优于所有先前的方法，并在树木紧密排列的情况下实现了高精度的实例分割和计数。最后，我们提供了适用于后续基准测试和评估的各种森林图像序列数据集，这些数据集在不同的高度和叶片条件下捕获。
46,A General Regret Bound of Preconditioned Gradient Method for DNN Training,CVPR,2023,"Hongwei Yong, Ying Sun, Lei Zhang","While adaptive learning rate methods, such as Adam, have achieved remarkable improvement in optimizing Deep Neural Networks (DNNs), they consider only the diagonal elements of the full preconditioned matrix. Though the full-matrix preconditioned gradient methods theoretically have a lower regret bound, they are impractical for use to train DNNs because of the high complexity. In this paper, we present a general regret bound with a constrained full-matrix preconditioned gradient and show that the updating formula of the preconditioner can be derived by solving a cone-constrained optimization problem. With the block-diagonal and Kronecker-factorized constraints, a specific guide function can be obtained. By minimizing the upper bound of the guide function, we develop a new DNN optimizer, termed AdaBK. A series of techniques, including statistics updating, dampening, efficient matrix inverse root computation, and gradient amplitude preservation, are developed to make AdaBK effective and efficient to implement. The proposed AdaBK can be readily embedded into many existing DNN optimizers, e.g., SGDM and AdamW, and the corresponding SGDM_BK and AdamW_BK algorithms demonstrate significant improvements over existing DNN optimizers on benchmark vision tasks, including image classification, object detection and segmentation. The source code will be made publicly available.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Yong_A_General_Regret_Bound_of_Preconditioned_Gradient_Method_for_DNN_CVPR_2023_paper.pdf,,3,本文介绍了一种新的深度神经网络优化器AdaBK，它通过解决锥约束优化问题得到了预处理器的更新公式，并通过最小化指导函数的上界来实现优化。AdaBK采用了一系列技术，包括统计更新、阻尼、高效矩阵逆根计算和梯度幅度保持等，使其在实现时既有效又高效。AdaBK可以轻松地嵌入到许多现有的深度神经网络优化器中，例如SGDM和AdamW，相应的SGDM_BK和AdamW_BK算法在基准视觉任务中，包括图像分类、目标检测和分割等方面，都表现出了显著的改进。源代码将公开发布。
48,Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving,CVPR,2023,"Ben Agro, Quinlan Sykora, Sergio Casas, Raquel Urtasun","A self-driving vehicle (SDV) must be able to perceive its surroundings and predict the future behavior of other traffic participants. Existing works either perform object detection followed by trajectory forecasting of the detected objects, or predict dense occupancy and flow grids for the whole scene. The former poses a safety concern as the number of detections needs to be kept low for efficiency reasons, sacrificing object recall. The latter is computationally expensive due to the high-dimensionality of the output grid, and suffers from the limited receptive field inherent to fully convolutional networks. Furthermore, both approaches employ many computational resources predicting areas or objects that might never be queried by the motion planner. This motivates our unified approach to perception and future prediction that implicitly represents occupancy and flow over time with a single neural network. Our method avoids unnecessary computation, as it can be directly queried by the motion planner at continuous spatio-temporal locations. Moreover, we design an architecture that overcomes the limited receptive field of previous explicit occupancy prediction methods by adding an efficient yet effective global attention mechanism. Through extensive experiments in both urban and highway settings, we demonstrate that our implicit model outperforms the current state-of-the-art. For more information, visit the project website: https://waabi.ai/research/implicito.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Agro_Implicit_Occupancy_Flow_Fields_for_Perception_and_Prediction_in_Self-Driving_CVPR_2023_paper.pdf,,3,自动驾驶车辆必须能够感知周围环境并预测其他交通参与者的未来行为。现有的方法要么执行对象检测，然后预测检测到的对象的轨迹，要么为整个场景预测密集的占用和流格。前者存在安全问题，因为为了效率原因，需要保持检测数量较少，从而牺牲对象召回率。后者由于输出格的高维度和完全卷积网络固有的有限接受域而计算成本高。此外，这两种方法都使用许多计算资源来预测可能永远不会被运动规划器查询的区域或对象。这激发了我们对感知和未来预测的统一方法，该方法使用单个神经网络隐式表示随时间的占用和流。我们的方法避免了不必要的计算，因为它可以直接在连续的时空位置上被运动规划器查询。此外，我们设计了一种
51,The Differentiable Lens: Compound Lens Search Over Glass Surfaces and Materials for Object Detection,CVPR,2023,"Geoffroi Côté, Fahim Mannan, Simon Thibault, Jean-François Lalonde, Felix Heide","Most camera lens systems are designed in isolation, separately from downstream computer vision methods. Recently, joint optimization approaches that design lenses alongside other components of the image acquisition and processing pipeline--notably, downstream neural networks--have achieved improved imaging quality or better performance on vision tasks. However, these existing methods optimize only a subset of lens parameters and cannot optimize glass materials given their categorical nature. In this work, we develop a differentiable spherical lens simulation model that accurately captures geometrical aberrations. We propose an optimization strategy to address the challenges of lens design--notorious for non-convex loss function landscapes and many manufacturing constraints--that are exacerbated in joint optimization tasks. Specifically, we introduce quantized continuous glass variables to facilitate the optimization and selection of glass materials in an end-to-end design context, and couple this with carefully designed constraints to support manufacturability. In automotive object detection, we report improved detection performance over existing designs even when simplifying designs to two- or three-element lenses, despite significantly degrading the image quality.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Cote_The_Differentiable_Lens_Compound_Lens_Search_Over_Glass_Surfaces_and_CVPR_2023_paper.pdf,,3,本文介绍了一种新的可微分球形镜头模拟模型，该模型能够准确地捕捉几何畸变。作者提出了一种优化策略，以解决镜头设计中的挑战，包括非凸损失函数景观和许多制造约束，这些挑战在联合优化任务中更加严峻。作者引入了量化连续玻璃变量，以促进在端到端设计环境中优化和选择玻璃材料，并结合精心设计的约束来支持可制造性。在汽车目标检测中，即使将设计简化为两个或三个元素的镜头，也能报告比现有设计更好的检测性能，尽管图像质量显著降低。
53,Camouflaged Object Detection With Feature Decomposition and Edge Reconstruction,CVPR,2023,"Chunming He, Kai Li, Yachao Zhang, Longxiang Tang, Yulun Zhang, Zhenhua Guo, Xiu Li","Camouflaged object detection (COD) aims to address the tough issue of identifying camouflaged objects visually blended into the surrounding backgrounds. COD is a challenging task due to the intrinsic similarity of camouflaged objects with the background, as well as their ambiguous boundaries. Existing approaches to this problem have developed various techniques to mimic the human visual system. Albeit effective in many cases, these methods still struggle when camouflaged objects are so deceptive to the vision system. In this paper, we propose the FEature Decomposition and Edge Reconstruction (FEDER) model for COD. The FEDER model addresses the intrinsic similarity of foreground and background by decomposing the features into different frequency bands using learnable wavelets. It then focuses on the most informative bands to mine subtle cues that differentiate foreground and background. To achieve this, a frequency attention module and a guidance-based feature aggregation module are developed. To combat the ambiguous boundary problem, we propose to learn an auxiliary edge reconstruction task alongside the COD task. We design an ordinary differential equation-inspired edge reconstruction module that generates exact edges. By learning the auxiliary task in conjunction with the COD task, the FEDER model can generate precise prediction maps with accurate object boundaries. Experiments show that our FEDER model significantly outperforms state-of-the-art methods with cheaper computational and memory costs.",,https://openaccess.thecvf.com/content/CVPR2023/papers/He_Camouflaged_Object_Detection_With_Feature_Decomposition_and_Edge_Reconstruction_CVPR_2023_paper.pdf,,3,本文提出了一种名为FEature Decomposition and Edge Reconstruction (FEDER)的模型，用于解决伪装目标检测（COD）中的难题。COD是一项具有挑战性的任务，因为伪装目标与背景具有内在的相似性，以及它们的边界模糊不清。现有的方法已经开发出了各种技术来模拟人类视觉系统，但是当伪装目标对视觉系统具有欺骗性时，这些方法仍然存在困难。FEDER模型通过使用可学习的小波将特征分解成不同的频带，从而解决了前景和背景的内在相似性问题。然后，它专注于最具信息量的频带，以挖掘区分前景和背景的微妙线索。为了解决模糊的边界问题，我们提出了学习辅助边缘重建任务。通过学习COD任务和辅助任务，FEDER模型
62,Independent Component Alignment for Multi-Task Learning,CVPR,2023,"Dmitry Senushkin, Nikolay Patakin, Arseny Kuznetsov, Anton Konushin","In a multi-task learning (MTL) setting, a single model is trained to tackle a diverse set of tasks jointly. Despite rapid progress in the field, MTL remains challenging due to optimization issues such as conflicting and dominating gradients. In this work, we propose using a condition number of a linear system of gradients as a stability criterion of an MTL optimization. We theoretically demonstrate that a condition number reflects the aforementioned optimization issues. Accordingly, we present Aligned-MTL, a novel MTL optimization approach based on the proposed criterion, that eliminates instability in the training process by aligning the orthogonal components of the linear system of gradients. While many recent MTL approaches guarantee convergence to a minimum, task trade-offs cannot be specified in advance. In contrast, Aligned-MTL provably converges to an optimal point with pre-defined task-specific weights, which provides more control over the optimization result. Through experiments, we show that the proposed approach consistently improves performance on a diverse set of MTL benchmarks, including semantic and instance segmentation, depth estimation, surface normal estimation, and reinforcement learning.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Senushkin_Independent_Component_Alignment_for_Multi-Task_Learning_CVPR_2023_paper.pdf,,3,本文提出了一种基于梯度线性系统条件数的多任务学习（MTL）优化稳定性准则。该准则可以消除训练过程中的不稳定性，并通过对梯度线性系统的正交分量进行对齐来实现。与许多最近的MTL方法不同，Aligned-MTL可以预先指定任务权重并收敛到最优点，从而提供更多的优化结果控制。实验结果表明，该方法在语义和实例分割、深度估计、表面法线估计和强化学习等多个MTL基准测试中均能提高性能。
65,SpaText: Spatio-Textual Representation for Controllable Image Generation,CVPR,2023,"Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, Xi Yin","Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText --- a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition, we show how to extend the classifier-free guidance method in diffusion models to the multi-conditional case and present an alternative accelerated inference algorithm. Finally, we offer several automatic evaluation metrics and use them, in addition to FID scores and a user study, to evaluate our method and show that it achieves state-of-the-art results on image generation with free-form textual scene control.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Avrahami_SpaText_Spatio-Textual_Representation_for_Controllable_Image_Generation_CVPR_2023_paper.pdf,,3,该文介绍了一种新的文本到图像生成方法，名为SpaText。与以往的方法不同，该方法使用开放词汇场景控制，用户提供一个描述整个场景的全局文本提示和一个分割地图，其中每个感兴趣区域都有一个自由形式的自然语言描述。该方法基于一个新颖的CLIP-based空间-文本表示，并在两种最先进的扩散模型：基于像素和基于潜在因素的模型上展示了其有效性。此外，该文还展示了如何将扩散模型中的无分类器指导方法扩展到多条件情况，并提出了一种替代的加速推理算法。最后，该文提供了几个自动评估指标，并使用它们以及FID分数和用户研究来评估该方法，并展示了它在具有自由形式文本场景控制的图像生成方面的最新成果。
71,Learning To Exploit the Sequence-Specific Prior Knowledge for Image Processing Pipelines Optimization,CVPR,2023,"Haina Qin, Longfei Han, Weihua Xiong, Juan Wang, Wentao Ma, Bing Li, Weiming Hu","The hardware image signal processing (ISP) pipeline is the intermediate layer between the imaging sensor and the downstream application, processing the sensor signal into an RGB image. The ISP is less programmable and consists of a series of processing modules. Each processing module handles a subtask and contains a set of tunable hyperparameters. A large number of hyperparameters form a complex mapping with the ISP output. The industry typically relies on manual and time-consuming hyperparameter tuning by image experts, biased towards human perception. Recently, several automatic ISP hyperparameter optimization methods using downstream evaluation metrics come into sight. However, existing methods for ISP tuning treat the high-dimensional parameter space as a global space for optimization and prediction all at once without inducing the structure knowledge of ISP. To this end, we propose a sequential ISP hyperparameter prediction framework that utilizes the sequential relationship within ISP modules and the similarity among parameters to guide the model sequence process. We validate the proposed method on object detection, image segmentation, and image quality tasks.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_Learning_To_Exploit_the_Sequence-Specific_Prior_Knowledge_for_Image_Processing_CVPR_2023_paper.pdf,,3,硬件图像信号处理（ISP）管道是成像传感器和下游应用之间的中间层，将传感器信号处理成RGB图像。ISP不太可编程，由一系列处理模块组成。每个处理模块处理一个子任务，并包含一组可调的超参数。大量的超参数形成了与ISP输出的复杂映射。行业通常依赖于人工感知偏差的手动和耗时的超参数调整。最近，出现了几种使用下游评估指标的自动ISP超参数优化方法。然而，现有的ISP调整方法将高维参数空间视为全局空间进行优化和预测，而没有引入ISP的结构知识。为此，我们提出了一种顺序ISP超参数预测框架，利用ISP模块内的顺序关系和参数之间的相似性来指导模型序列过程。我们在目标检测、图像分割和图像质量任务上验证了所提出的方法。
72,OmniCity: Omnipotent City Understanding With Multi-Level and Multi-View Images,CVPR,2023,"Weijia Li, Yawen Lai, Linning Xu, Yuanbo Xiangli, Jinhua Yu, Conghui He, Gui-Song Xia, Dahua Lin","This paper presents OmniCity, a new dataset for omnipotent city understanding from multi-level and multi-view images. More precisely, OmniCity contains multi-view satellite images as well as street-level panorama and mono-view images, constituting over 100K pixel-wise annotated images that are well-aligned and collected from 25K geo-locations in New York City. To alleviate the substantial pixel-wise annotation efforts, we propose an efficient street-view image annotation pipeline that leverages the existing label maps of satellite view and the transformation relations between different views (satellite, panorama, and mono-view). With the new OmniCity dataset, we provide benchmarks for a variety of tasks including building footprint extraction, height estimation, and building plane/instance/fine-grained segmentation. Compared with existing multi-level and multi-view benchmarks, OmniCity contains a larger number of images with richer annotation types and more views, provides more benchmark results of state-of-the-art models, and introduces a new task for fine-grained building instance segmentation on street-level panorama images. Moreover, OmniCity provides new problem settings for existing tasks, such as cross-view image matching, synthesis, segmentation, detection, etc., and facilitates the developing of new methods for large-scale city understanding, reconstruction, and simulation. The OmniCity dataset as well as the benchmarks will be released at https://city-super.github.io/omnicity/.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Li_OmniCity_Omnipotent_City_Understanding_With_Multi-Level_and_Multi-View_Images_CVPR_2023_paper.pdf,,3,本文介绍了OmniCity，这是一个新的数据集，用于从多层次和多视角图像中理解全能城市。更具体地说，OmniCity包含多视角卫星图像以及街景全景和单视角图像，构成了超过100K像素级注释图像，这些图像都是从纽约市的25K地理位置收集而来，并且对齐良好。为了减轻大量的像素级注释工作，我们提出了一种高效的街景图像注释流水线，利用了卫星视图的现有标签映射和不同视图之间的变换关系（卫星、全景和单视角）。通过新的OmniCity数据集，我们为各种任务提供了基准，包括建筑物轮廓提取、高度估计和建筑平面/实例/细粒度分割。与现有的多层次和多视角基准相比，OmniCity包含更多类型更丰富的注释图像和更多视角，提
75,RA-CLIP: Retrieval Augmented Contrastive Language-Image Pre-Training,CVPR,2023,"Chen-Wei Xie, Siyang Sun, Xiong Xiong, Yun Zheng, Deli Zhao, Jingren Zhou","Contrastive Language-Image Pre-training (CLIP) is attracting increasing attention for its impressive zero-shot recognition performance on different down-stream tasks. However, training CLIP is data-hungry and requires lots of image-text pairs to memorize various semantic concepts. In this paper, we propose a novel and efficient framework: Retrieval Augmented Contrastive Language-Image Pre-training (RA-CLIP) to augment embeddings by online retrieval. Specifically, we sample part of image-text data as a hold-out reference set. Given an input image, relevant image-text pairs are retrieved from the reference set to enrich the representation of input image. This process can be considered as an open-book exam: with the reference set as a cheat sheet, the proposed method doesn't need to memorize all visual concepts in the training data. It explores how to recognize visual concepts by exploiting correspondence between images and texts in the cheat sheet. The proposed RA-CLIP implements this idea and comprehensive experiments are conducted to show how RA-CLIP works. Performances on 10 image classification datasets and 2 object detection datasets show that RA-CLIP outperforms vanilla CLIP baseline by a large margin on zero-shot image classification task (+12.7%), linear probe image classification task (+6.9%) and zero-shot ROI classification task (+2.8%).",,https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_RA-CLIP_Retrieval_Augmented_Contrastive_Language-Image_Pre-Training_CVPR_2023_paper.pdf,,3,本文介绍了一种新的、高效的语言-图像对比预训练框架：检索增强的对比语言-图像预训练（RA-CLIP）。该框架通过在线检索来增强嵌入，从而避免了传统的数据密集型训练方式。具体来说，我们将部分图像-文本数据作为参考集，对于输入的图像，从参考集中检索相关的图像-文本对，以丰富输入图像的表示。这个过程可以看作是一个开卷考试：通过参考集作弊，该方法不需要记忆训练数据中的所有视觉概念，而是通过利用作弊表中图像和文本之间的对应关系来识别视觉概念。实验结果表明，RA-CLIP在零样本图像分类任务（+12.7%）、线性探针图像分类任务（+6.9%）和零样本ROI分类任务（+2.8%）上均优于基线模型vanilla CLIP。
